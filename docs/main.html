<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>w4h API documentation</title>
<meta name="description" content="This is the wells4hydrogeology package …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>w4h</code></h1>
</header>
<section id="section-intro">
<p>This is the wells4hydrogeology package. </p>
<p>It contains the functions needed to convert raw well descriptions into usable (hydro)geologic data.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#__init__.py
&#34;&#34;&#34;
   This is the wells4hydrogeology package. 
   
   It contains the functions needed to convert raw well descriptions into usable (hydro)geologic data.

&#34;&#34;&#34;

from w4h.core import(logger_function,
                        verbose_print,
                        run,
                        get_resources)

from w4h.classify import (specific_define,
                          split_defined,
                          start_define,
                          wildcard_define,
                          remerge_data,
                          depth_define,
                          export_undefined,
                          fill_unclassified,
                          merge_lithologies,
                          get_unique_wells,
                          sort_dataframe)

from w4h.clean import (remove_nonlocated,
                       remove_no_topo,
                       remove_no_depth,
                       remove_bad_depth,
                       remove_no_description)

from w4h.export import (export_dataframe,
                        export_grids)

from w4h.layers import (get_layer_depths,
                        merge_metadata,
                        layer_target_thick,
                        layer_interp,
                        combine_dataset)

from w4h.mapping import (read_study_area,
                         coords2geometry,
                         clip_gdf2study_area,
                         sample_raster_points,
                         xyz_metadata_merge,
                         read_wms,
                         read_wcs,
                         grid2study_area,
                         read_model_grid,
                         read_grid,
                         align_rasters,
                         get_drift_thick)

from w4h.read import (get_current_date,
                      get_most_recent,
                      file_setup,
                      read_raw_csv,
                      read_xyz,
                      read_dict,
                      define_dtypes,
                      get_search_terms,
                      read_dictionary_terms,
                      read_lithologies,
                      add_control_points)


__all__=(&#39;logger_function&#39;,&#39;verbose_print&#39;, &#39;run&#39;, &#39;get_resources&#39;,
        &#39;specific_define&#39;, 
        &#39;split_defined&#39;, 
        &#39;start_define&#39;,
        &#39;wildcard_define&#39;,
        &#39;remerge_data&#39;, 
        &#39;depth_define&#39;, 
        &#39;export_undefined&#39;, 
        &#39;fill_unclassified&#39;, 
        &#39;merge_lithologies&#39;, 
        &#39;get_unique_wells&#39;,
        &#39;sort_dataframe&#39;,
         &#39;remove_nonlocated&#39;, 
         &#39;remove_no_topo&#39;, 
         &#39;remove_no_depth&#39;, 
         &#39;remove_bad_depth&#39;, 
         &#39;remove_no_description&#39;,
        &#39;export_dataframe&#39;,
        &#39;export_grids&#39;,
         &#39;get_layer_depths&#39;,
         &#39;merge_metadata&#39;, 
         &#39;layer_target_thick&#39;, 
         &#39;layer_interp&#39;,
         &#39;combine_dataset&#39;,
        &#39;read_study_area&#39;, 
        &#39;coords2geometry&#39;, 
        &#39;clip_gdf2study_area&#39;, 
        &#39;sample_raster_points&#39;, 
        &#39;xyz_metadata_merge&#39;, 
        &#39;read_wms&#39;,
        &#39;read_wcs&#39;, 
        &#39;grid2study_area&#39;,
        &#39;read_model_grid&#39;,
        &#39;read_grid&#39;,
        &#39;align_rasters&#39;,
        &#39;get_drift_thick&#39;,
         &#39;get_current_date&#39;,
         &#39;get_most_recent&#39;,
         &#39;file_setup&#39;,
         &#39;read_raw_csv&#39;,
         &#39;read_xyz&#39;,
         &#39;read_dict&#39;,
         &#39;define_dtypes&#39;,
         &#39;get_search_terms&#39;,
         &#39;read_dictionary_terms&#39;,
         &#39;read_lithologies&#39;,
         &#39;add_control_points&#39;
         )

# Update the w4h.run() help() return to actually be helpful
run.__doc__ = core._run_docstring()

__author__=&#39;Riley Balikian, Joe Franke, Allan Jones, Mike Krasowski&#39;</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="w4h.classify" href="classify.html">w4h.classify</a></code></dt>
<dd>
<div class="desc"><p>The Classify module contains functions for defining geological intervals into a preset subset of geologic interpretations.</p></div>
</dd>
<dt><code class="name"><a title="w4h.clean" href="clean.html">w4h.clean</a></code></dt>
<dd>
<div class="desc"><p>The Clean module contains functions for cleaning the data (i.e., removing data not to be used in further analysis)</p></div>
</dd>
<dt><code class="name"><a title="w4h.core" href="core.html">w4h.core</a></code></dt>
<dd>
<div class="desc"><p>The Core module contains core functions of the package used in other modules or as primary functions in the package.
This includes the main run() …</p></div>
</dd>
<dt><code class="name"><a title="w4h.export" href="export.html">w4h.export</a></code></dt>
<dd>
<div class="desc"><p>The Export module contains functions for exporting processed data.</p></div>
</dd>
<dt><code class="name"><a title="w4h.layers" href="layers.html">w4h.layers</a></code></dt>
<dd>
<div class="desc"><p>The Layers module contains functions for splitting data into a layered model
and for interpolating data within the layers</p></div>
</dd>
<dt><code class="name"><a title="w4h.mapping" href="mapping.html">w4h.mapping</a></code></dt>
<dd>
<div class="desc"><p>The Mapping module contains the functions used for geospatial analysis throughout the package.
This includes some input/output as well as functions to …</p></div>
</dd>
<dt><code class="name"><a title="w4h.read" href="read.html">w4h.read</a></code></dt>
<dd>
<div class="desc"><p>The Read module contains funtions primarily for the input of data through the reading of data files,
as well as support functions to carry out this task</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="w4h.add_control_points"><code class="name flex">
<span>def <span class="ident">add_control_points</span></span>(<span>df_without_control, df_control=None, xcol='LONGITUDE', ycol='LATITUDE', zcol='ELEV_FT', controlpoints_crs='EPSG:4269', output_crs='EPSG:4269', description_col='FORMATION', interp_col='INTERPRETATION', target_col='TARGET', verbose=False, log=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to add control points, primarily to aid in interpolation. This may be useful when conditions are known but do not exist in input well database</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df_without_control</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe with current working data</dd>
<dt><strong><code>df_control</code></strong> :&ensp;<code>str, pathlib.Purepath,</code> or <code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with control points</dd>
<dt><strong><code>well_key</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column containing the "key" (unique identifier) for each well, by default 'API_NUMBER'</dd>
<dt><strong><code>xcol</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column in df_control containing the x coordinates for each control point, by default 'LONGITUDE'</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column in df_control containing the y coordinates for each control point, by default 'LATITUDE'</dd>
<dt><strong><code>zcol</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column in df_control containing the z coordinates for each control point, by default 'ELEV_FT'</dd>
<dt><strong><code>controlpoints_crs</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column in df_control containing the crs of points, by default 'EPSG:4269'</dd>
<dt><strong><code>output_crs</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The output coordinate system, by default 'EPSG:4269'</dd>
<dt><strong><code>description_col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column in df_control with the description (if this is used), by default 'FORMATION'</dd>
<dt><strong><code>interp_col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column in df_control with the interpretation (if this is used), by default 'INTERPRETATION'</dd>
<dt><strong><code>target_col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The column in df_control with the target code (if this is used), by default 'TARGET'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to print information to terminal, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to log information in log file, by default False</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments of pandas.concat() or pandas.read_csv that will be passed to that function, except for objs, which are df and df_control</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas DataFrame with original data and control points formatted the same way and concatenated together</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_control_points(df_without_control, df_control=None,  xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, zcol=&#39;ELEV_FT&#39;, controlpoints_crs=&#39;EPSG:4269&#39;, output_crs=&#39;EPSG:4269&#39;, description_col=&#39;FORMATION&#39;, interp_col=&#39;INTERPRETATION&#39;, target_col=&#39;TARGET&#39;, verbose=False, log=False, **kwargs):
    &#34;&#34;&#34;Function to add control points, primarily to aid in interpolation. This may be useful when conditions are known but do not exist in input well database

    Parameters
    ----------
    df_without_control : pandas.DataFrame
        Dataframe with current working data
    df_control : str, pathlib.Purepath, or pandas.DataFrame
        Pandas dataframe with control points
    well_key : str, optional
        The column containing the &#34;key&#34; (unique identifier) for each well, by default &#39;API_NUMBER&#39;
    xcol : str, optional
        The column in df_control containing the x coordinates for each control point, by default &#39;LONGITUDE&#39;
    ycol : str, optional
        The column in df_control containing the y coordinates for each control point, by default &#39;LATITUDE&#39;
    zcol : str, optional
        The column in df_control containing the z coordinates for each control point, by default &#39;ELEV_FT&#39;
    controlpoints_crs : str, optional
        The column in df_control containing the crs of points, by default &#39;EPSG:4269&#39;
    output_crs : str, optional
        The output coordinate system, by default &#39;EPSG:4269&#39;
    description_col : str, optional
        The column in df_control with the description (if this is used), by default &#39;FORMATION&#39;
    interp_col : str, optional
        The column in df_control with the interpretation (if this is used), by default &#39;INTERPRETATION&#39;
    target_col : str, optional
        The column in df_control with the target code (if this is used), by default &#39;TARGET&#39;
    verbose : bool, optional
        Whether to print information to terminal, by default False
    log : bool, optional
        Whether to log information in log file, by default False
    **kwargs
        Keyword arguments of pandas.concat() or pandas.read_csv that will be passed to that function, except for objs, which are df and df_control

    Returns
    -------
    pandas.DataFrame
        Pandas DataFrame with original data and control points formatted the same way and concatenated together
    &#34;&#34;&#34;
    if verbose:
        verbose_print(add_control_points, locals(), exclude_params=[&#39;df_without_control&#39;, &#39;df_control&#39;])

    if df_control is None:
        return df_without_control
    elif isinstance(df_control, pd.DataFrame) or isinstance(df_control, gpd.GeoDataFrame):
        pass
    else:
        read_csv_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in inspect.signature(pd.read_csv).parameters.keys()}
        df_control = pd.read_csv(df_control, **read_csv_kwargs)
    
    #Drop unnecessary columns, if needed
    if target_col not in df_without_control.columns and target_col in df_control.columns:
        df_control.drop([target_col], axis=1, inplace=True)
        
    if interp_col not in df_without_control.columns and interp_col in df_control.columns:
        df_control.drop([interp_col], axis=1, inplace=True)
        
    if description_col not in df_without_control.columns and description_col in df_control.columns:
        df_control.drop([description_col], axis=1, inplace=True)            
        
    #If our main df is already a geodataframe, make df_control one too
    if isinstance(df_without_control, gpd.GeoDataFrame):
        from w4h import coords2geometry
        gdf_control = coords2geometry(df_no_geometry=df_control, xcol=xcol, ycol=ycol, zcol=zcol, input_coords_crs=controlpoints_crs, log=log)

    #Get kwargs passed to pd.concat, and set defaults for ignore_index and join
    concat_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in inspect.signature(pd.concat).parameters.keys()}
    if &#39;ignore_index&#39; not in concat_kwargs.keys():
        concat_kwargs[&#39;ignore_index&#39;] = True
    if &#39;join&#39; not in concat_kwargs.keys():
        concat_kwargs[&#39;join&#39;] = &#39;outer&#39;
        
    gdf = pd.concat([df_without_control, gdf_control], **concat_kwargs)
    
    if controlpoints_crs != output_crs:
        gdf = gdf.to_crs(output_crs)
    
    return gdf</code></pre>
</details>
</dd>
<dt id="w4h.align_rasters"><code class="name flex">
<span>def <span class="ident">align_rasters</span></span>(<span>grids_unaligned=None, model_grid=None, no_data_val_grid=0, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Reprojects two rasters and aligns their pixels</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>grids_unaligned</code></strong> :&ensp;<code>list</code> or <code>xarray.DataArray</code></dt>
<dd>Contains a list of grids or one unaligned grid</dd>
<dt><strong><code>model_grid</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>Contains model grid</dd>
<dt><strong><code>no_data_val_grid</code></strong> :&ensp;<code>int</code>, default=<code>0</code></dt>
<dd>Sets value of no data pixels</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>alignedGrids</code></strong> :&ensp;<code>list</code> or <code>xarray.DataArray</code></dt>
<dd>Contains aligned grids</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def align_rasters(grids_unaligned=None, model_grid=None,
                  no_data_val_grid=0, verbose=False, log=False):
    &#34;&#34;&#34;Reprojects two rasters and aligns their pixels

    Parameters
    ----------
    grids_unaligned : list or xarray.DataArray
        Contains a list of grids or one unaligned grid
    model_grid : xarray.DataArray
        Contains model grid
    no_data_val_grid : int, default=0
        Sets value of no data pixels
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    alignedGrids : list or xarray.DataArray
        Contains aligned grids
    &#34;&#34;&#34;
    
    if grids_unaligned is None:
        grids_unaligned = [w4h.get_resources()[&#39;surf_elev&#39;], w4h.get_resources()[&#39;bedrock_elev&#39;]]
    if model_grid is None:
        model_grid = w4h.get_resources()[&#39;model_grid&#39;]

    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(align_rasters, locals(), exclude_params=[&#39;grids_unaligned&#39;, &#39;model_grid&#39;])
    if isinstance(grids_unaligned, (tuple, list)):
        alignedGrids = []
        for g in grids_unaligned:
            alignedGrid = g.rio.reproject_match(model_grid)

            try:
                no_data_val_grid = alignedGrid.attrs[&#39;_FillValue&#39;] #Extract from dataset itself
            except:
                pass
            alignedGrid = alignedGrid.where(alignedGrid != no_data_val_grid)  #Replace no data values with NaNs
            alignedGrids.append(alignedGrid)
    else:
        alignedGrid = grids_unaligned.rio.reproject_match(model_grid)

        try:
            noDataVal = alignedGrid.attrs[&#39;_FillValue&#39;] #Extract from dataset itself
        except:
            pass

        alignedGrids = alignedGrid.where(alignedGrid != noDataVal, other=np.nan)  #Replace no data values with NaNs
        
    return alignedGrids</code></pre>
</details>
</dd>
<dt id="w4h.clip_gdf2study_area"><code class="name flex">
<span>def <span class="ident">clip_gdf2study_area</span></span>(<span>study_area, gdf, log=False, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Clips dataframe to only include things within study area.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>study_area</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>Inputs study area polygon</dd>
<dt><strong><code>gdf</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>Inputs point data</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>gdfClip</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>Contains only points within the study area</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clip_gdf2study_area(study_area, gdf, log=False, verbose=False):
    &#34;&#34;&#34;Clips dataframe to only include things within study area.

    Parameters
    ----------
    study_area : geopandas.GeoDataFrame
        Inputs study area polygon
    gdf : geopandas.GeoDataFrame
        Inputs point data
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    gdfClip : geopandas.GeoDataFrame
        Contains only points within the study area
    
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if verbose:
        verbose_print(clip_gdf2study_area, locals(), exclude_params=[&#39;study_area&#39;, &#39;gdf&#39;])

    if study_area is None:
        return gdf
    else:
        studyArea_proj = study_area.to_crs(gdf.crs).copy()
        gdfClip = gpd.clip(gdf, studyArea_proj) #Easier to project just study area to ensure data fit
        gdfClip.reset_index(inplace=True, drop=True) #Reset index
    
    return gdfClip</code></pre>
</details>
</dd>
<dt id="w4h.combine_dataset"><code class="name flex">
<span>def <span class="ident">combine_dataset</span></span>(<span>layer_dataset, surface_elev, bedrock_elev, layer_thick, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to combine xarray datasets or datarrays into a single xr.Dataset. Useful to add surface, bedrock, layer thick, and layer datasets all into one variable, for pickling, for example.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>layer_dataset</code></strong> :&ensp;<code>xr.DataArray </code></dt>
<dd>DataArray contining all the interpolated layer information.</dd>
<dt><strong><code>surface_elev</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>DataArray containing surface elevation data</dd>
<dt><strong><code>bedrock_elev</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>DataArray containing bedrock elevation data</dd>
<dt><strong><code>layer_thick</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>DataArray containing the layer thickness at each point in the model grid</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>xr.Dataset</code></dt>
<dd>Dataset with all input arrays set to different variables within the dataset.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_dataset(layer_dataset, surface_elev, bedrock_elev, layer_thick, log=False):
    &#34;&#34;&#34;Function to combine xarray datasets or datarrays into a single xr.Dataset. Useful to add surface, bedrock, layer thick, and layer datasets all into one variable, for pickling, for example.

    Parameters
    ----------
    layer_dataset : xr.DataArray 
        DataArray contining all the interpolated layer information.
    surface_elev : xr.DataArray
        DataArray containing surface elevation data
    bedrock_elev : xr.DataArray
        DataArray containing bedrock elevation data
    layer_thick : xr.DataArray
        DataArray containing the layer thickness at each point in the model grid
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    xr.Dataset
        Dataset with all input arrays set to different variables within the dataset.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    
    daDict = {}
    daDict[&#39;Layers&#39;] = layer_dataset
    daDict[&#39;Surface_Elev&#39;] = surface_elev
    daDict[&#39;Bedrock_Elev&#39;] = bedrock_elev
    daDict[&#39;Layer_Thickness&#39;] = layer_thick

    combined_dataset = xr.Dataset(daDict)

    return combined_dataset</code></pre>
</details>
</dd>
<dt id="w4h.coords2geometry"><code class="name flex">
<span>def <span class="ident">coords2geometry</span></span>(<span>df_no_geometry, xcol='LONGITUDE', ycol='LATITUDE', zcol='ELEV_FT', input_coords_crs='EPSG:4269', use_z=False, wkt_col='WKT', geometry_source='coords', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds geometry to points with xy coordinates in the specified coordinate reference system.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df_no_geometry</code></strong> :&ensp;<code>pandas.Dataframe</code></dt>
<dd>a Pandas dataframe containing points</dd>
<dt><strong><code>xcol</code></strong> :&ensp;<code>str</code>, default=<code>'LONGITUDE'</code></dt>
<dd>Name of column holding x coordinate data in df_no_geometry</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default=<code>'LATITUDE'</code></dt>
<dd>Name of column holding y coordinate data in df_no_geometry</dd>
<dt><strong><code>zcol</code></strong> :&ensp;<code>str</code>, default=<code>'ELEV_FT'</code></dt>
<dd>Name of column holding z coordinate data in df_no_geometry</dd>
<dt><strong><code>input_coords_crs</code></strong> :&ensp;<code>str</code>, default=<code>'EPSG:4269</code></dt>
<dd>Name of crs used for geometry</dd>
<dt><strong><code>use_z</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to use z column in calculation</dd>
<dt><strong><code>geometry_source</code></strong> :&ensp;<code>str {'coords', 'wkt', 'geometry'}</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>gdf</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>Geopandas dataframe with points and their geometry values</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coords2geometry(df_no_geometry, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, zcol=&#39;ELEV_FT&#39;, input_coords_crs=&#39;EPSG:4269&#39;, use_z=False, wkt_col=&#39;WKT&#39;, geometry_source=&#39;coords&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Adds geometry to points with xy coordinates in the specified coordinate reference system.

    Parameters
    ----------
    df_no_geometry : pandas.Dataframe
        a Pandas dataframe containing points
    xcol : str, default=&#39;LONGITUDE&#39;
        Name of column holding x coordinate data in df_no_geometry
    ycol : str, default=&#39;LATITUDE&#39;
        Name of column holding y coordinate data in df_no_geometry
    zcol : str, default=&#39;ELEV_FT&#39;
        Name of column holding z coordinate data in df_no_geometry
    input_coords_crs : str, default=&#39;EPSG:4269
        Name of crs used for geometry
    use_z : bool, default=False
        Whether to use z column in calculation
    geometry_source : str {&#39;coords&#39;, &#39;wkt&#39;, &#39;geometry&#39;}
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    gdf : geopandas.GeoDataFrame
        Geopandas dataframe with points and their geometry values

    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if verbose:
        verbose_print(coords2geometry, locals(), exclude_params=[&#39;df_no_geometry&#39;])

    if isinstance(df_no_geometry, gpd.GeoDataFrame):
        gdf = df_no_geometry
    else:
        wktList = [&#39;wkt&#39;, &#39;well known text&#39;, &#39;wellknowntext&#39;, &#39;well_known_text&#39;, &#39;w&#39;]
        coords_list = [&#39;coords&#39;, &#39;coordinates&#39;, &#39;xycoords&#39;, &#39;xy&#39;, &#39;xyz&#39;, &#39;xyzcoords&#39;, &#39;xycoordinates&#39;, &#39;xyzcoordinates&#39;, &#39;c&#39;]
        geometryList = [&#39;geometry&#39;, &#39;geom&#39;, &#39;geo&#39;, &#39;g&#39;]
        if geometry_source.lower() in wktList:
            from shapely import wkt
            df_no_geometry[&#39;geometry&#39;] = df_no_geometry[wkt_col].apply(wkt.loads)
            df_no_geometry.drop(wkt_col, axis=1, inplace=True) #Drop WKT column

        elif geometry_source.lower() in coords_list:#coords = pd.concat([y, x], axis=1)
            x = df_no_geometry[xcol].to_numpy()
            y = df_no_geometry[ycol].to_numpy()
            if use_z:
                z = df_no_geometry[zcol].to_numpy()
                df_no_geometry[&#34;geometry&#34;] = gpd.points_from_xy(x, y, z=z, crs=input_coords_crs)
            else:
                df_no_geometry[&#34;geometry&#34;] = gpd.points_from_xy(x, y, crs=input_coords_crs)
        elif geometry_source.lower() in geometryList:
            pass
        else:
            warnings.warn(message=f&#34;&#34;&#34;The parameter geometry_source={geometry_source} is not recognized.
                        Should be one of &#39;coords&#39; (if x, y (and/or z) columns with coordintes used), 
                        &#39;wkt&#39; (if column with wkt string used), or 
                        &#39;geometry&#39; (if column with shapely geometry objects used, as with a GeoDataFrame)&#34;&#34;&#34;)
            
        gdf = gpd.GeoDataFrame(df_no_geometry, geometry=&#39;geometry&#39;, crs=input_coords_crs)
    return gdf</code></pre>
</details>
</dd>
<dt id="w4h.define_dtypes"><code class="name flex">
<span>def <span class="ident">define_dtypes</span></span>(<span>undefined_df, datatypes=None, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to define datatypes of a dataframe, especially with file-indicated dyptes</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>undefined_df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Pandas dataframe with columns whose datatypes need to be (re)defined</dd>
<dt><strong><code>datatypes</code></strong> :&ensp;<code>dict, str, pathlib.PurePath() object,</code> or <code>None</code>, default <code>= None</code></dt>
<dd>Dictionary containing datatypes, to be used in pandas.DataFrame.astype() function. If None, will read from file indicated by dtype_file (which must be defined, along with dtype_dir), by default None</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dfout</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing redefined columns</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def define_dtypes(undefined_df, datatypes=None, verbose=False, log=False):
    &#34;&#34;&#34;Function to define datatypes of a dataframe, especially with file-indicated dyptes

    Parameters
    ----------
    undefined_df : pd.DataFrame
        Pandas dataframe with columns whose datatypes need to be (re)defined
    datatypes : dict, str, pathlib.PurePath() object, or None, default = None
        Dictionary containing datatypes, to be used in pandas.DataFrame.astype() function. If None, will read from file indicated by dtype_file (which must be defined, along with dtype_dir), by default None
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    dfout : pandas.DataFrame
        Pandas dataframe containing redefined columns
    &#34;&#34;&#34;
    if verbose:
        verbose_print(define_dtypes, locals(), exclude_params=[&#39;undefined_df&#39;])
    if undefined_df is None:
        dfout = None
    else:
        logger_function(log, locals(), inspect.currentframe().f_code.co_name)

        dfout = undefined_df.copy()
        
        if isinstance(datatypes, pathlib.PurePath) or isinstance(datatypes, str):
            datatypes = pathlib.Path(datatypes)

            if not datatypes.exists():
                if verbose:
                    print(&#34;ERROR: datatypes file &#39;{}&#39; does not exist, using inferred datatypes.&#34;.format(datatypes),)
                return dfout
            elif datatypes.is_dir():
                if verbose:
                    print(&#39;ERROR: datatypes must be either dict or filepath (path to directories not allowed)&#39;)
                return dfout

            datatypes = read_dict(file=datatypes)
            dfout = dfout.astype(datatypes)

        elif isinstance(datatypes, dict):
            if verbose:
                print(&#39;datatypes is None, not updating datatypes&#39;)
            dfout = dfout.astype(datatypes)
        else:
            if verbose:
                print(&#39;ERROR: datatypes must be either dict or a filepath, not {}&#39;.format(type(datatypes)))
            return dfout
        
        #This is likely redundant
        dfcols = dfout.columns
        for i in range(0, np.shape(dfout)[1]):
            dfout.iloc[:,i] = undefined_df.iloc[:,i].astype(datatypes[dfcols[i]])

    return dfout</code></pre>
</details>
</dd>
<dt id="w4h.depth_define"><code class="name flex">
<span>def <span class="ident">depth_define</span></span>(<span>df, top_col='TOP', thresh=550.0, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to define all intervals lower than thresh as bedrock</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe to classify</dd>
<dt><strong><code>top_col</code></strong> :&ensp;<code>str</code>, default <code>= 'TOP'</code></dt>
<dd>Name of column that contains the depth information, likely of the top of the well interval, by default 'TOP'</dd>
<dt><strong><code>thresh</code></strong> :&ensp;<code>float</code>, default <code>= 550.0</code></dt>
<dd>Depth (in units used in df['top_col']) below which all intervals will be classified as bedrock, by default 550.0.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print results, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log results to log file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing intervals classified as bedrock due to depth</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def depth_define(df, top_col=&#39;TOP&#39;, thresh=550.0, verbose=False, log=False):
    &#34;&#34;&#34;Function to define all intervals lower than thresh as bedrock

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe to classify
    top_col : str, default = &#39;TOP&#39;
        Name of column that contains the depth information, likely of the top of the well interval, by default &#39;TOP&#39;
    thresh : float, default = 550.0
        Depth (in units used in df[&#39;top_col&#39;]) below which all intervals will be classified as bedrock, by default 550.0.
    verbose : bool, default = False
        Whether to print results, by default False
    log : bool, default = True
        Whether to log results to log file

    Returns
    -------
    df : pandas.DataFrame
        Dataframe containing intervals classified as bedrock due to depth
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(depth_define, locals(), exclude_params=[&#39;df&#39;])
    df = df.copy()
    df[&#39;CLASS_FLAG&#39;].mask(df[top_col]&gt;thresh, 3 ,inplace=True) #Add a Classification Flag of 3 (bedrock b/c it&#39;s deepter than 550&#39;) to all records where the top of the interval is &gt;550&#39;
    df[&#39;BEDROCK_FLAG&#39;].mask(df[top_col]&gt;thresh, True, inplace=True)

    if verbose:
        if df.CLASS_FLAG.notnull().sum() == 0:
            brDepthClass = 0
        else:
            brDepthClass = df[&#39;CLASS_FLAG&#39;].value_counts()[3.0]
        total = df.shape[0]

        numRecsClass = int(df[df[&#39;CLASS_FLAG&#39;]==3][&#39;CLASS_FLAG&#39;].sum())
        percRecsClass= round((df[df[&#39;CLASS_FLAG&#39;]==3][&#39;CLASS_FLAG&#39;].sum()/df.shape[0])*100,2)
        recsRemainig = int(df.shape[0]-numRecsClass)

        print(&#39;Classified bedrock well records using depth threshold at depth of {}&#39;.format(thresh))
        print(&#34;\t{} records classified using bedrock threshold depth ({}% of unclassified  data)&#34;.format(numRecsClass, percRecsClass))
        print(&#39;\t{} records remain unclassified ({}% of unclassified  data).&#39;.format(recsRemainig, 1-percRecsClass))
        
    return df</code></pre>
</details>
</dd>
<dt id="w4h.export_dataframe"><code class="name flex">
<span>def <span class="ident">export_dataframe</span></span>(<span>df, out_dir, filename, date_stamp=True, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to export dataframes</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas dataframe,</code> or <code>list</code> of <code>pandas dataframes</code></dt>
<dd>Data frame or list of dataframes to be exported</dd>
<dt><strong><code>out_dir</code></strong> :&ensp;<code>string</code> or <code>pathlib.Path object</code></dt>
<dd>Directory to which to export dataframe object(s) as .csv</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code> or <code>list</code> of <code>strings</code></dt>
<dd>Filename(s) of output files</dd>
<dt><strong><code>date_stamp</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to include a datestamp in the filename. If true, file ends with _yyyy-mm-dd.csv of current date, by default True.</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_dataframe(df, out_dir, filename, date_stamp=True, log=False):
    &#34;&#34;&#34;Function to export dataframes

    Parameters
    ----------
    df : pandas dataframe, or list of pandas dataframes
        Data frame or list of dataframes to be exported
    out_dir : string or pathlib.Path object
        Directory to which to export dataframe object(s) as .csv
    filename : str or list of strings
        Filename(s) of output files
    date_stamp : bool, default=True
        Whether to include a datestamp in the filename. If true, file ends with _yyyy-mm-dd.csv of current date, by default True.
    log : bool, default = True
        Whether to log inputs and outputs to log file.        
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if date_stamp:
        nowTime = datetime.datetime.now()
        nowTime = str(nowTime).replace(&#39;:&#39;, &#39;-&#39;).replace(&#39; &#39;,&#39;_&#39;).split(&#39;.&#39;)[0]
        nowTimeStr = &#39;_&#39;+str(nowTime)
    else:
        nowTimeStr=&#39;&#39;

    if type(out_dir) is str or isinstance(out_dir, pathlib.PurePath):
        out_dir = str(out_dir)
        out_dir = out_dir.replace(&#39;\\&#39;, &#39;/&#39;).replace(&#39;\\&#39;[-1], &#39;/&#39;)
        if out_dir[-1] != &#39;/&#39;:
            out_dir = out_dir + &#39;/&#39;
    else:
        print(&#39;Please input string or pathlib object for out_dir parameters&#39;)
        return

    if type(filename) is str:
        dfOutFile =  out_dir+filename+nowTimeStr+&#39;.csv&#39;
        df.to_csv(dfOutFile, index_label=&#39;ID&#39;)
        print(&#39;Exported &#39;+filename+nowTimeStr+&#39;.csv&#39;)
    elif type(filename) is list and type(df) is list and len(df) == len(filename):
        for i, f in enumerate(df):
            fname = filename[i]
            dfOutFile =  out_dir+fname+nowTimeStr+&#39;.csv&#39;
            f.to_csv(dfOutFile, index_label=&#39;ID&#39;)
            print(&#39;Exported &#39;+fname+nowTimeStr+&#39;.csv&#39;)</code></pre>
</details>
</dd>
<dt id="w4h.export_grids"><code class="name flex">
<span>def <span class="ident">export_grids</span></span>(<span>grid_data, out_path, file_id='', filetype='tif', variable_sep=True, date_stamp=True, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to export grids to files.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>grid_data</code></strong> :&ensp;<code>xarray DataArray</code> or <code>xarray Dataset</code></dt>
<dd>Dataset or dataarray to be exported</dd>
<dt><strong><code>out_path</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>Output location for data export. If variable_sep=True, this should be a directory. Otherwise, this should also include the filename. The file extension should not be included here.</dd>
<dt><strong><code>file_id</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>If specified, will add this after 'LayerXX' or 'AllLayers' in the filename, just before datestamp, if used. Example filename for file_id='Coarse': Layer1_Coarse_2023-04-18.tif.</dd>
<dt><strong><code>filetype</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Output filetype. Can either be pickle or any file extension supported by rioxarray.rio.to_raster(). Can either include period or not., by default 'tif'</dd>
<dt><strong><code>variable_sep</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If grid_data is an xarray Dataset, this will export each variable in the dataset as a separate file, including the variable name in the filename, by default False</dd>
<dt><strong><code>date_stamp</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to include a date stamp in the file name., by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_grids(grid_data, out_path, file_id=&#39;&#39;,filetype=&#39;tif&#39;, variable_sep=True, date_stamp=True, verbose=False, log=False):
    &#34;&#34;&#34;Function to export grids to files.

    Parameters
    ----------
    grid_data : xarray DataArray or xarray Dataset
        Dataset or dataarray to be exported
    out_path : str or pathlib.Path object
        Output location for data export. If variable_sep=True, this should be a directory. Otherwise, this should also include the filename. The file extension should not be included here.
    file_id : str, optional
        If specified, will add this after &#39;LayerXX&#39; or &#39;AllLayers&#39; in the filename, just before datestamp, if used. Example filename for file_id=&#39;Coarse&#39;: Layer1_Coarse_2023-04-18.tif.
    filetype : str, optional
        Output filetype. Can either be pickle or any file extension supported by rioxarray.rio.to_raster(). Can either include period or not., by default &#39;tif&#39;
    variable_sep : bool, optional
        If grid_data is an xarray Dataset, this will export each variable in the dataset as a separate file, including the variable name in the filename, by default False
    date_stamp : bool, optional
        Whether to include a date stamp in the file name., by default True
    log : bool, default = True
        Whether to log inputs and outputs to log file.        
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(export_grids, locals(), exclude_params=[&#39;grid_data&#39;])
    #Initialize lists to determine which filetype will be used for export
    ncdfList = [&#39;netcdf&#39;, &#39;ncdf&#39;, &#39;n&#39;]
    tifList = [&#39;tif&#39;, &#39;tiff&#39;, &#39;geotiff&#39;, &#39;geotif&#39;, &#39;t&#39;]
    pickleList = [&#39;pickle&#39;, &#39;pkl&#39;, &#39;p&#39;]

    filenames = []

    #Format output string(s)
    #Format output filepath
    if isinstance(out_path, (pathlib.PurePath, str)):
        if isinstance(out_path, pathlib.PurePath):
            pass
        else:
            out_path = pathlib.Path(out_path)
            
        if out_path.parent.exists() == False:
            print(&#39;Directory does not exist. Please enter a different value for the out_path parameter.&#39;)
            return        

        if out_path.is_dir():
            if isinstance(grid_data, xr.DataArray):
                if variable_sep:
                    lyrs = grid_data.coords[&#39;Layer&#39;].values
                    filenames = []
                    for l in lyrs:
                        filenames.append(&#39;Layer&#39;+str(l))
                else:
                    filenames = [&#39;AllLayers&#39;]
            if isinstance(grid_data, xr.Dataset):
                if variable_sep:
                    filenames = []
                    for var in grid_data:
                        filenames.append(var)
                else:
                    filenames = [&#39;AllLayers&#39;]    
        else:
            filenames = [out_path.stem]
            out_path = out_path.parent
    else:
        print(&#39;No output path specified (out_path). Please input string or pathlib object for out_path parameters&#39;)
        return
    
    #Format datestamp, if desired in output filename
    if date_stamp:
        nowTime = datetime.datetime.now()
        nowTime = str(nowTime).replace(&#39;:&#39;, &#39;-&#39;).replace(&#39; &#39;,&#39;_&#39;).split(&#39;.&#39;)[0]
        nowTimeStr = &#39;_&#39;+str(nowTime)
    else:
        nowTimeStr=&#39;&#39;

    #Ensure the file suffix includes .
    if filetype[0] == &#39;.&#39;:
        pass
    else:
        filetype = &#39;.&#39; + filetype

    if file_id != &#39;&#39;:
        file_id = &#39;_&#39;+file_id

    out_path = out_path.as_posix()+&#39;/&#39;
    
    if verbose:
        print(&#39;Export filepath(s):&#39;)
        
    outPaths = []
    for f in filenames:
        currOutPath = out_path+f+file_id+nowTimeStr+filetype
        outPaths.append(currOutPath)
        if verbose:
            print(&#39;\t {}&#39;.format(currOutPath))
        
    #Do export
    if filetype.lower() in pickleList:
        import pickle
        for op in outPaths:
            try:
                with open(op, &#39;wb&#39;) as f:
                    pickle.dump(grid_data, f)
            except:
                print(&#39;An error occured during export.&#39;)
                print(op, &#39;could not be exported as a pickle object.&#39;)
                print(&#39;Try again using different parameters.&#39;)
    else:
        import rioxarray as rxr
        try:
            if isinstance(grid_data, xr.Dataset):
                if variable_sep:
                    for i, var in enumerate(grid_data.data_vars):
                        grid_data[var].rio.to_raster(outPaths[i])
                else:
                    grid_data.rio.to_raster(outPaths[0])
            elif isinstance(grid_data, xr.DataArray):
                if variable_sep:
                    lyrs = grid_data.coords[&#39;Layer&#39;].values
                    for i, l in enumerate(lyrs):
                        out_grid = grid_data.sel(Layer = l).copy()
                        out_grid.rio.to_raster(outPaths[i])
                else:
                    grid_data.rio.to_raster(outPaths[0])
            else:
                grid_data.rio.to_raster(outPaths[0])
        except:
            print(&#39;An error occured during export.&#39;)
            print(&#39;{} could not be exported as {} file.&#39;.format(outPaths, filetype))
            print(&#39;Try again using different parameters.&#39;)

    return</code></pre>
</details>
</dd>
<dt id="w4h.export_undefined"><code class="name flex">
<span>def <span class="ident">export_undefined</span></span>(<span>df, outdir)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to export terms that still need to be defined.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing at least some unclassified data</dd>
<dt><strong><code>outdir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code></dt>
<dd>Directory to save file. Filename will be generated automatically based on today's date.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>stillNeededDF</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing only unclassified terms, and the number of times they occur</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_undefined(df, outdir):
    &#34;&#34;&#34;Function to export terms that still need to be defined.

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe containing at least some unclassified data
    outdir : str or pathlib.Path
        Directory to save file. Filename will be generated automatically based on today&#39;s date.

    Returns
    -------
    stillNeededDF : pandas.DataFrame
        Dataframe containing only unclassified terms, and the number of times they occur
    &#34;&#34;&#34;
    import pathlib
    
    
    if isinstance(outdir, pathlib.PurePath):
        if not outdir.is_dir() or not outdir.exists():
            print(&#39;Please specify a valid directory for export. Filename is generated automatically.&#39;)
            return
        outdir = outdir.as_posix()
    else:
        outdir.replace(&#39;\\&#39;,&#39;/&#39;)
        outdir.replace(&#39;\\&#39;[-1], &#39;/&#39;)

    #Get directory path correct        
    if outdir[-1] != &#39;/&#39;:
        outdir = outdir+&#39;/&#39;

    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)
    searchDF = df[df[&#39;CLASS_FLAG&#39;].isna()]
    
    stillNeededDF=searchDF[&#39;FORMATION&#39;].value_counts()
    stillNeededDF.to_csv(outdir+&#39;Undefined_&#39;+todayDateStr+&#39;.csv&#39;)
    return stillNeededDF</code></pre>
</details>
</dd>
<dt id="w4h.file_setup"><code class="name flex">
<span>def <span class="ident">file_setup</span></span>(<span>well_data, metadata=None, data_filename='*ISGS_DOWNHOLE_DATA*.txt', metadata_filename='*ISGS_HEADER*.txt', log_dir=None, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to setup files, assuming data, metadata, and elevation/location are in separate files (there should be one "key"/identifying column consistent across all files to join/merge them later)</p>
<p>This function may not be useful if files are organized differently than this structure.
If that is the case, it is recommended to use the get_most_recent() function for each individual file if needed.
It may also be of use to simply skip this function altogether and directly define each filepath in a manner that can be used by pandas.read_csv()</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>well_data</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>Str or pathlib.Path to directory containing input files, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, optional</dt>
<dd>Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>data_filename</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Pattern used by pathlib.glob() to get the most recent data file, by default '<em>ISGS_DOWNHOLE_DATA</em>.txt'</dd>
<dt><strong><code>metadata_filename</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Pattern used by pathlib.glob() to get the most recent metadata file, by default '<em>ISGS_HEADER</em>.txt'</dd>
<dt><strong><code>log_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.PurePath()</code> or <code>None</code>, default=<code>None</code></dt>
<dd>Directory to place log file in. This is not read directly, but is used indirectly by w4h.logger_function()</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print name of files to terminal, by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Tuple with paths to (well_data, metadata)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def file_setup(well_data, metadata=None, data_filename=&#39;*ISGS_DOWNHOLE_DATA*.txt&#39;, metadata_filename=&#39;*ISGS_HEADER*.txt&#39;, log_dir=None, verbose=False, log=False):
    &#34;&#34;&#34;Function to setup files, assuming data, metadata, and elevation/location are in separate files (there should be one &#34;key&#34;/identifying column consistent across all files to join/merge them later)

    This function may not be useful if files are organized differently than this structure. 
    If that is the case, it is recommended to use the get_most_recent() function for each individual file if needed.
    It may also be of use to simply skip this function altogether and directly define each filepath in a manner that can be used by pandas.read_csv()

    Parameters
    ----------
    well_data : str or pathlib.Path object
        Str or pathlib.Path to directory containing input files, by default str(repoDir)+&#39;/resources&#39;
    metadata : str or pathlib.Path object, optional
        Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+&#39;/resources&#39;
    data_filename : str, optional
        Pattern used by pathlib.glob() to get the most recent data file, by default &#39;*ISGS_DOWNHOLE_DATA*.txt&#39;
    metadata_filename : str, optional
        Pattern used by pathlib.glob() to get the most recent metadata file, by default &#39;*ISGS_HEADER*.txt&#39;
    log_dir : str or pathlib.PurePath() or None, default=None
        Directory to place log file in. This is not read directly, but is used indirectly by w4h.logger_function()
    verbose : bool, default = False
        Whether to print name of files to terminal, by default True
    log : bool, default = True
        Whether to log inputs and outputs to log file.

    Returns
    -------
    tuple
        Tuple with paths to (well_data, metadata)
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(file_setup, locals(), exclude_params=[&#39;well_data&#39;])
    #Define  filepath variables to be used later for reading/writing files
    data_path = pathlib.Path(well_data)
    if metadata is None:
        origMetaPath = None
        metadata=data_path
    else:
        origMetaPath = metadata
        metadata=pathlib.Path(metadata)

    #If input path is a directory, find most recent version of the file. If file, just read the file
    if data_path.is_dir():
        downholeDataFILE = get_most_recent(data_path, data_filename, verbose=verbose)
    else:
        downholeDataFILE = data_path
    
    if metadata.is_dir():
        headerDataFILE = get_most_recent(metadata, metadata_filename, verbose=verbose)
        if headerDataFILE == []:
            headerDataFILE = downholeDataFILE
    else:
        if origMetaPath is None:
            headerDataFILE = downholeDataFILE
        else:
            headerDataFILE = metadata
       #Set all input as pathlib.Path objects (may be redundant, but just in case)
    downholeDataPATH = pathlib.Path(downholeDataFILE)
    headerDataPATH = pathlib.Path(headerDataFILE)

    if verbose:
        print(&#39;Using the following file(s):&#39;)
        print(&#39;\t&#39;, downholeDataFILE)
        if headerDataFILE != downholeDataFILE:
            print(&#39;\t&#39;, headerDataFILE)

    #Define datatypes, to use later
    #downholeDataDTYPES = {&#39;ID&#39;:np.uint32, &#34;API_NUMBER&#34;:np.uint64,&#34;TABLE_NAME&#34;:str,&#34;WHO&#34;:str,&#34;INTERPRET_DATE&#34;:str,&#34;FORMATION&#34;:str,&#34;THICKNESS&#34;:np.float64,&#34;TOP&#34;:np.float64,&#34;BOTTOM&#34;:np.float64}
    #headerDataDTYPES = {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#34;TDFORMATION&#34;:str,&#34;PRODFORM&#34;:str,&#34;TOTAL_DEPTH&#34;:np.float64,&#34;SECTION&#34;:np.float64,&#34;TWP&#34;:np.float64,&#34;TDIR&#34;:str,&#34;RNG&#34;:np.float64,&#34;RDIR&#34;:str,&#34;MERIDIAN&#34;:np.float64,&#34;FARM_NAME&#34;:str,&#34;NSFOOT&#34;:np.float64,&#34;NSDIR&#34;:str,&#34;EWFOOT&#34;:np.float64,&#34;EWDIR&#34;:str,&#34;QUARTERS&#34;:str,&#34;ELEVATION&#34;:np.float64,&#34;ELEVREF&#34;:str,&#34;COMP_DATE&#34;:str,&#34;STATUS&#34;:str,&#34;FARM_NUM&#34;:str,&#34;COUNTY_CODE&#34;:np.float64,&#34;PERMIT_NUMBER&#34;:str,&#34;COMPANY_NAME&#34;:str,&#34;COMPANY_CODE&#34;:str,&#34;PERMIT_DATE&#34;:str,&#34;CORNER&#34;:str,&#34;LATITUDE&#34;:np.float64,&#34;LONGITUDE&#34;:np.float64,&#34;ENTERED_BY&#34;:str,&#34;UPDDATE&#34;:str,&#34;ELEVSOURCE&#34;:str, &#34;ELEV_FT&#34;:np.float64}
    return downholeDataPATH, headerDataPATH</code></pre>
</details>
</dd>
<dt id="w4h.fill_unclassified"><code class="name flex">
<span>def <span class="ident">fill_unclassified</span></span>(<span>df, classification_col='CLASS_FLAG')</span>
</code></dt>
<dd>
<div class="desc"><p>Fills unclassified rows in 'CLASS_FLAG' column with np.nan</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe on which to perform operation</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe on which operation has been performed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fill_unclassified(df, classification_col=&#39;CLASS_FLAG&#39;):
    &#34;&#34;&#34;Fills unclassified rows in &#39;CLASS_FLAG&#39; column with np.nan

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe on which to perform operation

    Returns
    -------
    df : pandas.DataFrame
        Dataframe on which operation has been performed
    &#34;&#34;&#34;
    df[classification_col].fillna(0, inplace=True)
    return df</code></pre>
</details>
</dd>
<dt id="w4h.get_current_date"><code class="name flex">
<span>def <span class="ident">get_current_date</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="gets-the-current-date-to-help-with-finding-the-most-recent-file">Gets The Current Date To Help With Finding The Most Recent File</h2>
<h2 id="parameters">Parameters</h2>
<p>None</p>
<hr>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>todayDate
</code></dt>
<dd>datetime object with today's date</dd>
<dt><code>dateSuffix
</code></dt>
<dd>str to use for naming output files</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_current_date():
    &#34;&#34;&#34; Gets the current date to help with finding the most recent file
        ---------------------
        Parameters:
            None

        ---------------------
        Returns:
            todayDate   : datetime object with today&#39;s date
            dateSuffix  : str to use for naming output files
    &#34;&#34;&#34;
    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)
    dateSuffix = &#39;_&#39;+todayDateStr
    return todayDate, dateSuffix</code></pre>
</details>
</dd>
<dt id="w4h.get_drift_thick"><code class="name flex">
<span>def <span class="ident">get_drift_thick</span></span>(<span>surface_elev=None, bedrock_elev=None, layers=9, plot=False, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the distance from surface_elev to bedrock_elev and then divides by number of layers to get layer thickness.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>surface_elev</code></strong> :&ensp;<code>rioxarray.DataArray</code></dt>
<dd>array holding surface elevation</dd>
<dt><strong><code>bedrock_elev</code></strong> :&ensp;<code>rioxarray.DataArray</code></dt>
<dd>array holding bedrock elevation</dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>int</code>, default=<code>9</code></dt>
<dd>number of layers needed to calculate thickness for</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>tells function to either plot the data or not</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>driftThick</code></strong> :&ensp;<code>rioxarray.DataArray</code></dt>
<dd>Contains data array containing depth to bedrock at each point</dd>
<dt><strong><code>layerThick</code></strong> :&ensp;<code>rioxarray.DataArray</code></dt>
<dd>Contains data array with layer thickness at each point</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_drift_thick(surface_elev=None, bedrock_elev=None, layers=9, plot=False, verbose=False, log=False):
    &#34;&#34;&#34;Finds the distance from surface_elev to bedrock_elev and then divides by number of layers to get layer thickness.

    Parameters
    ----------
    surface_elev : rioxarray.DataArray
        array holding surface elevation
    bedrock_elev : rioxarray.DataArray
        array holding bedrock elevation
    layers : int, default=9
        number of layers needed to calculate thickness for
    plot : bool, default=False
        tells function to either plot the data or not

    Returns
    -------
    driftThick : rioxarray.DataArray
        Contains data array containing depth to bedrock at each point
    layerThick : rioxarray.DataArray
        Contains data array with layer thickness at each point

    &#34;&#34;&#34;
    
    if surface_elev is None:
        surface_elev = w4h.get_resources()[&#39;surf_elev&#39;]
    if bedrock_elev is None:
        bedrock_elev = w4h.get_resources()[&#39;bedrock_elev&#39;]
    
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(get_drift_thick, locals(), exclude_params=[&#39;surface_elev&#39;, &#39;bedrock_elev&#39;])
    xr.set_options(keep_attrs=True)

    driftThick = surface_elev - bedrock_elev
    driftThick = driftThick.clip(0,max=5000,keep_attrs=True)
    if plot:
        import matplotlib.pyplot as plt
        fig, ax = plt.subplots()
        maxThick = np.nanmax(driftThick)
        if maxThick &gt; 550:
            maxThick = 550
        dtPlot = driftThick.plot(vmin=0, vmax=maxThick, ax=ax)
        ax.set_title(&#34;Drift Thickness&#34;)
    try:
        noDataVal = driftThick.attrs[&#39;_FillValue&#39;] #Extract from dataset itself
    except:
        noDataVal = 100001
    
    driftThick = driftThick.where(driftThick &lt;100000, other=np.nan)  #Replace no data values with NaNs
    driftThick = driftThick.where(driftThick &gt;-100000, other=np.nan)  #Replace no data values with NaNs

    layerThick = driftThick/layers
    
    xr.set_options(keep_attrs=&#39;default&#39;)

    return driftThick, layerThick</code></pre>
</details>
</dd>
<dt id="w4h.get_layer_depths"><code class="name flex">
<span>def <span class="ident">get_layer_depths</span></span>(<span>df_with_depths, surface_elev_col='SURFACE_ELEV', layer_thick_col='LAYER_THICK', layers=9, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to calculate depths and elevations of each model layer at each well based on surface elevation, bedrock elevation, and number of layers/layer thickness</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df_with_depths</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing well metdata</dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>int</code>, default=<code>9</code></dt>
<dd>Number of layers. This should correlate with get_drift_thick() input parameter, if drift thickness was calculated using that function, by default 9.</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Dataframe</code></dt>
<dd>Dataframe containing new columns for depth to layers and elevation of layers.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_layer_depths(df_with_depths, surface_elev_col=&#39;SURFACE_ELEV&#39;, layer_thick_col=&#39;LAYER_THICK&#39;, layers=9, log=False):
    &#34;&#34;&#34;Function to calculate depths and elevations of each model layer at each well based on surface elevation, bedrock elevation, and number of layers/layer thickness

    Parameters
    ----------
    df_with_depths : pandas.DataFrame
        Dataframe containing well metdata
    layers : int, default=9
        Number of layers. This should correlate with get_drift_thick() input parameter, if drift thickness was calculated using that function, by default 9.
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    pandas.Dataframe
        Dataframe containing new columns for depth to layers and elevation of layers.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    for layer in range(0, layers): #For each layer
        #Make column names
        depthColName  = &#39;DEPTH_LAYER&#39;+str(layer+1)
        #depthMcolName = &#39;Depth_M_LAYER&#39;+str(layer) 

        #Calculate depth to each layer at each well, in feet and meters
        df_with_depths[depthColName]  = df_with_depths[layer_thick_col] * layer
        #headerData[depthMcolName] = headerData[depthColName] * 0.3048

    for layer in range(0, layers): #For each layer
        elevColName = &#39;ELEV_LAYER&#39;+str(layer+1)
        #elevMColName = &#39;ELEV_M_LAYER&#39;+str(layer)
            
        df_with_depths[elevColName]  = df_with_depths[surface_elev_col] - df_with_depths[layer_thick_col] * layer
        #headerData[elevMColName]  = headerData[&#39;SURFACE_ELEV_M&#39;] - headerData[&#39;LAYER_THICK_M&#39;] * layer
        
    return df_with_depths</code></pre>
</details>
</dd>
<dt id="w4h.get_most_recent"><code class="name flex">
<span>def <span class="ident">get_most_recent</span></span>(<span>dir=WindowsPath('c:/Users/riley/LocalData/Github/wells4hydrogeology/w4h/resources'), glob_pattern='*', verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to find the most recent file with the indicated pattern, using pathlib.glob function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, optional</dt>
<dd>Directory in which to find the most recent file, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>glob_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>String used by the pathlib.glob() function/method for searching, by default '*'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pathlib.Path object</code></dt>
<dd>Pathlib Path object of the most recent file fitting the glob pattern indicated in the glob_pattern parameter.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_most_recent(dir=resource_dir, glob_pattern=&#39;*&#39;, verbose=True):
    &#34;&#34;&#34;Function to find the most recent file with the indicated pattern, using pathlib.glob function.

    Parameters
    ----------
    dir : str or pathlib.Path object, optional
        Directory in which to find the most recent file, by default str(repoDir)+&#39;/resources&#39;
    glob_pattern : str, optional
        String used by the pathlib.glob() function/method for searching, by default &#39;*&#39;

    Returns
    -------
    pathlib.Path object
        Pathlib Path object of the most recent file fitting the glob pattern indicated in the glob_pattern parameter.
    &#34;&#34;&#34;
    if verbose:
        verbose_print(get_most_recent, locals())

    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)

    files = pathlib.Path(dir).rglob(glob_pattern) #Get all the files that fit the pattern
    fileDates = []
    for f in files: #Get the file dates from their file modification times
        fileDates.append(np.datetime64(datetime.datetime.fromtimestamp(os.path.getmtime(f))))
    
    if fileDates == []:
        #If no files found that match pattern, return an empty pathlib.Path()
        if verbose:
            print(&#39;No file found in {} matching {} pattern&#39;.format(dir, glob_pattern))
        mostRecentFile = pathlib.Path()
        return mostRecentFile
    else:
        globInd = np.argmin(np.datetime64(todayDateStr) - np.array(fileDates)) #Find the index of the most recent file

    #Iterate through glob/files again (need to recreate glob)
    files = pathlib.Path(dir).rglob(glob_pattern)
    for j, f in enumerate(files):
        if j == globInd:
            mostRecentFile=f
            break
    
    if verbose:
        print(&#39;Most Recent version of file fitting {} pattern is: {}&#39;.format(glob_pattern, mostRecentFile.name))

    return mostRecentFile</code></pre>
</details>
</dd>
<dt id="w4h.get_resources"><code class="name flex">
<span>def <span class="ident">get_resources</span></span>(<span>resource_type='filepaths', scope='local', verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to get filepaths for resources included with package</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>resource_type</code></strong> :&ensp;<code>str, {'filepaths', 'data'}</code></dt>
<dd>If filepaths, will return dictionary with filepaths to sample data. If data, returns dictionary with data objects.</dd>
<dt><strong><code>scope</code></strong> :&ensp;<code>str, {'local', 'statewide'}</code></dt>
<dd>If 'local', will read in sample data for a local (around county sized) project. If 'state', will read in sample data for a statewide project (Illinois)</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to print results to terminal, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>resources_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary containing key, value pairs with filepaths to resources that may be of interest.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_resources(resource_type=&#39;filepaths&#39;, scope=&#39;local&#39;, verbose=False):
    &#34;&#34;&#34;Function to get filepaths for resources included with package

    Parameters
    ----------
    resource_type : str, {&#39;filepaths&#39;, &#39;data&#39;}
        If filepaths, will return dictionary with filepaths to sample data. If data, returns dictionary with data objects.
    scope : str, {&#39;local&#39;, &#39;statewide&#39;}
        If &#39;local&#39;, will read in sample data for a local (around county sized) project. If &#39;state&#39;, will read in sample data for a statewide project (Illinois)
    verbose : bool, optional
        Whether to print results to terminal, by default False

    Returns
    -------
    resources_dict : dict
        Dictionary containing key, value pairs with filepaths to resources that may be of interest.
    &#34;&#34;&#34;
    resources_dict = {}
    sample_data_dir = resource_dir.joinpath(&#39;sample_data&#39;)

    #Get sample data
    #Get lithology dictionaries&#39; filepaths
    sample_dictionary_dir = sample_data_dir.joinpath(&#39;DictionaryTerms&#39;)
    resources_dict[&#39;LithologyDict_Exact&#39;] = w4h.get_most_recent(dir=sample_dictionary_dir, glob_pattern=&#39;*DICTIONARY_SearchTerms*&#39;, verbose=verbose)
    resources_dict[&#39;LithologyDict_Start&#39;] = w4h.get_most_recent(dir=sample_dictionary_dir, glob_pattern=&#39;*SearchTerms-Start*&#39;, verbose=verbose)
    resources_dict[&#39;LithologyDict_Wildcard&#39;] = w4h.get_most_recent(dir=sample_dictionary_dir, glob_pattern=&#39;*SearchTerms-Wildcard*&#39;, verbose=verbose)

    #Get Lithology Interpretation filepaths
    lith_interp_dir = sample_data_dir.joinpath(&#39;LithologyInterpretations&#39;)
    resources_dict[&#39;LithInterps_FineCoarse&#39;] = w4h.get_most_recent(dir=lith_interp_dir, glob_pattern=&#39;*FineCoarse*&#39;, verbose=verbose)
    resources_dict[&#39;LithInterps_Clay&#39;] = w4h.get_most_recent(dir=lith_interp_dir, glob_pattern=&#39;*Clay*&#39;, verbose=verbose)
    resources_dict[&#39;LithInterps_Silt&#39;] = w4h.get_most_recent(dir=lith_interp_dir, glob_pattern=&#39;*Silt*&#39;, verbose=verbose)    
    resources_dict[&#39;LithInterps_Sand&#39;] = w4h.get_most_recent(dir=lith_interp_dir, glob_pattern=&#39;*Sand*&#39;, verbose=verbose)    
    resources_dict[&#39;LithInterps_Gravel&#39;] = w4h.get_most_recent(dir=lith_interp_dir, glob_pattern=&#39;*Gravel*&#39;, verbose=verbose)    

    #Get other resource filepaths
    resources_dict[&#39;well_data_dtypes&#39;] = w4h.get_most_recent(dir=sample_data_dir, glob_pattern=&#39;*downholeDataTypes*&#39;, verbose=verbose)
    resources_dict[&#39;metadata_dtypes&#39;] = w4h.get_most_recent(dir=sample_data_dir, glob_pattern=&#39;*headerDataTypes*&#39;, verbose=verbose)
    resources_dict[&#39;ISWS_CRS&#39;] = w4h.get_most_recent(dir=sample_data_dir, glob_pattern=&#39;isws_crs.json&#39;, verbose=verbose)
    resources_dict[&#39;xyz_dtypes&#39;] = w4h.get_most_recent(dir=sample_data_dir, glob_pattern=&#39;xyzDataTypes.json&#39;, verbose=verbose)

    resources_dict[&#39;model_grid&#39;] = w4h.get_most_recent(dir=sample_data_dir, glob_pattern=&#39;grid_625_raster.tif&#39;, verbose=verbose)

    statewideSampleDir = sample_data_dir.joinpath(&#39;statewide_sample_data&#39;)
    statewideList = [&#39;statewide&#39;, &#39;state&#39;, &#39;regional&#39;, &#39;region&#39;, &#39;s&#39;, &#39;r&#39;]
    if scope.lower() in statewideList:
        resources_dict[&#39;well_data&#39;] = statewideSampleDir.joinpath(&#34;IL_Statewide_WellData_XYz_2023-07-20_cleaned.zip&#34;)

        resources_dict[&#39;surf_elev&#39;] = w4h.get_most_recent(dir=statewideSampleDir, glob_pattern=&#39;*IL_Statewide_Surface_Elev_ft_625ft_Lambert_GridAlign*&#39;, verbose=verbose)
        resources_dict[&#39;bedrock_elev&#39;] = w4h.get_most_recent(dir=statewideSampleDir, glob_pattern=&#39;*IL_Statewide_Bedrock_Elev_2023_ft_625ft_Lambert_GridAlign*&#39;, verbose=verbose)
        resources_dict[&#39;study_area&#39;] = w4h.get_most_recent(dir=statewideSampleDir, glob_pattern=&#39;*IL_Statewide_boundary*&#39;, verbose=verbose)
    else:
        resources_dict[&#39;study_area&#39;] = w4h.get_most_recent(dir=sample_data_dir, glob_pattern=&#39;*sample_studyArea*&#39;, verbose=verbose)
        resources_dict[&#39;surf_elev&#39;] = w4h.get_most_recent(dir=sample_data_dir, glob_pattern=&#39;*sample_surface_bedrock_lidarresampled100ft*&#39;, verbose=verbose)
        resources_dict[&#39;bedrock_elev&#39;] = w4h.get_most_recent(dir=sample_data_dir, glob_pattern=&#39;*LocalSample_Bedrock_elev_EStLGrimleyPhillips*&#39;, verbose=verbose)

        resources_dict[&#39;well_data&#39;] = w4h.get_most_recent(dir=sample_data_dir, glob_pattern=&#39;sample_well_data*&#39;, verbose=verbose)

    # Get data objects if specified
    dataObjList = [&#39;data&#39;, &#39;objects&#39;, &#39;do&#39;, &#39;data objects&#39;, &#39;dataobjects&#39;]
    if resource_type.lower() in dataObjList:
        resources_dict[&#39;LithologyDict_Exact&#39;] = pd.read_csv(resources_dict[&#39;LithologyDict_Exact&#39;], 
                                                            dtype={&#34;ID&#34;:int, &#34;DESCRIPTION&#34;:str, &#34;LITHOLOGY&#34;:str,
                                                            &#34;COLOR&#34;:str, &#34;CONSISTENCY&#34;:str, &#34;MOD1&#34;:str, &#34;MOD2&#34;:str,
                                                            &#34;INTERPRETED&#34;:str, &#34;COMPLETED&#34;:str, &#34;ORIGIN_INDIANA&#34;:str},
                                                            index_col=&#39;ID&#39;)
        resources_dict[&#39;LithologyDict_Start&#39;] = pd.read_csv(resources_dict[&#39;LithologyDict_Start&#39;])
        resources_dict[&#39;LithologyDict_Wildcard&#39;] = pd.read_csv(resources_dict[&#39;LithologyDict_Wildcard&#39;])

        resources_dict[&#39;LithInterps_FineCoarse&#39;] = pd.read_csv(resources_dict[&#39;LithInterps_FineCoarse&#39;])
        resources_dict[&#39;LithInterps_Clay&#39;] = pd.read_csv(resources_dict[&#39;LithInterps_Clay&#39;])
        resources_dict[&#39;LithInterps_Silt&#39;] = pd.read_csv(resources_dict[&#39;LithInterps_Silt&#39;])
        resources_dict[&#39;LithInterps_Sand&#39;] = pd.read_csv(resources_dict[&#39;LithInterps_Sand&#39;])
        resources_dict[&#39;LithInterps_Gravel&#39;] = pd.read_csv(resources_dict[&#39;LithInterps_Gravel&#39;])

        
        with open(resources_dict[&#39;well_data_dtypes&#39;], &#39;r&#39;, encoding=&#39;utf-8&#39;) as f:
            resources_dict[&#39;well_data_dtypes&#39;] = json.load(f)

        with open(resources_dict[&#39;metadata_dtypes&#39;], &#39;r&#39;, encoding=&#39;utf-8&#39;) as f:
            resources_dict[&#39;metadata_dtypes&#39;] = json.load(f)            

        with open(resources_dict[&#39;ISWS_CRS&#39;], &#39;r&#39;, encoding=&#39;utf-8&#39;) as f:
            resources_dict[&#39;ISWS_CRS&#39;] = json.load(f)
        
        with open(resources_dict[&#39;xyz_dtypes&#39;], &#39;r&#39;, encoding=&#39;utf-8&#39;) as f:
            resources_dict[&#39;xyz_dtypes&#39;] = json.load(f)


        if scope.lower() in statewideList:
            sacrs = resources_dict[&#39;ISWS_CRS&#39;]
            with zipfile.ZipFile(resources_dict[&#39;well_data&#39;].as_posix(), &#39;r&#39;) as archive:
                for file_name in archive.namelist():
                    with archive.open(file_name) as file:
                        if &#39;HEADER&#39; in file_name:
                            metaDF = pd.read_csv(file)
                        else:
                            resources_dict[&#39;well_data&#39;] = pd.read_csv(file)
            geometry = [Point(xy) for xy in zip(resources_dict[&#39;well_data&#39;][&#39;LONGITUDE&#39;], resources_dict[&#39;well_data&#39;][&#39;LATITUDE&#39;])]
            resources_dict[&#39;well_data&#39;] = gpd.GeoDataFrame(resources_dict[&#39;well_data&#39;], geometry=geometry, crs=&#39;EPSG:4269&#39;)
            
        else:
            sacrs = &#39;EPSG:4269&#39;
            df = pd.read_csv(resources_dict[&#39;well_data&#39;])
            df[&#39;geometry&#39;] = df[&#39;geometry&#39;].apply(wkt.loads)
            resources_dict[&#39;well_data&#39;] = gpd.GeoDataFrame(df, geometry=&#39;geometry&#39;)


        resources_dict[&#39;study_area&#39;] = gpd.read_file(resources_dict[&#39;study_area&#39;], geometry=&#39;geometry&#39;, crs=sacrs)

        resources_dict[&#39;model_grid&#39;] = rxr.open_rasterio(resources_dict[&#39;model_grid&#39;])
        resources_dict[&#39;surf_elev&#39;] = rxr.open_rasterio(resources_dict[&#39;surf_elev&#39;])
        #resources_dict[&#39;surf_elev&#39;] = resources_dict[&#39;surf_elev&#39;].sel(band=1)
        resources_dict[&#39;bedrock_elev&#39;] = rxr.open_rasterio(resources_dict[&#39;bedrock_elev&#39;])
        #resources_dict[&#39;bedrock_elev&#39;] = resources_dict[&#39;bedrock_elev&#39;].sel(band=1)

    return resources_dict</code></pre>
</details>
</dd>
<dt id="w4h.get_search_terms"><code class="name flex">
<span>def <span class="ident">get_search_terms</span></span>(<span>spec_path='C:\\Users\\riley\\LocalData\\Github\\wells4hydrogeology/resources/', spec_glob_pattern='*SearchTerms-Specific*', start_path=None, start_glob_pattern='*SearchTerms-Start*', wildcard_path=None, wildcard_glob_pattern='*SearchTerms-Wildcard', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Read in dictionary files for downhole data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spec_path</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, optional</dt>
<dd>Directory where the file containing the specific search terms is located, by default str(repoDir)+'/resources/'</dd>
<dt><strong><code>spec_glob_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default '<em>SearchTerms-Specific</em>'</dd>
<dt><strong><code>start_path</code></strong> :&ensp;<code>str</code> or <code>None</code>, optional</dt>
<dd>Directory where the file containing the start search terms is located, by default None</dd>
<dt><strong><code>start_glob_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default '<em>SearchTerms-Start</em>'</dd>
<dt><strong><code>wildcard_path</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, default <code>= None</code></dt>
<dd>Directory where the file containing the wildcard search terms is located, by default None</dd>
<dt><strong><code>wildcard_glob_pattern</code></strong> :&ensp;<code>str</code>, default <code>= '*SearchTerms-Wildcard'</code></dt>
<dd>Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default '<em>SearchTerms-Wildcard</em>'</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(specTermsPath, startTermsPath, wilcardTermsPath) : tuple
Tuple containing the pandas dataframes with specific search terms,
with start search terms, and with wildcard search terms</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_search_terms(spec_path=str(repoDir)+&#39;/resources/&#39;, spec_glob_pattern=&#39;*SearchTerms-Specific*&#39;, 
                     start_path=None, start_glob_pattern = &#39;*SearchTerms-Start*&#39;, 
                     wildcard_path=None, wildcard_glob_pattern=&#39;*SearchTerms-Wildcard&#39;,
                     verbose=False, log=False):
    &#34;&#34;&#34;Read in dictionary files for downhole data

    Parameters
    ----------
    spec_path : str or pathlib.Path, optional
        Directory where the file containing the specific search terms is located, by default str(repoDir)+&#39;/resources/&#39;
    spec_glob_pattern : str, optional
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Specific*&#39;
    start_path : str or None, optional
        Directory where the file containing the start search terms is located, by default None
    start_glob_pattern : str, optional
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Start*&#39;
    wildcard_path : str or pathlib.Path, default = None
        Directory where the file containing the wildcard search terms is located, by default None    
    wildcard_glob_pattern : str, default = &#39;*SearchTerms-Wildcard&#39;
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Wildcard*&#39;
    log : bool, default = True
        Whether to log inputs and outputs to log file.        

    Returns
    -------
    (specTermsPath, startTermsPath, wilcardTermsPath) : tuple
        Tuple containing the pandas dataframes with specific search terms,  with start search terms, and with wildcard search terms
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(get_search_terms, locals())    
    #specTermsFile = &#34;SearchTerms-Specific_BedrockOrNo_2022-09.csv&#34; #Specific matches
    #startTermsFile = &#34;SearchTerms-Start_BedrockOrNo.csv&#34; #Wildcard matches for the start of the description

    #Exact match path
    if spec_path is None:
        specTermsPath = spec_path        
    else:
        spec_path = pathlib.Path(spec_path)

        if spec_path.is_dir():
            specTermsPath = get_most_recent(spec_path, spec_glob_pattern)
        else:
            specTermsPath = spec_path

    #Startswith path
    if start_path is None:
        startTermsPath = start_path        
    else:
        start_path = pathlib.Path(start_path)

        if start_path.is_dir():
            startTermsPath = get_most_recent(start_path, start_glob_pattern)
        else:
            startTermsPath = start_path

    #Wildcard Path
    if wildcard_path is None:
        wilcardTermsPath = wildcard_path        
    else:
        wildcard_path = pathlib.Path(wildcard_path)

        if wildcard_path.is_dir():
            wilcardTermsPath = get_most_recent(wildcard_path, wildcard_glob_pattern)
        else:
            wilcardTermsPath = wildcard_path
    
    return specTermsPath, startTermsPath, wilcardTermsPath</code></pre>
</details>
</dd>
<dt id="w4h.get_unique_wells"><code class="name flex">
<span>def <span class="ident">get_unique_wells</span></span>(<span>df, wellid_col='API_NUMBER', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets unique wells as a dataframe based on a given column name.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all wells and/or well intervals of interest</dd>
<dt><strong><code>wellid_col</code></strong> :&ensp;<code>str</code>, default=<code>"API_NUMBER"</code></dt>
<dd>Name of column in df containing a unique identifier for each well, by default 'API_NUMBER'. .unique() will be run on this column to get the unique values.</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>wellsDF</code></dt>
<dd>DataFrame containing only the unique well IDs</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_unique_wells(df, wellid_col=&#39;API_NUMBER&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Gets unique wells as a dataframe based on a given column name.

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe containing all wells and/or well intervals of interest
    wellid_col : str, default=&#34;API_NUMBER&#34;
        Name of column in df containing a unique identifier for each well, by default &#39;API_NUMBER&#39;. .unique() will be run on this column to get the unique values.
    log : bool, default = False
        Whether to log results to log file

    Returns
    -------
    wellsDF
        DataFrame containing only the unique well IDs
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(get_unique_wells, locals(), exclude_params=[&#39;df&#39;])
    #Get Unique well APIs
    uniqueWells = df[wellid_col].unique()
    wellsDF = pd.DataFrame(uniqueWells)
    if verbose:
        print(&#39;Number of unique wells: &#39;+str(wellsDF.shape[0]))
    wellsDF.columns = [&#39;UNIQUE_ID&#39;]
    
    return wellsDF</code></pre>
</details>
</dd>
<dt id="w4h.grid2study_area"><code class="name flex">
<span>def <span class="ident">grid2study_area</span></span>(<span>study_area, grid, output_crs='EPSG:4269', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Clips grid to study area.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>study_area</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>inputs study area polygon</dd>
<dt><strong><code>grid</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>inputs grid array</dd>
<dt><strong><code>output_crs</code></strong> :&ensp;<code>str</code>, default=<code>'EPSG:4269'</code></dt>
<dd>inputs the coordinate reference system for the study area</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>grid</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>returns xarray containing grid clipped only to area within study area</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def grid2study_area(study_area, grid, output_crs=&#39;EPSG:4269&#39;,verbose=False, log=False):
    &#34;&#34;&#34;Clips grid to study area.

    Parameters
    ----------
    study_area : geopandas.GeoDataFrame
        inputs study area polygon
    grid : xarray.DataArray
        inputs grid array
    output_crs : str, default=&#39;EPSG:4269&#39;
        inputs the coordinate reference system for the study area
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    grid : xarray.DataArray
        returns xarray containing grid clipped only to area within study area

    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(grid2study_area, locals(), exclude_params=[&#39;study_area&#39;, &#39;grid&#39;])
    
    if output_crs==&#39;&#39;:
        output_crs=study_area.crs

    #Get input CRS&#39;s
    grid_crs = pyproj.CRS.from_wkt(grid.rio.crs.to_wkt())
    study_area_crs = study_area.crs
    
    # Reproject if needed
    if output_crs != grid_crs:
        grid = grid.rio.reproject(output_crs)
    grid_crs = pyproj.CRS.from_wkt(grid.rio.crs.to_wkt())

    if grid_crs != study_area_crs:
        study_area = study_area.to_crs(grid_crs)   

    # We&#39;ll just clip to outer bounds of study area
    saExtent = study_area.total_bounds

    if grid[&#39;y&#39;][-1].values - grid[&#39;y&#39;][0].values &gt; 0:
        miny=saExtent[1]
        maxy=saExtent[3]
    else:
        miny=saExtent[3]
        maxy=saExtent[1]        
        
    if grid[&#39;x&#39;][-1].values - grid[&#39;x&#39;][0].values &gt; 0:
        minx=saExtent[0]
        maxx=saExtent[2]
    else:
        minx=saExtent[2]
        maxx=saExtent[0]
    
    # &#34;Clip&#34; it
    grid = grid.sel(x=slice(minx, maxx), y=slice(miny, maxy))
    if &#39;band&#39; in grid.dims:
        grid = grid.sel(band=1)

    return grid</code></pre>
</details>
</dd>
<dt id="w4h.layer_interp"><code class="name flex">
<span>def <span class="ident">layer_interp</span></span>(<span>points, grid, layers=None, interp_kind='nearest', return_type='dataarray', export_dir=None, target_col='TARG_THICK_PER', layer_col='LAYER', xcol=None, ycol=None, xcoord='x', ycoord='y', log=False, verbose=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to interpolate results, going from points to grid data. Uses scipy.interpolate module.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>points</code></strong> :&ensp;<code>list</code></dt>
<dd>List containing pandas dataframes or geopandas geoadataframes containing the point data. Should be resDF_list output from layer_target_thick().</dd>
<dt><strong><code>grid</code></strong> :&ensp;<code>xr.DataArray</code> or <code>xr.Dataset</code></dt>
<dd>Xarray DataArray or DataSet with the coordinates/spatial reference of the output grid to interpolate to</dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>int</code>, default=<code>None</code></dt>
<dd>Number of layers for interpolation. If None, uses the length ofthe points list to determine number of layers. By default None.</dd>
<dt><strong><code>interp_kind</code></strong> :&ensp;<code>str, {'nearest', 'interp2d','linear', 'cloughtocher', 'radial basis function'}</code></dt>
<dd>Type of interpolation to use. See scipy.interpolate N-D scattered. Values can be any of the following (also shown in "kind" column of N-D scattered section of table here: <a href="https://docs.scipy.org/doc/scipy/tutorial/interpolate.html">https://docs.scipy.org/doc/scipy/tutorial/interpolate.html</a>). By default 'nearest'</dd>
<dt><strong><code>return_type</code></strong> :&ensp;<code>str, {'dataarray', 'dataset'}</code></dt>
<dd>Type of xarray object to return, either xr.DataArray or xr.Dataset, by default 'dataarray.'</dd>
<dt><strong><code>export_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, default=<code>None</code></dt>
<dd>Export directory for interpolated grids, using w4h.export_grids(). If None, does not export, by default None.</dd>
<dt><strong><code>target_col</code></strong> :&ensp;<code>str</code>, default <code>= 'TARG_THICK_PER'</code></dt>
<dd>Name of column in points containing data to be interpolated, by default 'TARG_THICK_PER'.</dd>
<dt><strong><code>layer_col</code></strong> :&ensp;<code>str</code>, default <code>= 'Layer'</code></dt>
<dd>Name of column containing layer number. Not currently used, by default 'LAYER'</dd>
<dt><strong><code>xcol</code></strong> :&ensp;<code>str</code>, default <code>= 'None'</code></dt>
<dd>Name of column containing x coordinates. If None, will look for 'geometry' column, as in a geopandas.GeoDataframe. By default None</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default <code>= 'None'</code></dt>
<dd>Name of column containing y coordinates. If None, will look for 'geometry' column, as in a geopandas.GeoDataframe. By default None</dd>
<dt><strong><code>xcoord</code></strong> :&ensp;<code>str</code>, default=<code>'x'</code></dt>
<dd>Name of x coordinate in grid, used to extract x values of grid, by default 'x'</dd>
<dt><strong><code>ycoord</code></strong> :&ensp;<code>str</code>, default=<code>'y'</code></dt>
<dd>Name of y coordinate in grid, used to extract x values of grid, by default 'y'</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments to be read directly into whichever scipy.interpolate function is designated by the interp_kind parameter.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>interp_data</code></strong> :&ensp;<code>xr.DataArray</code> or <code>xr.Dataset, depending on return_type</code></dt>
<dd>By default, returns an xr.DataArray object with the layers added as a new dimension called Layer. Can also specify return_type='dataset' to return an xr.Dataset with each layer as a separate variable.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def layer_interp(points, grid, layers=None, interp_kind=&#39;nearest&#39;, return_type=&#39;dataarray&#39;, export_dir=None, target_col=&#39;TARG_THICK_PER&#39;, layer_col=&#39;LAYER&#39;, xcol=None, ycol=None, xcoord=&#39;x&#39;, ycoord=&#39;y&#39;, log=False, verbose=False, **kwargs):
    &#34;&#34;&#34;Function to interpolate results, going from points to grid data. Uses scipy.interpolate module.

    Parameters
    ----------
    points : list
        List containing pandas dataframes or geopandas geoadataframes containing the point data. Should be resDF_list output from layer_target_thick().
    grid : xr.DataArray or xr.Dataset
        Xarray DataArray or DataSet with the coordinates/spatial reference of the output grid to interpolate to
    layers : int, default=None
        Number of layers for interpolation. If None, uses the length ofthe points list to determine number of layers. By default None.
    interp_kind : str, {&#39;nearest&#39;, &#39;interp2d&#39;,&#39;linear&#39;, &#39;cloughtocher&#39;, &#39;radial basis function&#39;}
        Type of interpolation to use. See scipy.interpolate N-D scattered. Values can be any of the following (also shown in &#34;kind&#34; column of N-D scattered section of table here: https://docs.scipy.org/doc/scipy/tutorial/interpolate.html). By default &#39;nearest&#39;
    return_type : str, {&#39;dataarray&#39;, &#39;dataset&#39;}
        Type of xarray object to return, either xr.DataArray or xr.Dataset, by default &#39;dataarray.&#39;
    export_dir : str or pathlib.Path, default=None
        Export directory for interpolated grids, using w4h.export_grids(). If None, does not export, by default None.
    target_col : str, default = &#39;TARG_THICK_PER&#39;
        Name of column in points containing data to be interpolated, by default &#39;TARG_THICK_PER&#39;.
    layer_col : str, default = &#39;Layer&#39;
        Name of column containing layer number. Not currently used, by default &#39;LAYER&#39;
    xcol : str, default = &#39;None&#39;
        Name of column containing x coordinates. If None, will look for &#39;geometry&#39; column, as in a geopandas.GeoDataframe. By default None
    ycol : str, default = &#39;None&#39;
        Name of column containing y coordinates. If None, will look for &#39;geometry&#39; column, as in a geopandas.GeoDataframe. By default None
    xcoord : str, default=&#39;x&#39;
        Name of x coordinate in grid, used to extract x values of grid, by default &#39;x&#39;
    ycoord : str, default=&#39;y&#39;
        Name of y coordinate in grid, used to extract x values of grid, by default &#39;y&#39;
    log : bool, default = True
        Whether to log inputs and outputs to log file.
    **kwargs
        Keyword arguments to be read directly into whichever scipy.interpolate function is designated by the interp_kind parameter.

    Returns
    -------
    interp_data : xr.DataArray or xr.Dataset, depending on return_type
        By default, returns an xr.DataArray object with the layers added as a new dimension called Layer. Can also specify return_type=&#39;dataset&#39; to return an xr.Dataset with each layer as a separate variable.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if verbose:
        verbose_print(layer_interp, locals(), exclude_params=[&#39;points&#39;, &#39;grid&#39;])

    nnList = [&#39;nearest&#39;, &#39;nearest neighbor&#39;, &#39;nearestneighbor&#39;,&#39;neighbor&#39;,  &#39;nn&#39;,&#39;n&#39;]
    splineList = [&#39;interp2d&#39;, &#39;interp2&#39;, &#39;interp&#39;, &#39;spline&#39;, &#39;spl&#39;, &#39;sp&#39;, &#39;s&#39;]
    linList = [&#39;linear&#39;, &#39;lin&#39;, &#39;l&#39;]
    ctList = [&#39;clough tocher&#39;, &#39;clough&#39;, &#39;cloughtocher&#39;, &#39;ct&#39;, &#39;c&#39;]
    rbfList = [&#39;rbf&#39;, &#39;radial basis&#39;, &#39;radial basis function&#39;, &#39;r&#39;, &#39;radial&#39;]
    #k-nearest neighbors from scikit-learn?
    #kriging? (from pykrige or maybe also from scikit-learn)
    
    X = np.round(grid[xcoord].values, 5)# #Extract xcoords from grid
    Y = np.round(grid[ycoord].values, 5)# #Extract ycoords from grid
    
    if layers is None and (type(points) is list or type(points) is dict):
        layers = len(points)

    if len(points) != layers:
        print(&#39;You have specified a different number of layers than what is iterable in the points argument. This may not work properly.&#39;)

    if verbose:
        print(&#39;Interpolating target lithology at each layer:&#39;)
    daDict = {}
    for lyr in range(1, layers+1):
        
        if type(points) is list or type(points) is dict:
            pts = points[lyr-1]
        else:
            pts = points

        if xcol is None:
            if &#39;geometry&#39; in pts.columns:
                dataX = pts[&#39;geometry&#39;].x
            else:
                print(&#39;xcol not specified and geometry column not detected (points is not/does not contain geopandas.GeoDataFrame)&#39; )
                return
        else:
            dataX = pts[xcol]
        
        if ycol is None:
            if &#39;geometry&#39; in pts.columns:
                dataY = pts[&#39;geometry&#39;].y
            else:
                print(&#39;ycol not specified and geometry column not detected (points is not/does not contain geopandas.GeoDataFrame)&#39; )
                return
        else:
            dataY = pts[ycol]

        #layer = pts[layer_col]        
        interpVal = pts[target_col]

        #return points
        dataX.dropna(inplace=True)
        dataY = dataY.loc[dataX.index]
        dataY.dropna(inplace=True)
        interpVal = interpVal.loc[dataY.index]
        interpVal.dropna(inplace=True)
        
        dataX = dataX.loc[interpVal.index]
        dataY = dataY.loc[interpVal.index]
        
        dataX = dataX.reset_index(drop=True)
        dataY = dataY.reset_index(drop=True)
        interpVal = interpVal.reset_index(drop=True)
        
        if interp_kind.lower() in nnList:
            interpType = &#39;Nearest Neighbor&#39;
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            dataPoints = np.array(list(zip(dataX, dataY)))
            interp = interpolate.NearestNDInterpolator(dataPoints, interpVal, **kwargs)
            Z = interp(X, Y)
        elif interp_kind.lower() in linList:
            interpType = &#39;Linear&#39; 
            dataPoints = np.array(list(zip(dataX, dataY)))
            interp = interpolate.LinearNDInterpolator(dataPoints, interpVal, **kwargs)
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            Z = interp(X, Y)
        elif interp_kind.lower() in ctList:
            interpType = &#39;Clough-Toucher&#39;
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            if &#39;tol&#39; not in kwargs:
                kwargs[&#39;tol&#39;] = 1e10
            interp = interpolate.CloughTocher2DInterpolator(list(zip(dataX, dataY)), interpVal, **kwargs)
            Z = interp(X, Y) 
        elif interp_kind.lower() in rbfList:
            interpType = &#39;Radial Basis&#39;
            dataXY=  np.column_stack((dataX, dataY))
            interp = interpolate.RBFInterpolator(dataXY, interpVal, **kwargs)
            print(&#34;Radial Basis Function does not work well with many well-based datasets. Consider instead specifying &#39;nearest&#39;, &#39;linear&#39;, &#39;spline&#39;, or &#39;clough tocher&#39; for interpolation interp_kind.&#34;)
            Z = interp(np.column_stack((X.ravel(), Y.ravel()))).reshape(X.shape)
        elif interp_kind.lower() in splineList:
            interpType = &#39;Spline Interpolation&#39;
            Z = interpolate.bisplrep(dataX, dataY, interpVal, **kwargs)
                #interp = interpolate.interp2d(dataX, dataY, interpVal, kind=lin_kind, **kwargs)
                #Z = interp(X, Y)
        else:
            if verbose:
                print(&#39;Specified interpolation interp_kind not recognized, using nearest neighbor.&#39;)
            interpType = &#39;Nearest Neighbor&#39;
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            interp = interpolate.NearestNDInterpolator(list(zip(dataX, dataY)), interpVal, **kwargs)
            Z = interp(X, Y)

        #global ZTest
        #ZTest = Z

        interp_grid = xr.DataArray( #Create new datarray with new data values, but everything else the same
                    data=Z,
                    dims=grid.dims,
                    coords=grid.coords)
        
        if &#39;band&#39; in interp_grid.coords:
            interp_grid = interp_grid.drop_vars(&#39;band&#39;)
        interp_grid = interp_grid.clip(min=0, max=1, keep_attrs=True)

        interp_grid = interp_grid.expand_dims(dim=&#39;Layer&#39;)
        interp_grid = interp_grid.assign_coords(Layer=[lyr])

        del Z
        del dataX
        del dataY
        del interpVal
        del interp

        #interp_grid=interp_grid.interpolate_na(dim=x)
        zFillDigs = len(str(layers))
        daDict[&#39;Layer&#39;+str(lyr).zfill(zFillDigs)] = interp_grid
        del interp_grid
        if verbose:
            print(&#39;\tCompleted {} interpolation for Layer {}&#39;.format(str(interpType).lower(), str(lyr).zfill(zFillDigs)))

    dataAList = [&#39;dataarray&#39;, &#39;da&#39;, &#39;a&#39;, &#39;array&#39;]
    dataSList = [&#39;dataset&#39;, &#39;ds&#39;, &#39;set&#39;]

    if return_type.lower() in dataAList:
        interp_data = xr.concat(daDict.values(), dim=&#39;Layer&#39;)
        interp_data = interp_data.assign_coords(Layer=np.arange(1,10))
    elif return_type.lower() in dataSList:
        interp_data = xr.Dataset(daDict)
        if verbose:
            print(&#39;Done with interpolation, getting global attrs&#39;)

        #Get common attributes from all layers to use as &#34;global&#34; attributes
        common_attrs = {}
        for i, (var_name, data_array) in enumerate(interp_data.data_vars.items()):
            if i == 0:
                common_attrs = data_array.attrs
            else:
                common_attrs = {k: v for k, v in common_attrs.items() if k in data_array.attrs and data_array.attrs[k] == v}
        interp_data.attrs.update(common_attrs)
    else:
        print(f&#34;{return_type} is not a valid input for return_type. Please set return_type to either &#39;dataarray&#39; or &#39;dataset&#39;&#34;)
        return

    if verbose:
        for i, layer_data in enumerate(interp_data):
            pts = points[i]
            pts.plot(c=pts[target_col])

    if export_dir is None:
        pass
    else:
        w4h.export_grids(grid_data=interp_data, out_path=export_dir, file_id=&#39;&#39;,filetype=&#39;tif&#39;, variable_sep=True, date_stamp=True)
        print(&#39;Exported to {}&#39;.format(export_dir))

    return interp_data</code></pre>
</details>
</dd>
<dt id="w4h.layer_target_thick"><code class="name flex">
<span>def <span class="ident">layer_target_thick</span></span>(<span>df, layers=9, return_all=False, export_dir=None, outfile_prefix=None, depth_top_col='TOP', depth_bot_col='BOTTOM', log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to calculate thickness of target material in each layer at each well point</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>geopandas.geodataframe</code></dt>
<dd>Geodataframe containing classified data, surface elevation, bedrock elevation, layer depths, geometry.</dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>int</code>, default=<code>9</code></dt>
<dd>Number of layers in model, by default 9</dd>
<dt><strong><code>return_all</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>If True, return list of original geodataframes with extra column added for target thick for each layer.
If False, return list of geopandas.geodataframes with only essential information for each layer.</dd>
<dt><strong><code>export_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, default=<code>None</code></dt>
<dd>If str or pathlib.Path, should be directory to which to export dataframes built in function.</dd>
<dt><strong><code>outfile_prefix</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>Only used if export_dir is set. Will be used at the start of the exported filenames</dd>
<dt><strong><code>depth_top_col</code></strong> :&ensp;<code>str</code>, default=<code>'TOP'</code></dt>
<dd>Name of column containing data for depth to top of described well intervals</dd>
<dt><strong><code>depth_bot_col</code></strong> :&ensp;<code>str</code>, default=<code>'BOTTOM'</code></dt>
<dd>Name of column containing data for depth to bottom of described well intervals</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>res_df</code> or <code>res : geopandas.geodataframe</code></dt>
<dd>Geopandas geodataframe containing only important information needed for next stage of analysis.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def layer_target_thick(df, layers=9, return_all=False, export_dir=None, outfile_prefix=None, depth_top_col=&#39;TOP&#39;, depth_bot_col=&#39;BOTTOM&#39;, log=False):
    &#34;&#34;&#34;Function to calculate thickness of target material in each layer at each well point

    Parameters
    ----------
    df : geopandas.geodataframe
        Geodataframe containing classified data, surface elevation, bedrock elevation, layer depths, geometry.
    layers : int, default=9
        Number of layers in model, by default 9
    return_all : bool, default=False
        If True, return list of original geodataframes with extra column added for target thick for each layer.
        If False, return list of geopandas.geodataframes with only essential information for each layer.
    export_dir : str or pathlib.Path, default=None
        If str or pathlib.Path, should be directory to which to export dataframes built in function.
    outfile_prefix : str, default=None
        Only used if export_dir is set. Will be used at the start of the exported filenames
    depth_top_col : str, default=&#39;TOP&#39;
        Name of column containing data for depth to top of described well intervals
    depth_bot_col : str, default=&#39;BOTTOM&#39;
        Name of column containing data for depth to bottom of described well intervals
    log : bool, default = True
        Whether to log inputs and outputs to log file.
    
    Returns
    -------
    res_df or res : geopandas.geodataframe
        Geopandas geodataframe containing only important information needed for next stage of analysis.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    df[&#39;TOP_ELEV&#39;] = df[&#39;SURFACE_ELEV&#39;] - df[depth_top_col]
    df[&#39;BOT_ELEV&#39;] = df[&#39;SURFACE_ELEV&#39;] - df[depth_bot_col]

    layerList = range(1,layers+1)
    res_list = []
    resdf_list = []

    #Generate Column names based on (looped) integers
    for layer in layerList:
        zStr = &#39;ELEV&#39;
        zColT = &#39;TOP_ELEV&#39;
        zColB = &#39;BOT_ELEV&#39;
        topCol = zStr+&#39;_LAYER&#39;+str(layer)
        if layer != 9: #For all layers except the bottom layer....
            botCol = zStr+&#39;_LAYER&#39;+str(layer+1) #use the layer below it to 
        else: #Otherwise, ...
            botCol = &#34;BEDROCK_&#34;+zStr #Use the (corrected) bedrock depth

        #Divide records into 4 categories for ease of calculation, to be joined back together later  
            #Category 1: Well interval starts above layer top, ends within model layer
            #Category 2: Well interval is entirely contained withing model layer
            #Category 3: Well interval starts within model layer, continues through bottom of model layer
            #Category 4: well interval begins and ends on either side of model layer (model layer is contained within well layer)

        #records1 = intervals that go through the top of the layer and bottom is within layer
        records1 = df.loc[(df[zColT] &gt;= df[topCol]) &amp; #Top of the well interval is above or equal to the top of the layer
                        (df[zColB] &lt;= df[topCol]) &amp; # &amp; #Bottom is below the top of the layer
                        (df[zColB] &lt;= df[zColT])].copy() #Bottom is deeper than top (should already be the case)
        records1[&#39;TARG_THICK&#39;] = pd.DataFrame(np.round((records1.loc[:,topCol]-records1.loc[: , zColB]) * records1[&#39;TARGET&#39;],3)).copy() #Multiply &#34;target&#34; (1 or 0) by length within layer            
        #records2 = entire interval is within layer
        records2 = df.loc[(df[zColT] &lt;= df[topCol]) &amp; #Top of the well is lower than top of the layer 
                    (df[zColB] &gt;= df[botCol]) &amp; #Bottom of the well is above bottom of the layer 
                    (df[zColB] &lt;= df[zColT])].copy() #Bottom ofthe well is deeper than or equal to top (should already be the case)
        records2[&#39;TARG_THICK&#39;] = pd.DataFrame(np.round((records2.loc[: , zColT] - records2.loc[: , zColB]) * records2[&#39;TARGET&#39;],3)).copy()

        #records3 = intervals with top within layer and bottom of interval going through bottom of layer
        records3 = df.loc[(df[zColT] &gt; df[botCol]) &amp; #Top of the well is above bottom of layer
                    (df[zColB] &lt; df[botCol]) &amp; #Bottom of the well is below bottom of layer
                    (df[zColT] &lt;= df[topCol]) &amp; #Top of well is below top of layer
                    (df[zColB] &lt;= df[zColT])].copy() #Bottom is deeper than top (should already be the case)
        records3[&#39;TARG_THICK&#39;] = pd.DataFrame(np.round((records3.loc[: , zColT] - (records3.loc[:,botCol]))*records3[&#39;TARGET&#39;],3)).copy()

        #records4 = interval goes through entire layer
        records4 = df.loc[(df[zColT] &gt;= df[topCol]) &amp; #Top of well is above top of layer
                    (df[zColB] &lt; df[botCol]) &amp; #Bottom of well is below bottom of layer
                    (df[zColB] &lt;= df[zColT])].copy() #Bottom of well is below top of well
        records4[&#39;TARG_THICK&#39;] = pd.DataFrame(np.round((records4.loc[: , topCol]-records4.loc[: , botCol]) * records4[&#39;TARGET&#39;],3)).copy()

        #Put the four calculated record categories back together into single dataframe
        res = pd.concat([records1, records2, records3, records4])
        #The sign may be reversed if using depth rather than elevation
        if (res[&#39;TARG_THICK&#39;] &lt; 0).all():
            res[&#39;TARG_THICK&#39;] = res[&#39;TARG_THICK&#39;] * -1
        
        #Cannot have negative thicknesses
        res[&#39;TARG_THICK&#39;].clip(lower=0, inplace=True)
        res[&#39;LAYER_THICK&#39;].clip(lower=0, inplace=True)
        
        #Get geometrys for each unique API/well
        res.reset_index(drop=True, inplace=True)
        res_df = res.groupby(by=[&#39;API_NUMBER&#39;,&#39;LATITUDE&#39;,&#39;LONGITUDE&#39;], as_index=False).sum(numeric_only=True)#Calculate thickness for each well interval in the layer indicated (e.g., if there are two well intervals from same well in one model layer)
        uniqInd = pd.DataFrame([v.values[0] for k, v in res.groupby(by=[&#39;API_NUMBER&#39;,&#39;LATITUDE&#39;,&#39;LONGITUDE&#39;]).groups.items()]).loc[:,0]
        geomCol = res.loc[uniqInd, &#39;geometry&#39;]
        geomCol = pd.DataFrame(geomCol[~geomCol.index.duplicated(keep=&#39;first&#39;)]).reset_index()
        

        res_df[&#39;TARG_THICK_PER&#39;] =  pd.DataFrame(np.round(res_df[&#39;TARG_THICK&#39;]/res_df[&#39;LAYER_THICK&#39;],3)) #Calculate thickness as percent of total layer thickness
        #Replace np.inf and np.nans with 0
        res_df[&#39;TARG_THICK_PER&#39;] = res_df[&#39;TARG_THICK_PER&#39;].where(res_df[&#39;TARG_THICK_PER&#39;]!=np.inf, other=0)
        res_df[&#39;TARG_THICK_PER&#39;] = res_df[&#39;TARG_THICK_PER&#39;].where(res_df[&#39;TARG_THICK_PER&#39;]!=np.nan, other=0)

        res_df[&#34;LAYER&#34;] = layer #Just to have as part of the output file, include the present layer in the file itself as a separate column
        res_df = res_df[[&#39;API_NUMBER&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;LATITUDE_PROJ&#39;, &#39;LONGITUDE_PROJ&#39;,&#39;TOP&#39;, &#39;BOTTOM&#39;, &#39;TOP_ELEV&#39;, &#39;BOT_ELEV&#39;, &#39;SURFACE_ELEV&#39;, topCol, botCol,&#39;LAYER_THICK&#39;,&#39;TARG_THICK&#39;, &#39;TARG_THICK_PER&#39;, &#39;LAYER&#39;]].copy() #Format dataframe for output
        res_df = gpd.GeoDataFrame(res_df, geometry=geomCol.loc[:,&#39;geometry&#39;])
        resdf_list.append(res_df)
        res_list.append(res)

        if isinstance(export_dir, (pathlib.PurePath, str)):
            export_dir = pathlib.Path(export_dir)
            if export_dir.is_dir():
                pass
            else:
                try:
                    os.mkdir(export_dir)
                except:
                    print(&#39;Specified export directory does not exist and cannot be created. Function will continue run, but data will not be exported.&#39;)

            #Format and build export filepath
            export_dir = str(export_dir).replace(&#39;\\&#39;, &#39;/&#39;).replace(&#39;\\&#39;[0], &#39;/&#39;)
            zFillDigs = len(str(len(layerList)))
            if str(outfile_prefix).endswith(&#39;_&#39;):
                outfile_prefix = outfile_prefix[:-1]
            elif outfile_prefix is None:
                outfile_prefix = &#39;&#39;
            

            if export_dir[-1] ==&#39;/&#39;:
                export_dir = export_dir[:-1]
            nowStr = str(datetime.datetime.today().date())+&#39;_&#39;+str(datetime.datetime.today().hour)+&#39;-&#39;+str(datetime.datetime.today().minute)+&#39;-&#39;+str(datetime.datetime.today().second)
            outPath = export_dir+&#39;/&#39;+outfile_prefix+&#39;_Lyr&#39;+str(layer).zfill(zFillDigs)+&#39;_&#39;+nowStr+&#39;.csv&#39;

            if return_all:
                res.to_csv(outPath, sep=&#39;,&#39;, na_rep=np.nan, index_label=&#39;ID&#39;)
                res_df.to_csv(outPath, sep=&#39;,&#39;, na_rep=np.nan, index_label=&#39;ID&#39;)
            else:
                res_df.to_csv(outPath, sep=&#39;,&#39;, na_rep=np.nan, index_label=&#39;ID&#39;)

    if return_all:
        return res_list, resdf_list
    else:
        return resdf_list</code></pre>
</details>
</dd>
<dt id="w4h.logger_function"><code class="name flex">
<span>def <span class="ident">logger_function</span></span>(<span>logtocommence, parameters, func_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to log other functions, to be called from within other functions</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>logtocommence</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to perform logging steps</dd>
<dt><strong><code>parameters</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary containing parameters and their values, from function</dd>
<dt><strong><code>func_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of function within which this is called</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def logger_function(logtocommence, parameters, func_name):
    &#34;&#34;&#34;Function to log other functions, to be called from within other functions

    Parameters
    ----------
    logtocommence : bool
        Whether to perform logging steps
    parameters : dict
        Dictionary containing parameters and their values, from function
    func_name : str
        Name of function within which this is called
    &#34;&#34;&#34;
    if logtocommence:
        global log_filename
        #log parameter should be false by default on all. If true, will show up in kwargs
        
        #Get the log parameter value
        if &#39;log&#39; in parameters.keys():
            log_file = parameters.pop(&#39;log&#39;, None)
        else:
            #If it wasn&#39;t set, default to None
            log_file = None
        
        #Get currenet time and setup format for log messages
        curr_time = datetime.datetime.now()
        FORMAT = &#39;%(asctime)s  %(message)s&#39;

        #Check if we are starting a new logfile (only does this during run of file_setup() or (currently non-existent) new_logfile() functions)
        if log_file == True and (func_name == &#39;file_setup&#39; or func_name == &#39;new_logfile&#39;):

            #Get the log_dir variable set as a file_setup() parameter, or default to None if not specified
            out_dir = parameters.pop(&#39;log_dir&#39;, None)
            if out_dir is None:
                #If output directory not specified, default to the input directory
                out_dir = parameters[&#39;well_data&#39;]
            
            #Get the timestamp for the filename (this won&#39;t change, so represents the start of logging)
            timestamp = curr_time.strftime(&#39;%Y-%m-%d_%H-%M-%S&#39;)
            log_filename = pathlib.Path(out_dir).joinpath(f&#34;log_{timestamp}.txt&#34;)
            if &#39;verbose&#39; in parameters.keys():
                print(&#39;Logging data to&#39;, log_filename)

            #Set up logging stream using logging module
            logging.basicConfig(filename=log_filename, level=logging.INFO, format=FORMAT, filemode=&#39;w&#39;)

            #Log 
            logging.info(f&#34;{func_name} CALLED WITH PARAMETERS:\n\t {parameters}&#34;)
        elif log_file == True:
            #Run this for functions that aren&#39;t setting up logging file
            if log_filename:
                #Get the log stream and log this function&#39;s call with parameters
                logging.basicConfig(filename=log_filename, level=logging.INFO, format=FORMAT)
                logging.info(f&#34;{func_name} CALLED WITH PARAMETERS: \n\t{parameters}&#34;)
            else:
                #If log file has not already been set up, set it up
                timestamp = curr_time.strftime(&#39;%Y-%m-%d_%H-%M-%S&#39;)
                log_filename = f&#34;log_{timestamp}.txt&#34;

                #Now, get the log stream and log this function&#39;s call with parameters
                logging.basicConfig(filename=log_filename, level=logging.INFO, format=FORMAT)
                logging.info(f&#34;{func_name} CALLED WITH PARAMETERS: \n\t{parameters}&#34;)
        else:
            #Don&#39;t log if log=False
            pass
    return</code></pre>
</details>
</dd>
<dt id="w4h.merge_lithologies"><code class="name flex">
<span>def <span class="ident">merge_lithologies</span></span>(<span>well_data_df, targinterps_df, interp_col='INTERPRETATION', target_col='TARGET', target_class='bool')</span>
</code></dt>
<dd>
<div class="desc"><p>Function to merge lithologies and target booleans based on classifications</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>well_data_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing classified well data</dd>
<dt><strong><code>targinterps_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing lithologies and their target interpretations, depending on what the target is for this analysis (often, coarse materials=1, fine=0)</dd>
<dt><strong><code>target_col</code></strong> :&ensp;<code>str</code>, default <code>= 'TARGET'</code></dt>
<dd>Name of column in targinterps_df containing the target interpretations</dd>
</dl>
<p>target_class, default = 'bool'
Whether the input column is using boolean values as its target indicator</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df_targ</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing merged lithologies/targets</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_lithologies(well_data_df, targinterps_df, interp_col=&#39;INTERPRETATION&#39;, target_col=&#39;TARGET&#39;, target_class=&#39;bool&#39;):
    &#34;&#34;&#34;Function to merge lithologies and target booleans based on classifications
    
    Parameters
    ----------
    well_data_df : pandas.DataFrame
        Dataframe containing classified well data
    targinterps_df : pandas.DataFrame
        Dataframe containing lithologies and their target interpretations, depending on what the target is for this analysis (often, coarse materials=1, fine=0)
    target_col : str, default = &#39;TARGET&#39;
        Name of column in targinterps_df containing the target interpretations
    target_class, default = &#39;bool&#39;
        Whether the input column is using boolean values as its target indicator
        
    Returns
    -------
    df_targ : pandas.DataFrame
        Dataframe containing merged lithologies/targets
    
    &#34;&#34;&#34;    
    
    #by default, use the boolean input 
    if target_class==&#39;bool&#39;:
        targinterps_df[target_col] = targinterps_df[target_col].where(targinterps_df[target_col]==&#39;1&#39;, other=&#39;0&#39;).astype(int)
        targinterps_df[target_col].fillna(value=0, inplace=True)
    else:
        targinterps_df[target_col].replace(&#39;DoNotUse&#39;, value=-1, inplace=True)
        targinterps_df[target_col].fillna(value=-2, inplace=True)
        targinterps_df[target_col].astype(np.int8)

    df_targ = pd.merge(well_data_df, targinterps_df.set_index(interp_col), right_on=interp_col, left_on=&#39;LITHOLOGY&#39;, how=&#39;left&#39;)
    
    return df_targ</code></pre>
</details>
</dd>
<dt id="w4h.merge_metadata"><code class="name flex">
<span>def <span class="ident">merge_metadata</span></span>(<span>data_df, header_df, data_cols=None, header_cols=None, auto_pick_cols=False, drop_duplicate_cols=True, log=False, verbose=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to merge tables, intended for merging metadata table with data table</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>"Left" dataframe, intended for this purpose to be dataframe with main data, but can be anything</dd>
<dt><strong><code>header_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>"Right" dataframe, intended for this purpose to be dataframe with metadata, but can be anything</dd>
<dt><strong><code>data_cols</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>List of strings of column names, for columns to be included after join from "left" table (data table). If None, all columns are kept, by default None</dd>
<dt><strong><code>header_cols</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>List of strings of columns names, for columns to be included in merged table after merge from "right" table (metadata). If None, all columns are kept, by default None</dd>
<dt><strong><code>auto_pick_cols</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to autopick the columns from the metadata table. If True, the following column names are kept:['API_NUMBER', 'LATITUDE', 'LONGITUDE', 'BEDROCK_ELEV', 'SURFACE_ELEV', 'BEDROCK_DEPTH', 'LAYER_THICK'], by default False</dd>
<dt><strong><code>drop_duplicate_cols</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, drops duplicate columns from the tables so that columns do not get renamed upon merge, by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>kwargs that are passed directly to pd.merge(). By default, the 'on' and 'how' parameters are defined as on='API_NUMBER' and how='inner'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mergedTable</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Merged dataframe</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_metadata(data_df, header_df, data_cols=None, header_cols=None, auto_pick_cols=False, drop_duplicate_cols=True, log=False, verbose=False, **kwargs):
    &#34;&#34;&#34;Function to merge tables, intended for merging metadata table with data table

    Parameters
    ----------
    data_df : pandas.DataFrame
        &#34;Left&#34; dataframe, intended for this purpose to be dataframe with main data, but can be anything
    header_df : pandas.DataFrame
        &#34;Right&#34; dataframe, intended for this purpose to be dataframe with metadata, but can be anything
    data_cols : list, optional
        List of strings of column names, for columns to be included after join from &#34;left&#34; table (data table). If None, all columns are kept, by default None
    header_cols : list, optional
        List of strings of columns names, for columns to be included in merged table after merge from &#34;right&#34; table (metadata). If None, all columns are kept, by default None
    auto_pick_cols : bool, default = False
        Whether to autopick the columns from the metadata table. If True, the following column names are kept:[&#39;API_NUMBER&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;BEDROCK_ELEV&#39;, &#39;SURFACE_ELEV&#39;, &#39;BEDROCK_DEPTH&#39;, &#39;LAYER_THICK&#39;], by default False
    drop_duplicate_cols : bool, optional
        If True, drops duplicate columns from the tables so that columns do not get renamed upon merge, by default True
    log : bool, default = False
        Whether to log inputs and outputs to log file.
    **kwargs
        kwargs that are passed directly to pd.merge(). By default, the &#39;on&#39; and &#39;how&#39; parameters are defined as on=&#39;API_NUMBER&#39; and how=&#39;inner&#39;

    Returns
    -------
    mergedTable : pandas.DataFrame
        Merged dataframe
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(merge_metadata, locals(), exclude_params=[&#39;data_df&#39;, &#39;header_df&#39;])
    if header_df is None:
        #Figure out which columns to include
        if data_cols is None:
            #If not specified, get all the cols
            data_cols = data_df.columns                      
        else:
            if header_cols is not None:
                data_cols = data_cols + header_cols
        
        data_df = data_df[data_cols]

        mergedTable = data_df
    else:   
        if auto_pick_cols:
            header_cols = [&#39;API_NUMBER&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;BEDROCK_ELEV&#39;, &#39;SURFACE_ELEV&#39;, &#39;BEDROCK_DEPTH&#39;, &#39;LAYER_THICK&#39;]
            for c in header_df.columns:
                if c.startswith(&#39;ELEV_&#39;) or c.startswith(&#39;DEPTH&#39;):
                    header_cols.append(c)
                if &#39;_PROJ&#39; in c:
                    header_cols.append(c)
            header_cols.append(&#39;geometry&#39;)
        elif header_cols is None:
            header_cols = header_df.columns
        else:
            header_cols = header_cols

        #If not specified, get all the cols
        if data_cols is None:
            data_cols = data_df.columns

        #Defults for on and how
        if &#39;on&#39; not in kwargs.keys():
            kwargs[&#39;on&#39;]=&#39;API_NUMBER&#39;

        if &#39;how&#39; not in kwargs.keys():
            kwargs[&#39;how&#39;]=&#39;inner&#39;

        #Drop duplicate columns
        if drop_duplicate_cols:
            header_colCopy= header_cols.copy()
            remCount = 0
            for i, c in enumerate(header_colCopy):
                if c in data_cols and c != kwargs[&#39;on&#39;]:
                    if verbose:
                        print(&#39;Removing {} (duplicate columns) from data.&#39;.format(header_cols[i-remCount]))
                    header_cols.pop(i - remCount)
                    remCount += 1

        leftTable_join = data_df[data_cols]
        rightTable_join = header_df[header_cols]

        mergedTable = pd.merge(left=leftTable_join, right=rightTable_join, **kwargs)
    return mergedTable</code></pre>
</details>
</dd>
<dt id="w4h.read_dict"><code class="name flex">
<span>def <span class="ident">read_dict</span></span>(<span>file, keytype='np')</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read a text file with a dictionary in it into a python dictionary</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>Filepath to the file of interest containing the dictionary text</dd>
<dt><strong><code>keytype</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>String indicating the datatypes used in the text, currently only 'np' is implemented, by default 'np'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Dictionary translated from text file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_dict(file, keytype=&#39;np&#39;):
    &#34;&#34;&#34;Function to read a text file with a dictionary in it into a python dictionary

    Parameters
    ----------
    file : str or pathlib.Path object
        Filepath to the file of interest containing the dictionary text
    keytype : str, optional
        String indicating the datatypes used in the text, currently only &#39;np&#39; is implemented, by default &#39;np&#39;

    Returns
    -------
    dict
        Dictionary translated from text file.
    &#34;&#34;&#34;

    if pathlib.Path(file).suffix == &#39;.json&#39;:
        with open(file, &#39;r&#39;) as f:
            jsDict = json.load(f)
        return jsDict

    with open(file, &#39;r&#39;) as f:
        data= f.read()

    jsDict = json.loads(data)
    if keytype==&#39;np&#39;:
        for k in jsDict.keys():
            jsDict[k] = getattr(np, jsDict[k])
    
    return jsDict</code></pre>
</details>
</dd>
<dt id="w4h.read_dictionary_terms"><code class="name flex">
<span>def <span class="ident">read_dictionary_terms</span></span>(<span>dict_file=None, id_col='ID', search_col='DESCRIPTION', definition_col='LITHOLOGY', class_flag_col='CLASS_FLAG', dictionary_type=None, class_flag=6, rem_extra_cols=True, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read dictionary terms from file into pandas dataframe</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dict_file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>list</code> of <code>these</code></dt>
<dd>File or list of files to be read</dd>
<dt><strong><code>search_col</code></strong> :&ensp;<code>str</code>, default <code>= 'DESCRIPTION'</code></dt>
<dd>Name of column containing search terms (geologic formations)</dd>
<dt><strong><code>definition_col</code></strong> :&ensp;<code>str</code>, default <code>= 'LITHOLOGY'</code></dt>
<dd>Name of column containing interpretations of search terms (lithologies)</dd>
<dt><strong><code>dictionary_type</code></strong> :&ensp;<code>str</code> or <code>None, {None, 'exact', 'start', 'wildcard',}</code></dt>
<dd>Indicator of which kind of dictionary terms to be read in: None, 'exact', 'start', or 'wildcard' by default None.
- If None, uses name of file to try to determine. If it cannot, it will default to using the classification flag from class_flag
- If 'exact', will be used to search for exact matches to geologic descriptions
- If 'start', will be used as with the .startswith() string method to find inexact matches to geologic descriptions
- If 'wildcard', will be used to find any matching substring for inexact geologic matches</dd>
<dt><strong><code>class_flag</code></strong> :&ensp;<code>int</code>, default <code>= 1</code></dt>
<dd>Classification flag to be used if dictionary_type is None and cannot be otherwise determined, by default 1</dd>
<dt><strong><code>rem_extra_cols</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to remove the extra columns from the input file after it is read in as a pandas dataframe, by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict_terms</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with formatting ready to be used in the classification steps of this package</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_dictionary_terms(dict_file=None, id_col=&#39;ID&#39;, search_col=&#39;DESCRIPTION&#39;, definition_col=&#39;LITHOLOGY&#39;, class_flag_col=&#39;CLASS_FLAG&#39;, dictionary_type=None, class_flag=6, rem_extra_cols=True, verbose=False, log=False):
    &#34;&#34;&#34;Function to read dictionary terms from file into pandas dataframe

    Parameters
    ----------
    dict_file : str or pathlib.Path object, or list of these
        File or list of files to be read
    search_col : str, default = &#39;DESCRIPTION&#39;
        Name of column containing search terms (geologic formations)
    definition_col : str, default = &#39;LITHOLOGY&#39;
        Name of column containing interpretations of search terms (lithologies)
    dictionary_type : str or None, {None, &#39;exact&#39;, &#39;start&#39;, &#39;wildcard&#39;,}
        Indicator of which kind of dictionary terms to be read in: None, &#39;exact&#39;, &#39;start&#39;, or &#39;wildcard&#39; by default None.
            - If None, uses name of file to try to determine. If it cannot, it will default to using the classification flag from class_flag
            - If &#39;exact&#39;, will be used to search for exact matches to geologic descriptions
            - If &#39;start&#39;, will be used as with the .startswith() string method to find inexact matches to geologic descriptions
            - If &#39;wildcard&#39;, will be used to find any matching substring for inexact geologic matches
    class_flag : int, default = 1
        Classification flag to be used if dictionary_type is None and cannot be otherwise determined, by default 1
    rem_extra_cols : bool, default = True
        Whether to remove the extra columns from the input file after it is read in as a pandas dataframe, by default True
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    dict_terms : pandas.DataFrame
        Pandas dataframe with formatting ready to be used in the classification steps of this package
    &#34;&#34;&#34;
    
    if dict_file is None:
        dict_file=w4h.get_resources()[&#39;LithologyDict_Exact&#39;]
    
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(read_dictionary_terms, locals())    
    #Read files into pandas dataframes
    dict_terms = []
    #if dict_file is None:
    #    dict_file = get_resources()[&#39;LithologyDict_Exact&#39;]
    #    df = pd.DataFrame(columns=[&#39;ID&#39;, &#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#39;CLASS_FLAGS&#39;])
    #    dict_terms.append(df)
    #    dict_file = [&#39;&#39;]
    if isinstance(dict_file, (tuple, list)):
        for i, f in enumerate(dict_file):
            if not f.exists():
                df = pd.DataFrame(columns=[&#39;ID&#39;, &#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#39;CLASS_FLAGS&#39;])
                dict_terms.append(df)
            else:
                dict_terms.append(pd.read_csv(f))

            if id_col in dict_terms[i].columns:
                dict_terms[i].set_index(id_col, drop=True, inplace=True)
    else:
        if dict_file is None:
            dict_file = w4h.get_resources()[&#39;LithologyDict_Exact&#39;]
        
        dict_file = pathlib.Path(dict_file)
        if dict_file.exists() and dict_file.is_file():
            dict_terms.append(pd.read_csv(dict_file, low_memory=False))
            if id_col in dict_terms[-1].columns:
                dict_terms[-1].set_index(id_col, drop=True, inplace=True)
            dict_file = [dict_file]
        else:
            print(f&#39;ERROR: dict_file ({dict_file}) does not exist.&#39;)
            #Create empty dataframe to return
            dict_terms = pd.DataFrame(columns=[&#39;ID&#39;, &#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#34;CLASS_FLAGS&#34;])
            return dict_terms

    #Rename important columns
    searchTermList = [&#39;searchterm&#39;, &#39;search&#39;, &#39;exact&#39;]
    startTermList = [&#39;startterm&#39;, &#39;start&#39;, &#39;startswith&#39;]
    wildcardTermList = [&#39;wildcard&#39;, &#39;substring&#39;, ]

    #Recast all columns to datatypes of headerData to defined types
    dict_termDtypes = {search_col:str, definition_col:str, class_flag_col:np.uint8}

    if dictionary_type is None:
        dictionary_type=&#39;&#39; #Allow string methods on this variable

    #Iterating, to allow reading of multiple dict file at once (also works with just one at at time)
    for i, d in enumerate(dict_terms):
        if dictionary_type.lower() in searchTermList or (dictionary_type==&#39;&#39; and &#39;spec&#39; in str(dict_file[i]).lower()):
            d[class_flag_col] = 1
        elif dictionary_type.lower() in startTermList or (dictionary_type==&#39;&#39; and &#39;start&#39; in str(dict_file[i]).lower()):
            d[class_flag_col] = 4 #Start term classification flag
        elif dictionary_type.lower() in wildcardTermList or (dictionary_type==&#39;&#39; and &#39;wildcard&#39; in str(dict_file[i]).lower()):
            d[class_flag_col] = 5 #Wildcard term classification flag
        else:
            d[class_flag_col] = class_flag #Custom classification flag, defined as argument
        #1: exact classification match, 2: (not defined...ML?), 3: bedrock classification for obvious bedrock, 4: start term, 5: wildcard/substring, 6: Undefined

        #Rename columns so it is consistent through rest of code
        if search_col != &#39;DESCRIPTION&#39;:
            d.rename(columns={search_col:&#39;DESCRIPTION&#39;}, inplace=True)
        if definition_col != &#39;LITHOLOGY&#39;:
            d.rename(columns={definition_col:&#39;LITHOLOGY&#39;}, inplace=True)
        if class_flag_col != &#39;CLASS_FLAG&#39;:
            d.rename(columns={class_flag_col:&#39;CLASS_FLAG&#39;}, inplace=True)

        #Cast all columns as type str, if not already
        for i in range(0, np.shape(d)[1]):
            if d.iloc[:,i].name in list(dict_termDtypes.keys()):
                d.iloc[:,i] = d.iloc[:,i].astype(dict_termDtypes[d.iloc[:,i].name])
        
        #Delete duplicate definitions
        d.drop_duplicates(subset=&#39;DESCRIPTION&#39;,inplace=True) #Apparently, there are some duplicate definitions, which need to be deleted first
        d.reset_index(inplace=True, drop=True)

        #Whether to remove extra columns that aren&#39;t needed from dataframe
        if rem_extra_cols:
            d = d[[&#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#39;CLASS_FLAG&#39;]]

    #If only one file designated, convert it so it&#39;s no longer a list, but just a dataframe

    if len(dict_terms) == 1:
        dict_terms = dict_terms[0]

    return dict_terms</code></pre>
</details>
</dd>
<dt id="w4h.read_grid"><code class="name flex">
<span>def <span class="ident">read_grid</span></span>(<span>grid_path=None, grid_type='model', no_data_val_grid=0, use_service=False, study_area=None, grid_crs=None, output_crs='EPSG:4269', verbose=False, log=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads in grid</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>grid_path</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, default=<code>None</code></dt>
<dd>Path to a grid file</dd>
<dt><strong><code>grid_type</code></strong> :&ensp;<code>str</code>, default=<code>'model'</code></dt>
<dd>Sets what type of grid to load in</dd>
<dt><strong><code>no_data_val_grid</code></strong> :&ensp;<code>int</code>, default=<code>0</code></dt>
<dd>Sets the no data value of the grid</dd>
<dt><strong><code>use_service</code></strong> :&ensp;<code>str</code>, default=<code>False</code></dt>
<dd>Sets which service the function uses</dd>
<dt><strong><code>study_area</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code>, default=<code>None</code></dt>
<dd>Dataframe containing study area polygon</dd>
<dt><strong><code>grid_crs</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>Sets crs to use if clipping to study area</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>gridIN</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>Returns grid</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_grid(grid_path=None, grid_type=&#39;model&#39;, no_data_val_grid=0, use_service=False, study_area=None,  grid_crs=None, output_crs=&#39;EPSG:4269&#39;, verbose=False, log=False, **kwargs):
    &#34;&#34;&#34;Reads in grid

    Parameters
    ----------
    grid_path : str or pathlib.Path, default=None
        Path to a grid file
    grid_type : str, default=&#39;model&#39;
        Sets what type of grid to load in
    no_data_val_grid : int, default=0
        Sets the no data value of the grid
    use_service : str, default=False
        Sets which service the function uses
    study_area : geopandas.GeoDataFrame, default=None
        Dataframe containing study area polygon
    grid_crs : str, default=None
        Sets crs to use if clipping to study area
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    gridIN : xarray.DataArray
        Returns grid
    
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(read_grid, locals(), exclude_params=[&#39;study_area&#39;])
        
    if grid_type==&#39;model&#39;:
        if &#39;read_grid&#39; in list(kwargs.keys()):
            rgrid = kwargs[&#39;read_grid&#39;]
        else:
            rgrid=True
        gridIN = read_model_grid(model_grid_path=grid_path, study_area=study_area,  no_data_val_grid=0, read_grid=rgrid, grid_crs=grid_crs, output_crs=output_crs, verbose=verbose)
    else:
        if use_service==False:
            gridIN = rxr.open_rasterio(grid_path)
        elif use_service.lower()==&#39;wcs&#39;:
            gridIN = read_wcs(study_area, wcs_url=lidarURL, **kwargs)
        elif use_service.lower()==&#39;wms&#39;:
            gridIN = read_wms(study_area, wcs_url=lidarURL, **kwargs)
        elif use_service:
            #Deafults to wms
            gridIN = read_wms(study_area, wcs_url=lidarURL, **kwargs)
        
        if study_area is not None:
            study_area_crs = study_area.crs
            if grid_crs is None:
                try:
                    grid_crs = pyproj.CRS.from_wkt(gridIN.rio.crs.to_wkt())
                except Exception:
                    iswsCRS = pyproj.CRS.from_json_dict(w4h.read_dict(w4h.get_resources()[&#39;ISWS_CRS&#39;]))
                    gridIN.rio.write_crs(iswsCRS)
            elif grid_crs.lower()==&#39;isws&#39;:
                iswsCRS = pyproj.CRS.from_json_dict(w4h.read_dict(w4h.get_resources()[&#39;ISWS_CRS&#39;]))
                gridIN.rio.write_crs(iswsCRS)
                        
            if study_area_crs is None:
                study_area_crs=study_area.crs
            study_area = study_area.to_crs(output_crs)
            study_area_crs=study_area.crs
            
            gridIN = gridIN.rio.reproject(output_crs)
            gridIN = grid2study_area(study_area=study_area, grid=gridIN, output_crs=output_crs,)
        else:
            gridIN = gridIN.rio.reproject(output_crs)
            if &#39;band&#39; in gridIN.dims:
                gridIN = gridIN.sel(band=1)

        try:
            no_data_val_grid = gridIN.attrs[&#39;_FillValue&#39;] #Extract from dataset itself
        except:
            pass
                
        gridIN = gridIN.where(gridIN != no_data_val_grid, other=np.nan)  #Replace no data values with NaNs

    return gridIN</code></pre>
</details>
</dd>
<dt id="w4h.read_lithologies"><code class="name flex">
<span>def <span class="ident">read_lithologies</span></span>(<span>lith_file=None, interp_col='LITHOLOGY', target_col='CODE', use_cols=None, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read lithology file into pandas dataframe</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lith_file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, default <code>= None</code></dt>
<dd>Filename of lithology file. If None, default is contained within repository, by default None</dd>
<dt><strong><code>interp_col</code></strong> :&ensp;<code>str</code>, default <code>= 'LITHOLOGY'</code></dt>
<dd>Column to used to match interpretations</dd>
<dt><strong><code>target_col</code></strong> :&ensp;<code>str</code>, default <code>= 'CODE'</code></dt>
<dd>Column to be used as target code</dd>
<dt><strong><code>use_cols</code></strong> :&ensp;<code>list</code>, default <code>= None</code></dt>
<dd>Which columns to use when reading in dataframe. If None, defaults to ['LITHOLOGY', 'CODE'].</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with lithology information</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_lithologies(lith_file=None, interp_col=&#39;LITHOLOGY&#39;, target_col=&#39;CODE&#39;, use_cols=None, verbose=False,  log=False):
    &#34;&#34;&#34;Function to read lithology file into pandas dataframe

    Parameters
    ----------
    lith_file : str or pathlib.Path object, default = None
        Filename of lithology file. If None, default is contained within repository, by default None
    interp_col : str, default = &#39;LITHOLOGY&#39;
        Column to used to match interpretations
    target_col : str, default = &#39;CODE&#39;
        Column to be used as target code
    use_cols : list, default = None
        Which columns to use when reading in dataframe. If None, defaults to [&#39;LITHOLOGY&#39;, &#39;CODE&#39;].
    log : bool, default = True
        Whether to log inputs and outputs to log file.

    Returns
    -------
    pandas.DataFrame
        Pandas dataframe with lithology information
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(read_lithologies, locals())   
    if lith_file is None:
        #Find resources
        import w4h
        lith_file= w4h.get_resources()[&#39;LithInterps_FineCoarse&#39;]
    
    if not isinstance(lith_file, pathlib.PurePath):
        lith_file = pathlib.Path(lith_file)

    if use_cols is None:
        use_cols = [interp_col, target_col]

    lithoDF = pd.read_csv(lith_file, usecols=use_cols)

    lithoDF.rename(columns={interp_col:&#39;INTERPRETATION&#39;, target_col:&#39;TARGET&#39;}, inplace=True)

    return lithoDF</code></pre>
</details>
</dd>
<dt id="w4h.read_model_grid"><code class="name flex">
<span>def <span class="ident">read_model_grid</span></span>(<span>model_grid_path, study_area=None, no_data_val_grid=0, read_grid=True, node_byspace=True, grid_crs=None, output_crs='EPSG:4269', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads in model grid to xarray data array</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>grid_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to model grid file</dd>
<dt><strong><code>study_area</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code>, default=<code>None</code></dt>
<dd>Dataframe containing study area polygon</dd>
<dt><strong><code>no_data_val_grid</code></strong> :&ensp;<code>int</code>, default=<code>0</code></dt>
<dd>value assigned to areas with no data</dd>
<dt><strong><code>readGrid</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether function to either read grid or create grid</dd>
<dt><strong><code>node_byspace</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Denotes how to create grid</dd>
<dt><strong><code>output_crs</code></strong> :&ensp;<code>str</code>, default=<code>'EPSG:4269'</code></dt>
<dd>Inputs study area crs</dd>
<dt><strong><code>grid_crs</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>Inputs grid crs</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>modelGrid</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>Data array containing model grid</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_model_grid(model_grid_path, study_area=None, no_data_val_grid=0, read_grid=True, node_byspace=True, grid_crs=None, output_crs=&#39;EPSG:4269&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Reads in model grid to xarray data array

    Parameters
    ----------
    grid_path : str
        Path to model grid file
    study_area : geopandas.GeoDataFrame, default=None
        Dataframe containing study area polygon
    no_data_val_grid : int, default=0
        value assigned to areas with no data
    readGrid : bool, default=True
        Whether function to either read grid or create grid
    node_byspace : bool, default=False
        Denotes how to create grid
    output_crs : str, default=&#39;EPSG:4269&#39;
        Inputs study area crs
    grid_crs : str, default=None
        Inputs grid crs
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    modelGrid : xarray.DataArray
        Data array containing model grid
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(read_model_grid, locals(), exclude_params=&#39;study_area&#39;)
    
    if read_grid and model_grid_path is not None:
        modelGridIN = rxr.open_rasterio(model_grid_path)

        if grid_crs is None:
            try:
                grid_crs=modelGridIN.spatial_ref.crs_wkt
            except Exception:
                iswsCRSPath = w4h.get_resources()[&#39;ISWS_CRS&#39;]
                iswsCRS = w4h.read_dict(iswsCRSPath, keytype=None)
                grid_crs = iswsCRS              
                modelGridIN.rio.write_crs(grid_crs)
        elif isinstance(grid_crs, str) and grid_crs.lower()==&#39;isws&#39;:
            iswsCRSPath = w4h.get_resources()[&#39;ISWS_CRS&#39;]
            iswsCRS = w4h.read_dict(iswsCRSPath, keytype=None)            
            grid_crs = iswsCRS              
            modelGridIN.rio.write_crs(grid_crs)
        elif isinstance(grid_crs, pathlib.PurePath) or (isinstance(grid_crs, str) and pathlib.Path(grid_crs).exists()):
            iswsCRSPath = w4h.get_resources()[&#39;ISWS_CRS&#39;]
            grid_crs = w4h.read_dict(iswsCRSPath, keytype=None)            
            modelGridIN.rio.write_crs(grid_crs)
        else:
            warnings.warn(f&#39;CRS Specification for grid is {grid_crs}, but this cannot be written to the grid&#39;)
        
        modelGridIN = modelGridIN.rio.reproject(output_crs) 
           
        if study_area is not None:                
            study_area = study_area.to_crs(output_crs)
            modelGrid = grid2study_area(study_area=study_area, grid=modelGridIN, output_crs=output_crs)
        else:
            modelGrid = modelGridIN

        try:
            noDataVal = float(modelGrid.attrs[&#39;_FillValue&#39;]) #Extract from dataset itsel
        except:
            noDataVal = no_data_val_grid

        modelGrid = modelGrid.where(modelGrid != noDataVal, other=np.nan)   #Replace no data values with NaNs
        modelGrid = modelGrid.where(modelGrid == np.nan, other=1) #Replace all other values with 1
        modelGrid.rio.reproject(grid_crs, inplace=True)     
    elif model_grid_path is None and study_area is None:
        if verbose:
            print(&#34;ERROR: Either model_grid_path or study_area must be defined.&#34;)
    else:
        spatRefDict = w4h.read_dict(iswsCRSPath, keytype=None)            

        saExtent = study_area.total_bounds

        startX = saExtent[0] #Starting X Coordinate
        startY = saExtent[1] #starting Y Coordinate
        
        endX = saExtent[2]
        endY = saExtent[3]
        
        if node_byspace:
            xSpacing = 625 #X Node spacing 
            ySpacing = xSpacing #Y Node spacing  
            
            x = np.arange(startX, endX, xSpacing)
            y = np.arange(startY, endY, ySpacing)
        else:
            xNodes = 100 #Number of X Nodes
            yNodes = 100 #Number of Y Nodes

            x = np.linspace(startX, endX, num=xNodes)
            y = np.linspace(startY, endY, num=yNodes)        
        
        xx, yy = np.meshgrid(x, y)
        zz = np.ones_like(xx).transpose()

        yIn = np.flipud(y)

        coords = {&#39;x&#39;:x,&#39;y&#39;:yIn, &#39;spatial_ref&#39;:0}
        dims = {&#39;x&#39;:x,&#39;y&#39;:yIn}
        
        modelGrid = xr.DataArray(data=zz,coords=coords,attrs={&#39;_FillValue&#39;:no_data_val_grid}, dims=dims)
        modelGrid.spatial_ref.attrs[&#39;spatial_ref&#39;] = {}
        if grid_crs is None or grid_crs==&#39;isws&#39; or grid_crs==&#39;ISWS&#39;:
            for k in spatRefDict:
                modelGrid.spatial_ref.attrs[k] = spatRefDict[k]
    return modelGrid</code></pre>
</details>
</dd>
<dt id="w4h.read_raw_csv"><code class="name flex">
<span>def <span class="ident">read_raw_csv</span></span>(<span>data_filepath, metadata_filepath, data_cols=None, metadata_cols=None, xcol='LONGITUDE', ycol='LATITUDE', well_key='API_NUMBER', encoding='latin-1', verbose=False, log=False, **read_csv_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Easy function to read raw .txt files output from (for example), an Access database</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the file containing data, including the extension.</dd>
<dt><strong><code>metadata_filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the file containing metadata, including the extension.</dd>
<dt><strong><code>data_cols</code></strong> :&ensp;<code>list</code>, default <code>= None</code></dt>
<dd>List with strings with names of columns from txt file to keep after reading. If None, ["API_NUMBER","TABLE_NAME","FORMATION","THICKNESS","TOP","BOTTOM"], by default None.</dd>
<dt><strong><code>metadata_cols</code></strong> :&ensp;<code>list</code>, default <code>= None</code></dt>
<dd>List with strings with names of columns from txt file to keep after reading. If None, ['API_NUMBER',"TOTAL_DEPTH","SECTION","TWP","TDIR","RNG","RDIR","MERIDIAN","QUARTERS","ELEVATION","ELEVREF","COUNTY_CODE","LATITUDE","LONGITUDE","ELEVSOURCE"], by default None</dd>
<dt><strong><code>x_col</code></strong> :&ensp;<code>str</code>, default <code>= 'LONGITUDE'</code></dt>
<dd>Name of column in metadata file indicating the x-location of the well, by default 'LONGITUDE'</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default <code>= 'LATITUDE'</code></dt>
<dd>Name of the column in metadata file indicating the y-location of the well, by default 'LATITUDE'</dd>
<dt><strong><code>well_key</code></strong> :&ensp;<code>str</code>, default <code>= 'API_NUMBER'</code></dt>
<dd>Name of the column with the key/identifier that will be used to merge data later, by default 'API_NUMBER'</dd>
<dt><strong><code>encoding</code></strong> :&ensp;<code>str</code>, default <code>= 'latin-1'</code></dt>
<dd>Encoding of the data in the input files, by default 'latin-1'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print the number of rows in the input columns, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
<dt><strong><code>**read_csv_kwargs</code></strong></dt>
<dd>**kwargs that get passed to pd.read_csv()</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(pandas.DataFrame, pandas.DataFrame/None)
Tuple/list with two pandas dataframes: (well_data, metadata) metadata is None if only well_data is used</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_raw_csv(data_filepath, metadata_filepath, data_cols=None, metadata_cols=None, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, well_key=&#39;API_NUMBER&#39;, encoding=&#39;latin-1&#39;, verbose=False, log=False, **read_csv_kwargs):
    &#34;&#34;&#34;Easy function to read raw .txt files output from (for example), an Access database

    Parameters
    ----------
    data_filepath : str
        Filename of the file containing data, including the extension.
    metadata_filepath : str
        Filename of the file containing metadata, including the extension.
    data_cols : list, default = None
        List with strings with names of columns from txt file to keep after reading. If None, [&#34;API_NUMBER&#34;,&#34;TABLE_NAME&#34;,&#34;FORMATION&#34;,&#34;THICKNESS&#34;,&#34;TOP&#34;,&#34;BOTTOM&#34;], by default None.
    metadata_cols : list, default = None
        List with strings with names of columns from txt file to keep after reading. If None, [&#39;API_NUMBER&#39;,&#34;TOTAL_DEPTH&#34;,&#34;SECTION&#34;,&#34;TWP&#34;,&#34;TDIR&#34;,&#34;RNG&#34;,&#34;RDIR&#34;,&#34;MERIDIAN&#34;,&#34;QUARTERS&#34;,&#34;ELEVATION&#34;,&#34;ELEVREF&#34;,&#34;COUNTY_CODE&#34;,&#34;LATITUDE&#34;,&#34;LONGITUDE&#34;,&#34;ELEVSOURCE&#34;], by default None
    x_col : str, default = &#39;LONGITUDE&#39;
        Name of column in metadata file indicating the x-location of the well, by default &#39;LONGITUDE&#39;
    ycol : str, default = &#39;LATITUDE&#39;
        Name of the column in metadata file indicating the y-location of the well, by default &#39;LATITUDE&#39;
    well_key : str, default = &#39;API_NUMBER&#39;
        Name of the column with the key/identifier that will be used to merge data later, by default &#39;API_NUMBER&#39;
    encoding : str, default = &#39;latin-1&#39;
        Encoding of the data in the input files, by default &#39;latin-1&#39;
    verbose : bool, default = False
        Whether to print the number of rows in the input columns, by default False
    log : bool, default = False
        Whether to log inputs and outputs to log file.
    **read_csv_kwargs
        **kwargs that get passed to pd.read_csv()

    Returns
    -------
    (pandas.DataFrame, pandas.DataFrame/None)
        Tuple/list with two pandas dataframes: (well_data, metadata) metadata is None if only well_data is used
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(read_raw_csv, locals())
    #Check if input data is already dataframe, otherwise, read it in as dataframe
    if not isinstance(data_filepath, pd.DataFrame) or isinstance(data_filepath, gpd.GeoDataFrame):
        downholeDataIN = pd.read_csv(data_filepath, sep=&#39;,&#39;, header=&#39;infer&#39;, encoding=encoding, **read_csv_kwargs)

    if data_cols is None:
        data_useCols = downholeDataIN.columns
    elif data_cols == &#39;auto&#39;:
        data_useCols = [&#34;API_NUMBER&#34;,&#34;TABLE_NAME&#34;,&#34;FORMATION&#34;,&#34;THICKNESS&#34;,&#34;TOP&#34;,&#34;BOTTOM&#34;]
    else:
        data_useCols= data_cols
    downholeDataIN = downholeDataIN[data_useCols]

    if metadata_filepath is None:
        headerDataIN = None
    elif not isinstance(metadata_filepath, pd.DataFrame) or isinstance(metadata_filepath, gpd.GeoDataFrame):
        headerDataIN = pd.read_csv(metadata_filepath, sep=&#39;,&#39;, header=&#39;infer&#39;, encoding=encoding, **read_csv_kwargs)
    else:
        headerDataIN = None

    if metadata_cols is None and headerDataIN is not None:
        metadata_useCols = headerDataIN.columns
    elif metadata_cols == &#39;auto&#39;:
        metadata_useCols = [&#39;API_NUMBER&#39;,&#34;TOTAL_DEPTH&#34;,&#34;SECTION&#34;,&#34;TWP&#34;,&#34;TDIR&#34;,&#34;RNG&#34;,&#34;RDIR&#34;,&#34;MERIDIAN&#34;,&#34;QUARTERS&#34;,&#34;ELEVATION&#34;,&#34;ELEVREF&#34;,&#34;COUNTY_CODE&#34;,&#34;LATITUDE&#34;,&#34;LONGITUDE&#34;,&#34;ELEVSOURCE&#34;]
    else:
        metadata_useCols= metadata_cols

    if headerDataIN is not None:
        headerDataIN = headerDataIN[metadata_useCols]
        
    #Drop data with no API        
    downholeDataIN = downholeDataIN.dropna(subset=[well_key]) #Drop data with no API
    if headerDataIN is not None:
        headerDataIN = headerDataIN.dropna(subset=[well_key]) #Drop metadata with no API

        #Drop data with no or missing location information
        headerDataIN = headerDataIN.dropna(subset=[ycol]) 
        headerDataIN = headerDataIN.dropna(subset=[xcol])

        #Reset index so index goes from 0 in numerical/integer order
        headerDataIN.reset_index(inplace=True, drop=True)
    else:
        #***UPDATE: Need to make sure these columns exist in this case***
        #Drop data with no or missing location information
        downholeDataIN = downholeDataIN.dropna(subset=[ycol]) 
        downholeDataIN = downholeDataIN.dropna(subset=[xcol])        
    downholeDataIN.reset_index(inplace=True, drop=True)
    
    #Print outputs to terminal, if designated
    if verbose:
        print(&#39;Data file has &#39; + str(downholeDataIN.shape[0])+&#39; valid well records.&#39;)
        if headerDataIN is not None:
            print(&#34;Metadata file has &#34;+str(headerDataIN.shape[0])+&#34; unique wells with valid location information.&#34;)
    
    return downholeDataIN, headerDataIN</code></pre>
</details>
</dd>
<dt id="w4h.read_study_area"><code class="name flex">
<span>def <span class="ident">read_study_area</span></span>(<span>study_area=None, output_crs='EPSG:4269', buffer=None, return_original=False, log=False, verbose=False, **read_file_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Read study area geospatial file into geopandas</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>study_area</code></strong> :&ensp;<code>str, pathlib.Path, geopandas.GeoDataFrame,</code> or <code>shapely.Geometry</code></dt>
<dd>Filepath to any geospatial file readable by geopandas.
Polygon is best, but may work with other types if extent is correct.</dd>
<dt><strong><code>study_area_crs</code></strong> :&ensp;<code>str, tuple, dict</code>, optional</dt>
<dd>CRS designation readable by geopandas/pyproj</dd>
<dt><strong><code>buffer</code></strong> :&ensp;<code>None</code> or <code>numeric</code>, default=<code>None</code></dt>
<dd>If None, no buffer created. If a numeric value is given (float or int, for example), a buffer will be created at that distance in the unit of the study_area_crs.</dd>
<dt><strong><code>return_original</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to return the (reprojected) study area as well as the (reprojected) buffered study area. Study area is only used for clipping data, so usually return_original=False is sufficient.</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to print status and results to terminal</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>studyAreaIN</code></strong> :&ensp;<code>geopandas dataframe</code></dt>
<dd>Geopandas dataframe with polygon geometry.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_study_area(study_area=None, output_crs=&#39;EPSG:4269&#39;, buffer=None, return_original=False, log=False, verbose=False, **read_file_kwargs):
    &#34;&#34;&#34;Read study area geospatial file into geopandas

    Parameters
    ----------
    study_area : str, pathlib.Path, geopandas.GeoDataFrame, or shapely.Geometry
        Filepath to any geospatial file readable by geopandas. 
        Polygon is best, but may work with other types if extent is correct.
    study_area_crs : str, tuple, dict, optional
        CRS designation readable by geopandas/pyproj
    buffer : None or numeric, default=None
        If None, no buffer created. If a numeric value is given (float or int, for example), a buffer will be created at that distance in the unit of the study_area_crs.
    return_original : bool, default=False
        Whether to return the (reprojected) study area as well as the (reprojected) buffered study area. Study area is only used for clipping data, so usually return_original=False is sufficient.
    log : bool, default = False
        Whether to log results to log file, by default False
    verbose : bool, default=False
        Whether to print status and results to terminal

    Returns
    -------
    studyAreaIN : geopandas dataframe
        Geopandas dataframe with polygon geometry.
    &#34;&#34;&#34;
    
    if study_area is None:
        study_area = w4h.get_resources()[&#39;study_area&#39;]

    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(read_study_area, locals())
    
    if isinstance(study_area, (gpd.GeoDataFrame, gpd.GeoSeries)):
        studyAreaIN = study_area
    elif isinstance(study_area, shapely.Geometry):
        if &#39;crs&#39; in read_file_kwargs:
            crs = read_file_kwargs[&#39;crs&#39;]
        else:
            warnings.warn(&#39;A shapely Geometry object was read in as the study area, but no crs specified. Using CRS specified in output_crs: {output_crs}.\
                          You can specify the crs as a keyword argument in the read_study_area() function&#39;)
            crs = output_crs
        studyAreaIN = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[study_area])
    else:
        studyAreaIN = gpd.read_file(study_area, **read_file_kwargs)
    studyAreaIN.to_crs(output_crs, inplace=True)

    # Create a buffered study area for clipping
    if buffer is not None:
        if return_original:
            studyAreaNoBuffer = studyAreaIN
        studyAreaIN = studyAreaIN.buffer(distance=buffer)
        
        if verbose:
            print(&#39;\tBuffer applied.&#39;)
    
    if verbose:
        print(&#34;\tStudy area read.&#34;)

    if return_original:
        return studyAreaIN, studyAreaNoBuffer
    return studyAreaIN</code></pre>
</details>
</dd>
<dt id="w4h.read_wcs"><code class="name flex">
<span>def <span class="ident">read_wcs</span></span>(<span>study_area, wcs_url='https://data.isgs.illinois.edu/arcgis/services/Elevation/IL_Statewide_Lidar_DEM_WGS/ImageServer/WCSServer?request=GetCapabilities&service=WCS', res_x=30, res_y=30, verbose=False, log=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads a WebCoverageService from a url and returns a rioxarray dataset containing it.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>study_area</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>Dataframe containing study area polygon</dd>
<dt><strong><code>wcs_url</code></strong> :&ensp;<code>str</code>, default=<code>lidarURL</code></dt>
<dd>&nbsp;</dd>
<dt>Represents the url for the WCS</dt>
<dt><strong><code>res_x</code></strong> :&ensp;<code>int</code>, default=<code>30</code></dt>
<dd>Sets resolution for x axis</dd>
<dt><strong><code>res_y</code></strong> :&ensp;<code>int</code>, default=<code>30</code></dt>
<dd>Sets resolution for y axis</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>wcsData_rxr</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>A xarray dataarray holding the image from the WebCoverageService</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_wcs(study_area, wcs_url=lidarURL, res_x=30, res_y=30, verbose=False, log=False, **kwargs):
    &#34;&#34;&#34;Reads a WebCoverageService from a url and returns a rioxarray dataset containing it.

    Parameters
    ----------
    study_area : geopandas.GeoDataFrame
        Dataframe containing study area polygon
    wcs_url : str, default=lidarURL
    Represents the url for the WCS
    res_x : int, default=30
        Sets resolution for x axis
    res_y : int, default=30
        Sets resolution for y axis
    log : bool, default = False
        Whether to log results to log file, by default False
    **kwargs

    Returns
    -------
    wcsData_rxr : xarray.DataArray
        A xarray dataarray holding the image from the WebCoverageService
    &#34;&#34;&#34;
    #Drawn largely from: https://git.wur.nl/isric/soilgrids/soilgrids.notebooks/-/blob/master/01-WCS-basics.ipynb
    
    #30m DEM
    #wcs_url = r&#39;https://data.isgs.illinois.edu/arcgis/services/Elevation/IL_DEM_30M/ImageServer/WCSServer?request=GetCapabilities&amp;service=WCS&#39;
    #lidar url:
    #lidarURL = r&#39;https://data.isgs.illinois.edu/arcgis/services/Elevation/IL_Statewide_Lidar_DEM_WGS/ImageServer/WCSServer?request=GetCapabilities&amp;service=WCS&#39;

    #studyAreaPath = r&#34;\\isgs-sinkhole.ad.uillinois.edu\geophysics\Balikian\ISWS_HydroGeo\WellDataAutoClassification\SampleData\ESL_StudyArea_5mi.shp&#34;
    #study_area = gpd.read_file(studyAreaPath)
    if study_area is None:
        print(&#39;ERROR: study_area must be specified to use read_wcs (currently set to {})&#39;.format(study_area))
        return

    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if verbose:
        verbose_print(read_wcs, locals(), exclude_params=[&#39;study_area&#39;])      

    if &#39;wcs_url&#39; in kwargs:
        wcs_url = kwargs[&#39;wcs_url&#39;]
    if &#39;res_x&#39; in kwargs:
        res_x = kwargs[&#39;res_x&#39;]
    if &#39;res_y&#39; in kwargs:
        res_y = kwargs[&#39;res_y&#39;]
    
    width_in = &#39;&#39;
    height_in= &#39;&#39;

    #Create coverage object
    my_wcs = WebCoverageService(wcs_url, version=&#39;1.0.0&#39;) 
    #names = [k for k in my_wcs.contents.keys()]
    #print(names)
    dataID = &#39;IL_Statewide_Lidar_DEM&#39;
    data = my_wcs.contents[dataID]
    dBBox = data.boundingboxes #Is this an error?
    
    study_area = study_area.to_crs(data.boundingboxes[0][&#39;nativeSrs&#39;])
    saBBox = study_area.total_bounds
    
    #In case study area bounding box goes outside data bounding box, use data bounding box values
    newBBox = []
    for i,c in enumerate(dBBox[0][&#39;bbox&#39;]):
        if i == 0 or i==2:
            if saBBox[i] &lt; c:
                newBBox.append(saBBox[i])
            else:
                newBBox.append(c)
        else:
            if saBBox[i] &gt; c:
                newBBox.append(saBBox[i])
            else:
                newBBox.append(c)

    #Recalculate resolution if it is too fine to read in
    #Start by getting the area of the study area bounding box
    saWidth = saBBox[2]-saBBox[0]
    saHeight = saBBox[3]-saBBox[1]
    saBBoxAreaM = saWidth*saHeight
    saBBoxAreaKM = saBBoxAreaM/(1000*1000) #Area in km^2

    if saBBoxAreaM/(res_x*res_y) &gt; (4100*15000)*0.457194: #What I think might be the max download size?
        print(&#34;Resolution inputs overriden, file request too large.&#34;)
        res_x=str(round(saWidth/2500, 2))

        width_in  = str(int(saWidth/float(res_x )))
        height_in = str(int(saHeight/float(res_x)))
        
        res_y=str(round(saHeight/height_in, 2))

        print(&#39;New resolution is: &#39;+res_x+&#39;m_x X &#39;+res_y+&#39;m_y&#39; )
        print(&#39;Dataset size: &#39;+width_in+&#39; pixels_x X &#39;+height_in+&#39; pixels_y&#39;)

    bBox = tuple(newBBox)
    bBox_str = str(tuple(newBBox)[1:-1]).replace(&#39; &#39;,&#39;&#39;)
    dataCRS = &#39;EPSG:3857&#39;

    #Format WCS request using owslib
    response = my_wcs.getCoverage(
        identifier=my_wcs[dataID].id, 
        crs=dataCRS,#&#39;urn:ogc:def:crs:EPSG::26716&#39;,
        bbox=bBox,
        resx=res_x, 
        resy=res_y,
        timeout=60,
        #width = width_in, height=height_in,
        format=&#39;GeoTIFF&#39;)
    response

    #If I can figure out url, this might be better?
    #baseURL = r&#39;https://data.isgs.illinois.edu/arcgis/services/Elevation/&#39;+dataID+&#39;/ImageServer/WCSServer&#39;
    #addonRequestURL = &#39;?request=GetCoverage&amp;service=WCS&amp;bbox=&#39;+bBox_str+&#39;&amp;srs=&#39;+dataCRS+&#39;&amp;format=GeoTIFF&#39;+&#39;&amp;WIDTH=&#39;+width_in+&#39;&amp;HEIGHT=&#39;+height_in+&#39;)&#39;
    #reqURL = baseURL+addonRequestURL
    #wcsData_rxr =  rxr.open_rasterio(reqURL)

    with MemoryFile(response) as memfile:
        with memfile.open() as dataset:
            wcsData_rxr =  rxr.open_rasterio(dataset)

    return wcsData_rxr</code></pre>
</details>
</dd>
<dt id="w4h.read_wms"><code class="name flex">
<span>def <span class="ident">read_wms</span></span>(<span>study_area, layer_name='IL_Statewide_Lidar_DEM_WGS:None', wms_url='https://data.isgs.illinois.edu/arcgis/services/Elevation/IL_Statewide_Lidar_DEM_WGS/ImageServer/WCSServer?request=GetCapabilities&service=WCS', srs='EPSG:3857', clip_to_studyarea=True, bbox=[-9889002.6155, 5134541.069716, -9737541.607038, 5239029.6274], res_x=30, res_y=30, size_x=512, size_y=512, format='image/tiff', verbose=False, log=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads a WebMapService from a url and returns a rioxarray dataset containing it.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>study_area</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>Dataframe containg study area polygon</dd>
<dt><strong><code>layer_name</code></strong> :&ensp;<code>str</code>, default=<code>'IL_Statewide_Lidar_DEM_WGS:None'</code></dt>
<dd>Represents the layer name in the WMS</dd>
<dt><strong><code>wms_url</code></strong> :&ensp;<code>str</code>, default=<code>lidarURL</code></dt>
<dd>Represents the url for the WMS</dd>
<dt><strong><code>srs</code></strong> :&ensp;<code>str</code>, default=<code>'EPSG:3857'</code></dt>
<dd>Sets the srs</dd>
<dt><strong><code>clip_to_studyarea</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to clip to study area or not</dd>
<dt><strong><code>res_x</code></strong> :&ensp;<code>int</code>, default=<code>30</code></dt>
<dd>Sets resolution for x axis</dd>
<dt><strong><code>res_y</code></strong> :&ensp;<code>int</code>, default=<code>512</code></dt>
<dd>Sets resolution for y axis</dd>
<dt><strong><code>size_x</code></strong> :&ensp;<code>int</code>, default=<code>512</code></dt>
<dd>Sets width of result</dd>
<dt><strong><code>size_y</code></strong> :&ensp;<code>int</code>, default=<code>512</code></dt>
<dd>Sets height of result</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>wmsData_rxr</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>Holds the image from the WebMapService</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_wms(study_area, layer_name=&#39;IL_Statewide_Lidar_DEM_WGS:None&#39;, wms_url=lidarURL, srs=&#39;EPSG:3857&#39;, 
             clip_to_studyarea=True, bbox=[-9889002.615500,5134541.069716,-9737541.607038,5239029.627400],
             res_x=30, res_y=30, size_x=512, size_y=512, 
             format=&#39;image/tiff&#39;, verbose=False, log=False, **kwargs):
    &#34;&#34;&#34;
    Reads a WebMapService from a url and returns a rioxarray dataset containing it.

    Parameters
    ----------
    study_area : geopandas.GeoDataFrame
        Dataframe containg study area polygon
    layer_name : str, default=&#39;IL_Statewide_Lidar_DEM_WGS:None&#39;
        Represents the layer name in the WMS
    wms_url : str, default=lidarURL
        Represents the url for the WMS
    srs : str, default=&#39;EPSG:3857&#39;
        Sets the srs
    clip_to_studyarea : bool, default=True
        Whether to clip to study area or not
    res_x : int, default=30
        Sets resolution for x axis
    res_y : int, default=512
        Sets resolution for y axis
    size_x : int, default=512
        Sets width of result
    size_y : int, default=512
        Sets height of result
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    wmsData_rxr : xarray.DataArray
        Holds the image from the WebMapService
    &#34;&#34;&#34;
    if study_area is None:
        print(&#39;ERROR: study_area must be specified to use read_wms (currently set to {})&#39;.format(study_area))
        return
    
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(read_wms, locals(), exclude_params=[&#39;study_area&#39;])
    from owslib.wms import WebMapService
    # Define WMS endpoint URL
    if &#39;wms_url&#39; in kwargs:
        wms_url = kwargs[&#39;wms_url&#39;]
    else:
        wms_url = wms_url

    # Create WMS connection object
    wms = WebMapService(wms_url)
    # Print available layers
    #print(wms.contents)

    # Select desired layer
    if &#39;layer_name&#39; in kwargs:
        layer = kwargs[&#39;layer_name&#39;]
    else:
        layer = layer_name
    
    data = wms.contents#[layer]
    if &#39;srs&#39; in kwargs:
        studyArea_proj = study_area.to_crs(kwargs[&#39;srs&#39;])
        saBBox = studyArea_proj.total_bounds
    else:
        studyArea_proj = study_area.to_crs(srs)
    
    saBBox = studyArea_proj.total_bounds

    if layer == &#39;IL_Statewide_Lidar_DEM_WGS:None&#39;:
        dBBox = data[&#39;0&#39;].boundingBox #Is this an error?

        gpdDict = {&#39;Label&#39;: [&#39;Surf Data Box&#39;], &#39;geometry&#39;: [shapely.geometry.Polygon(((dBBox[0], dBBox[1]), (dBBox[0], dBBox[3]), (dBBox[2], dBBox[3]), (dBBox[2], dBBox[1]), (dBBox[0], dBBox[1])))]}
        dBBoxGDF = gpd.GeoDataFrame(gpdDict, crs=dBBox[4])
        dBBoxGDF.to_crs(srs)

        #In case study area bounding box goes outside data bounding box, use data bounding box values
        newBBox = []
        for i,c in enumerate(dBBox):
            if type(c) is str:
                pass
            elif i == 0 or i==2:
                if saBBox[i] &lt; c:
                    newBBox.append(saBBox[i])
                else:
                    newBBox.append(c)
            else:
                if saBBox[i] &gt; c:
                    newBBox.append(saBBox[i])
                else:
                    newBBox.append(c)

    saWidth = saBBox[2]-saBBox[0]
    saHeight = saBBox[3]-saBBox[1]    
    # Check kwargs for rest of parameters
    if &#39;size_x&#39; in kwargs:
        size_x = kwargs[&#39;size_x&#39;]
    if &#39;size_y&#39; in kwargs:
        size_y = kwargs[&#39;size_y&#39;]
    if &#39;format&#39; in kwargs:
        format = kwargs[&#39;format&#39;]
    if &#39;clip_to_studyarea&#39; in kwargs:
        clip_to_studyarea = kwargs[&#39;clip_to_studyarea&#39;]
   
    #get the wms
    if clip_to_studyarea:
        img = wms.getmap(layers=[layer], srs=srs, bbox=saBBox, size=(size_x, size_y), format=format, transparent=True, timeout=60)        
    else:
        img = wms.getmap(layers=[layer], srs=srs, bbox=bbox, size=(size_x, size_y), format=format, transparent=True, timeout=60)

    #Save wms in memory to a raster dataset
    with MemoryFile(img) as memfile:
        with memfile.open() as dataset:
            wmsData_rxr = rxr.open_rasterio(dataset)

    #if clip_to_studyarea:
    #    wmsData_rxr = wmsData_rxr.sel(x=slice(saBBox[0], saBBox[2]), y=slice(saBBox[3], saBBox[1]))#.sel(band=1)

    return wmsData_rxr</code></pre>
</details>
</dd>
<dt id="w4h.read_xyz"><code class="name flex">
<span>def <span class="ident">read_xyz</span></span>(<span>xyzpath, datatypes=None, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read file containing xyz data (elevation/location)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>xyzpath</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code></dt>
<dd>Filepath of the xyz file, including extension</dd>
<dt><strong><code>datatypes</code></strong> :&ensp;<code>dict</code>, default <code>= None</code></dt>
<dd>Dictionary containing the datatypes for the columns int he xyz file. If None, {'ID':np.uint32,'API_NUMBER':np.uint64,'LATITUDE':np.float64,'LONGITUDE':np.float64,'ELEV_FT':np.float64}, by default None</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print the number of xyz records to the terminal, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing the elevation and location data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_xyz(xyzpath, datatypes=None, verbose=False, log=False):
    &#34;&#34;&#34;Function to read file containing xyz data (elevation/location)

    Parameters
    ----------
    xyzpath : str or pathlib.Path
        Filepath of the xyz file, including extension
    datatypes : dict, default = None
        Dictionary containing the datatypes for the columns int he xyz file. If None, {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#39;LATITUDE&#39;:np.float64,&#39;LONGITUDE&#39;:np.float64,&#39;ELEV_FT&#39;:np.float64}, by default None
    verbose : bool, default = False
        Whether to print the number of xyz records to the terminal, by default False
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    pandas.DataFrame
        Pandas dataframe containing the elevation and location data
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(read_xyz, locals())
    if datatypes is None:
        xyzDTypes = {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#39;LATITUDE&#39;:np.float64,&#39;LONGITUDE&#39;:np.float64,&#39;ELEV_FT&#39;:np.float64}

    xyzDataIN = pd.read_csv(xyzpath, sep=&#39;,&#39;, header=&#39;infer&#39;, dtype=xyzDTypes, index_col=&#39;ID&#39;)
    
    if verbose:
        print(&#39;XYZ file has &#39; + str(xyzDataIN.shape[0])+&#39; records with elevation and location.&#39;)
    return xyzDataIN</code></pre>
</details>
</dd>
<dt id="w4h.remerge_data"><code class="name flex">
<span>def <span class="ident">remerge_data</span></span>(<span>classifieddf, searchdf)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to merge newly-classified (or not) and previously classified data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>classifieddf</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe that had already been classified previously</dd>
<dt><strong><code>searchdf</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe with new classifications</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>remergeDF</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all the data, merged back together</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remerge_data(classifieddf, searchdf):
    &#34;&#34;&#34;Function to merge newly-classified (or not) and previously classified data

    Parameters
    ----------
    classifieddf : pandas.DataFrame
        Dataframe that had already been classified previously
    searchdf : pandas.DataFrame
        Dataframe with new classifications

    Returns
    -------
    remergeDF : pandas.DataFrame
        Dataframe containing all the data, merged back together
    &#34;&#34;&#34;
    remergeDF = pd.concat([classifieddf,searchdf], join=&#39;inner&#39;).sort_index()
    return remergeDF</code></pre>
</details>
</dd>
<dt id="w4h.remove_bad_depth"><code class="name flex">
<span>def <span class="ident">remove_bad_depth</span></span>(<span>df_with_depth, top_col='TOP', bottom_col='BOTTOM', depth_type='depth', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to remove all records in the dataframe with well interpretations where the depth information is bad (i.e., where the bottom of the record is neerer to the surface than the top)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df_with_depth</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing the well records and descriptions for each interval</dd>
<dt><strong><code>top_col</code></strong> :&ensp;<code>str</code>, default=<code>'TOP'</code></dt>
<dd>The name of the column containing the depth or elevation for the top of the interval, by default 'TOP'</dd>
<dt><strong><code>bottom_col</code></strong> :&ensp;<code>str</code>, default=<code>'BOTTOM'</code></dt>
<dd>The name of the column containing the depth or elevation for the bottom of each interval, by default 'BOTTOM'</dd>
<dt><strong><code>depth_type</code></strong> :&ensp;<code>str, {'depth', 'elevation'}</code></dt>
<dd>Whether the table is organized by depth or elevation. If depth, the top column will have smaller values than the bottom column. If elevation, the top column will have higher values than the bottom column, by default 'depth'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print results to the terminal, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Dataframe</code></dt>
<dd>Pandas dataframe with the records remvoed where the top is indicatd to be below the bottom.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_bad_depth(df_with_depth, top_col=&#39;TOP&#39;, bottom_col=&#39;BOTTOM&#39;, depth_type=&#39;depth&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to remove all records in the dataframe with well interpretations where the depth information is bad (i.e., where the bottom of the record is neerer to the surface than the top)

    Parameters
    ----------
    df_with_depth : pandas.DataFrame
        Pandas dataframe containing the well records and descriptions for each interval
    top_col : str, default=&#39;TOP&#39;
        The name of the column containing the depth or elevation for the top of the interval, by default &#39;TOP&#39;
    bottom_col : str, default=&#39;BOTTOM&#39;
        The name of the column containing the depth or elevation for the bottom of each interval, by default &#39;BOTTOM&#39;
    depth_type : str, {&#39;depth&#39;, &#39;elevation&#39;}
        Whether the table is organized by depth or elevation. If depth, the top column will have smaller values than the bottom column. If elevation, the top column will have higher values than the bottom column, by default &#39;depth&#39;
    verbose : bool, default = False
        Whether to print results to the terminal, by default False
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    pandas.Dataframe
        Pandas dataframe with the records remvoed where the top is indicatd to be below the bottom.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(remove_bad_depth, locals(), exclude_params=[&#39;df_with_depth&#39;])

    if depth_type.lower() ==&#39;depth&#39;:
        df_with_depth[&#39;THICKNESS&#39;] = df_with_depth[bottom_col] - df_with_depth[top_col] #Calculate interval thickness
    elif depth_type.lower() ==&#39;elevation&#39; or depth_type==&#39;elev&#39;:
        df_with_depth[&#39;THICKNESS&#39;] = df_with_depth[top_col] - df_with_depth[bottom_col] #Calculate interval thickness
    before = df_with_depth.shape[0] #Calculate number of rows before dropping
    df_with_depth = df_with_depth[(df_with_depth[&#39;THICKNESS&#39;] &gt;= 0)] #Only include rows where interval thickness is positive (bottom is deeper than top)
    df_with_depth.reset_index(inplace=True, drop=True) #Reset index

    if verbose:
        after = df_with_depth.shape[0]
        print(&#39;Removed well records with obviously bad depth information. &#39;)
        print(&#34;\tNumber of records before removing: &#34;+str(before))
        print(&#34;\tNumber of records after removing: &#34;+str(after))
        print(&#34;\t\t{} well records removed without depth information&#34;.format(before-after))

    return df_with_depth</code></pre>
</details>
</dd>
<dt id="w4h.remove_no_depth"><code class="name flex">
<span>def <span class="ident">remove_no_depth</span></span>(<span>df_with_depth, top_col='TOP', bottom_col='BOTTOM', no_data_val_table='', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to remove well intervals with no depth information</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df_with_depth</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing well descriptions</dd>
<dt><strong><code>top_col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of column containing information on the top of the well intervals, by default 'TOP'</dd>
<dt><strong><code>bottom_col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of column containing information on the bottom of the well intervals, by default 'BOTTOM'</dd>
<dt><strong><code>no_data_val_table</code></strong> :&ensp;<code>any</code>, optional</dt>
<dd>No data value in the input data, used by this function to indicate that depth data is not there, to be replaced by np.nan, by default ''</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to print results to console, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df_with_depth</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe with depths dropped</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_no_depth(df_with_depth, top_col=&#39;TOP&#39;, bottom_col=&#39;BOTTOM&#39;, no_data_val_table=&#39;&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to remove well intervals with no depth information

    Parameters
    ----------
    df_with_depth : pandas.DataFrame
        Dataframe containing well descriptions
    top_col : str, optional
        Name of column containing information on the top of the well intervals, by default &#39;TOP&#39;
    bottom_col : str, optional
        Name of column containing information on the bottom of the well intervals, by default &#39;BOTTOM&#39;
    no_data_val_table : any, optional
        No data value in the input data, used by this function to indicate that depth data is not there, to be replaced by np.nan, by default &#39;&#39;
    verbose : bool, optional
        Whether to print results to console, by default False
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    df_with_depth : pandas.DataFrame
        Dataframe with depths dropped
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if verbose:
        verbose_print(remove_no_depth, locals(), exclude_params=[&#39;df_with_depth&#39;])
        
    #Replace empty cells in top and bottom columns with nan
    df_with_depth[top_col] = df_with_depth[top_col].replace(no_data_val_table, np.nan)
    df_with_depth[bottom_col] = df_with_depth[bottom_col].replace(no_data_val_table, np.nan)
    
    #Calculate number of rows before dropping
    before = df_with_depth.shape[0]

    #Drop records without depth information
    df_with_depth = df_with_depth.dropna(subset=[top_col])
    df_with_depth = df_with_depth.dropna(subset=[bottom_col])
    df_with_depth.reset_index(inplace=True, drop=True) #Reset index
  
    if verbose:
        after = df_with_depth.shape[0]
        print(&#39;Removed well records with no depth information. &#39;)
        print(&#34;\tNumber of records before removing: &#34;+str(before))
        print(&#34;\tNumber of records after removing: &#34;+str(after))
        print(&#34;\t\t{} well records removed without depth information&#34;.format(before-after))
    
    return df_with_depth</code></pre>
</details>
</dd>
<dt id="w4h.remove_no_description"><code class="name flex">
<span>def <span class="ident">remove_no_description</span></span>(<span>df_with_descriptions, description_col='FORMATION', no_data_val_table='', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function that removes all records in the dataframe containing the well descriptions where no description is given.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df_with_descriptions</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing the well records with their individual descriptions</dd>
<dt><strong><code>description_col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of the column containing the geologic description of each interval, by default 'FORMATION'</dd>
<dt><strong><code>no_data_val_table</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The value expected if the column is empty or there is no data. These will be replaced by np.nan before being removed, by default ''</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to print the results of this step to the terminal, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with records with no description removed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_no_description(df_with_descriptions, description_col=&#39;FORMATION&#39;, no_data_val_table=&#39;&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function that removes all records in the dataframe containing the well descriptions where no description is given.

    Parameters
    ----------
    df_with_descriptions : pandas.DataFrame
        Pandas dataframe containing the well records with their individual descriptions
    description_col : str, optional
        Name of the column containing the geologic description of each interval, by default &#39;FORMATION&#39;
    no_data_val_table : str, optional
        The value expected if the column is empty or there is no data. These will be replaced by np.nan before being removed, by default &#39;&#39;
    verbose : bool, optional
        Whether to print the results of this step to the terminal, by default False
    log : bool, default = False
        Whether to log results to log file, by default False
        
    Returns
    -------
    pandas.DataFrame
        Pandas dataframe with records with no description removed.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(remove_no_description, locals(), exclude_params=[&#39;df_with_descriptions&#39;])
    #Replace empty cells in formation column with nans
    df_with_descriptions[description_col] = df_with_descriptions[description_col].replace(no_data_val_table, np.nan) 
    before = df_with_descriptions.shape[0] #Calculate number of rows before dropping

    #Drop records without FORMATION information
    df_with_descriptions = df_with_descriptions.dropna(subset=[description_col])
    df_with_descriptions.reset_index(inplace=True, drop=True) #Reset index

    if verbose:
        after = df_with_descriptions.shape[0]
        print(&#39;Removed well records without geologic descriptions. &#39;)
        print(&#34;\tNumber of records before removing: &#34;+str(before))
        print(&#34;\tNumber of records after removing: &#34;+str(after))
        print(&#34;\t\t{} well records removed without geologic descriptions&#34;.format(before-after))

    return df_with_descriptions</code></pre>
</details>
</dd>
<dt id="w4h.remove_no_topo"><code class="name flex">
<span>def <span class="ident">remove_no_topo</span></span>(<span>df_with_topo, zcol='ELEVATION', no_data_val_table='', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to remove wells that do not have topography data (needed for layer selection later).</p>
<p>This function is intended to be run on the metadata table after elevations have attempted to been added.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df_with_topo</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing elevation information.</dd>
<dt><strong><code>zcol</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of elevation column</dd>
<dt><strong><code>no_data_val_table</code></strong> :&ensp;<code>any</code></dt>
<dd>Value in dataset that indicates no data is present (replaced with np.nan)</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to print outputs, by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with intervals with no topography removed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_no_topo(df_with_topo, zcol=&#39;ELEVATION&#39;, no_data_val_table=&#39;&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to remove wells that do not have topography data (needed for layer selection later).

    This function is intended to be run on the metadata table after elevations have attempted to been added.

    Parameters
    ----------
    df_with_topo : pandas.DataFrame
        Pandas dataframe containing elevation information.
    zcol : str
        Name of elevation column
    no_data_val_table : any
        Value in dataset that indicates no data is present (replaced with np.nan)
    verbose : bool, optional
        Whether to print outputs, by default True
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    pandas.DataFrame
        Pandas dataframe with intervals with no topography removed.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if verbose:
        verbose_print(remove_no_topo, locals(), exclude_params=[&#39;df_with_topo&#39;])

    before = df_with_topo.shape[0]
    
    df_with_topo[zcol].replace(no_data_val_table, np.nan, inplace=True)
    df_with_topo.dropna(subset=[zcol], inplace=True)
    
    if verbose:
        after = df_with_topo.shape[0]
        print(&#39;Removed well records with no surface elevation information. &#39;)
        print(&#34;\tNumber of records before removing: &#34;+str(before))
        print(&#34;\tNumber of records after removing: &#34;+str(after))
        print(&#34;\t\t{} wells records removed without surface elevation information&#34;.format(before-after))
    
    return df_with_topo</code></pre>
</details>
</dd>
<dt id="w4h.remove_nonlocated"><code class="name flex">
<span>def <span class="ident">remove_nonlocated</span></span>(<span>df_with_locations, xcol='LONGITUDE', ycol='LATITUDE', no_data_val_table='', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to remove wells and well intervals where there is no location information</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df_with_locations</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing well descriptions</dd>
<dt><strong><code>metadata_DF</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing metadata, including well locations (e.g., Latitude/Longitude)</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df_with_locations</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing only data with location information</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_nonlocated(df_with_locations, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, no_data_val_table=&#39;&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to remove wells and well intervals where there is no location information

    Parameters
    ----------
    df_with_locations : pandas.DataFrame
        Pandas dataframe containing well descriptions
    metadata_DF : pandas.DataFrame
        Pandas dataframe containing metadata, including well locations (e.g., Latitude/Longitude)
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    df_with_locations : pandas.DataFrame
        Pandas dataframe containing only data with location information
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(remove_nonlocated, locals(), exclude_params=[&#39;df_with_locations&#39;])

    before = df_with_locations.shape[0] #Extract length of data before this process

    df_with_locations[xcol].replace(no_data_val_table, np.nan, inplace=True)
    df_with_locations[ycol].replace(no_data_val_table, np.nan, inplace=True)
    
    df_with_locations.dropna(subset=xcol, inplace=True)
    df_with_locations.dropna(subset=ycol, inplace=True)
    
    if verbose:
        after = df_with_locations.shape[0]
        print(&#39;Removed well records with no location information. &#39;)
        print(&#34;\tNumber of records before removing: &#34;+str(before))
        print(&#34;\tNumber of records after removing: &#34;+str(after))
        print(&#34;\t\t{} wells records removed without location information&#34;.format(before-after))

    return df_with_locations</code></pre>
</details>
</dd>
<dt id="w4h.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>well_data, surf_elev_grid, bedrock_elev_grid, model_grid=None, metadata=None, layers=9, well_data_cols=None, well_metadata_cols=None, description_col='FORMATION', top_col='TOP', bottom_col='BOTTOM', depth_type='depth', study_area=None, xcol='LONGITUDE', ycol='LATITUDE', zcol='ELEVATION', well_id_col='API_NUMBER', output_crs='EPSG:4269', lith_dict=None, lith_dict_start=None, lith_dict_wildcard=None, target_dict=None, target_name='', export_dir=None, verbose=False, log=False, **kw_params)</span>
</code></dt>
<dd>
<div class="desc"><p>w4h.run() is a function that runs the intended workflow of the wells4hydrogeology (w4h) package.
This means that it runs several constituent functions. The workflow that this follows is provided in the package wiki.
It accepts the parameters of the constituent functions. To see a list of these functions and parameters, use <code>help(<a title="w4h.run" href="#w4h.run">run()</a>)</code>.</p>
<pre><code>The following functions used in w4h.run() are listed below, along with their parameters and default values for those parameters. 
See the documentation for the each of the individual functions for more information on a specific parameter:
</code></pre>
<p><strong>file_setup</strong></p>
<pre><code>    well_data             | default = '&lt;no default&gt;'

    metadata              | default = None

    data_filename         | default = '*ISGS_DOWNHOLE_DATA*.txt'

    metadata_filename     | default = '*ISGS_HEADER*.txt'

    log_dir               | default = None

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>read_raw_csv</strong></p>
<pre><code>    data_filepath         | default = '&lt;output of previous function&gt;'

    metadata_filepath     | default = '&lt;output of previous function&gt;'

    data_cols             | default = None

    metadata_cols         | default = None

    xcol                  | default = 'LONGITUDE'

    ycol                  | default = 'LATITUDE'

    well_key              | default = 'API_NUMBER'

    encoding              | default = 'latin-1'

    verbose               | default = False

    log                   | default = False

    read_csv_kwargs       | default = {}
</code></pre>
<p><strong>define_dtypes</strong></p>
<pre><code>    undefined_df          | default = '&lt;output of previous function&gt;'

    datatypes             | default = None

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>merge_metadata</strong></p>
<pre><code>    data_df               | default = '&lt;output of previous function&gt;'

    header_df             | default = '&lt;output of previous function&gt;'

    data_cols             | default = None

    header_cols           | default = None

    auto_pick_cols        | default = False

    drop_duplicate_cols   | default = True

    log                   | default = False

    verbose               | default = False

    kwargs                | default = {}
</code></pre>
<p><strong>coords2geometry</strong></p>
<pre><code>    df_no_geometry        | default = '&lt;output of previous function&gt;'

    xcol                  | default = 'LONGITUDE'

    ycol                  | default = 'LATITUDE'

    zcol                  | default = 'ELEV_FT'

    input_coords_crs      | default = 'EPSG:4269'

    use_z                 | default = False

    wkt_col               | default = 'WKT'

    geometry_source       | default = 'coords'

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>read_study_area</strong></p>
<pre><code>    study_area            | default = None

    output_crs            | default = 'EPSG:4269'

    buffer                | default = None

    return_original       | default = False

    log                   | default = False

    verbose               | default = False

    read_file_kwargs      | default = {}
</code></pre>
<p><strong>clip_gdf2study_area</strong></p>
<pre><code>    study_area            | default = '&lt;output of previous function&gt;'

    gdf                   | default = '&lt;output of previous function&gt;'

    log                   | default = False

    verbose               | default = False
</code></pre>
<p><strong>read_grid</strong></p>
<pre><code>    grid_path             | default = None

    grid_type             | default = 'model'

    no_data_val_grid      | default = 0

    use_service           | default = False

    study_area            | default = None

    grid_crs              | default = None

    output_crs            | default = 'EPSG:4269'

    verbose               | default = False

    log                   | default = False

    kwargs                | default = {}
</code></pre>
<p><strong>add_control_points</strong></p>
<pre><code>    df_without_control    | default = '&lt;output of previous function&gt;'

    df_control            | default = None

    xcol                  | default = 'LONGITUDE'

    ycol                  | default = 'LATITUDE'

    zcol                  | default = 'ELEV_FT'

    controlpoints_crs     | default = 'EPSG:4269'

    output_crs            | default = 'EPSG:4269'

    description_col       | default = 'FORMATION'

    interp_col            | default = 'INTERPRETATION'

    target_col            | default = 'TARGET'

    verbose               | default = False

    log                   | default = False

    kwargs                | default = {}
</code></pre>
<p><strong>remove_nonlocated</strong></p>
<pre><code>    df_with_locations     | default = '&lt;output of previous function&gt;'

    xcol                  | default = 'LONGITUDE'

    ycol                  | default = 'LATITUDE'

    no_data_val_table     | default = ''

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>remove_no_topo</strong></p>
<pre><code>    df_with_topo          | default = '&lt;output of previous function&gt;'

    zcol                  | default = 'ELEVATION'

    no_data_val_table     | default = ''

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>remove_no_depth</strong></p>
<pre><code>    df_with_depth         | default = '&lt;output of previous function&gt;'

    top_col               | default = 'TOP'

    bottom_col            | default = 'BOTTOM'

    no_data_val_table     | default = ''

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>remove_bad_depth</strong></p>
<pre><code>    df_with_depth         | default = '&lt;output of previous function&gt;'

    top_col               | default = 'TOP'

    bottom_col            | default = 'BOTTOM'

    depth_type            | default = 'depth'

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>remove_no_description</strong></p>
<pre><code>    df_with_descriptions  | default = '&lt;output of previous function&gt;'

    description_col       | default = 'FORMATION'

    no_data_val_table     | default = ''

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>get_search_terms</strong></p>
<pre><code>    spec_path             | default = 'C:\Users\riley\LocalData\Github\wells4hydrogeology/resources/'

    spec_glob_pattern     | default = '*SearchTerms-Specific*'

    start_path            | default = None

    start_glob_pattern    | default = '*SearchTerms-Start*'

    wildcard_path         | default = None

    wildcard_glob_pattern | default = '*SearchTerms-Wildcard'

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>read_dictionary_terms</strong></p>
<pre><code>    dict_file             | default = None

    id_col                | default = 'ID'

    search_col            | default = 'DESCRIPTION'

    definition_col        | default = 'LITHOLOGY'

    class_flag_col        | default = 'CLASS_FLAG'

    dictionary_type       | default = None

    class_flag            | default = 6

    rem_extra_cols        | default = True

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>specific_define</strong></p>
<pre><code>    df                    | default = '&lt;output of previous function&gt;'

    terms_df              | default = '&lt;output of previous function&gt;'

    description_col       | default = 'FORMATION'

    terms_col             | default = 'DESCRIPTION'

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>split_defined</strong></p>
<pre><code>    df                    | default = '&lt;output of previous function&gt;'

    classification_col    | default = 'CLASS_FLAG'

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>start_define</strong></p>
<pre><code>    df                    | default = '&lt;output of previous function&gt;'

    terms_df              | default = '&lt;output of previous function&gt;'

    description_col       | default = 'FORMATION'

    terms_col             | default = 'DESCRIPTION'

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>wildcard_define</strong></p>
<pre><code>    df                    | default = '&lt;output of previous function&gt;'

    terms_df              | default = '&lt;output of previous function&gt;'

    description_col       | default = 'FORMATION'

    terms_col             | default = 'DESCRIPTION'

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>remerge_data</strong></p>
<pre><code>    classifieddf          | default = '&lt;output of previous function&gt;'

    searchdf              | default = '&lt;output of previous function&gt;'
</code></pre>
<p><strong>fill_unclassified</strong></p>
<pre><code>    df                    | default = '&lt;output of previous function&gt;'

    classification_col    | default = 'CLASS_FLAG'
</code></pre>
<p><strong>read_lithologies</strong></p>
<pre><code>    lith_file             | default = None

    interp_col            | default = 'LITHOLOGY'

    target_col            | default = 'CODE'

    use_cols              | default = None

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>merge_lithologies</strong></p>
<pre><code>    well_data_df          | default = '&lt;output of previous function&gt;'

    targinterps_df        | default = '&lt;output of previous function&gt;'

    interp_col            | default = 'INTERPRETATION'

    target_col            | default = 'TARGET'

    target_class          | default = 'bool'
</code></pre>
<p><strong>align_rasters</strong></p>
<pre><code>    grids_unaligned       | default = None

    model_grid            | default = None

    no_data_val_grid      | default = 0

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>get_drift_thick</strong></p>
<pre><code>    surface_elev          | default = None

    bedrock_elev          | default = None

    layers                | default = 9

    plot                  | default = False

    verbose               | default = False

    log                   | default = False
</code></pre>
<p><strong>sample_raster_points</strong></p>
<pre><code>    raster                | default = None

    points_df             | default = None

    well_id_col           | default = 'API_NUMBER'

    xcol                  | default = 'LONGITUDE'

    ycol                  | default = 'LATITUDE'

    new_col               | default = 'SAMPLED'

    verbose               | default = True

    log                   | default = False
</code></pre>
<p><strong>get_layer_depths</strong></p>
<pre><code>    df_with_depths        | default = '&lt;output of previous function&gt;'

    surface_elev_col      | default = 'SURFACE_ELEV'

    layer_thick_col       | default = 'LAYER_THICK'

    layers                | default = 9

    log                   | default = False
</code></pre>
<p><strong>layer_target_thick</strong></p>
<pre><code>    df                    | default = '&lt;output of previous function&gt;'

    layers                | default = 9

    return_all            | default = False

    export_dir            | default = None

    outfile_prefix        | default = None

    depth_top_col         | default = 'TOP'

    depth_bot_col         | default = 'BOTTOM'

    log                   | default = False
</code></pre>
<p><strong>layer_interp</strong></p>
<pre><code>    points                | default = '&lt;no default&gt;'

    grid                  | default = '&lt;no default&gt;'

    layers                | default = None

    interp_kind           | default = 'nearest'

    return_type           | default = 'dataarray'

    export_dir            | default = None

    target_col            | default = 'TARG_THICK_PER'

    layer_col             | default = 'LAYER'

    xcol                  | default = None

    ycol                  | default = None

    xcoord                | default = 'x'

    ycoord                | default = 'y'

    log                   | default = False

    verbose               | default = False

    kwargs                | default = {}
</code></pre>
<p><strong>export_grids</strong></p>
<pre><code>    grid_data             | default = '&lt;no default&gt;'

    out_path              | default = '&lt;no default&gt;'

    file_id               | default = ''

    filetype              | default = 'tif'

    variable_sep          | default = True

    date_stamp            | default = True

    verbose               | default = False

    log                   | default = False"
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(well_data,
        surf_elev_grid,
        bedrock_elev_grid,
        model_grid=None,
        metadata=None,
        layers = 9,
        well_data_cols=None, well_metadata_cols=None, description_col=&#39;FORMATION&#39;, top_col=&#39;TOP&#39;, bottom_col=&#39;BOTTOM&#39;, depth_type=&#39;depth&#39;,
        study_area=None, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, zcol=&#39;ELEVATION&#39;, well_id_col=&#39;API_NUMBER&#39;, output_crs=&#39;EPSG:4269&#39;,
        lith_dict=None, lith_dict_start=None, lith_dict_wildcard=None,
        target_dict=None,
        target_name=&#39;&#39;,
        export_dir=None,
        verbose=False,
        log=False,
        **kw_params):
    
    &#34;&#34;&#34;Function to run entire process with one line of code. 
    
    NOTE: verbose and log are boolean parameters used for most of the functions. verbose=True prints information to terminal, log=True logs to a file in the log_dir, which defaults to the export_dir

    Parameters
    ----------
    well_data : str or pathlib.Path obj
        Filepath to file or directory containing well data.
    surf_elev_grid : str or pathlib.Path object
        _description_
    bedrock_elev_grid : str or pathlib.Path object
        _description_
    model_grid : str or pathlib.Path object, or model grid parameters (see model_grid function)
        _description_        
    metadata : str or pathlib.Path object, or None, default=None
        Filepath to file or directory containing well metadata, such as location and elevation. If None, will check if well_data is a directory, and if so, will use metadata_filename to search in same directory.
    well_data_cols : List or list-like
        Columns to 
    well_metadata_cols : List or list-like
        _description_
    layers : int, default = 9
        The number of layers in the model grid
    description_col : str, default = &#39;FORMATION&#39;
        Name of column containing geologic descriptions of the well interval. This column should be in well_data.
    top_col : str, default = &#39;TOP&#39;
        Name of column containing depth/elevation at top of well interval. This column should be in well_data.
    bottom_col : str, default = &#39;BOTTOM&#39;
        Name of column containing depth/elevation at bottom of well interval. This column should be in well_data.    
    depth_type : str, default = &#39;depth&#39;
        Whether values top_col or bottom_col refer to depth or elevation.
    study_area : str or pathlib.Path object, or geopandas.GeoDataFrame
        _description_
    xcol : str, default = &#39;LONGITUDE&#39; 
        Name of column containing x coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.
    ycol : str, default = &#39;LATITUDE&#39;
        Name of column containing y coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.
    zcol : str, default = &#39;ELEVATION&#39; 
        Name of column containing z coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.
    output_crs : crs definition accepted by pyproj, default = &#39;EPSG:4269&#39;
        CRS to output all of the data into
    lith_dict : str or pathlib.Path object, or pandas.DataFrame
        _description_
    lith_dict_start : str or pathlib.Path object, or pandas.DataFrame
        _description_
    lith_dict_wildcard : str or pathlib.Path object, or pandas.DataFrame
        _description_
    target_dict : str or pathlib.Path object, or pandas.DataFrame
        _description_
    target_name : str, default = &#39;CoarseFine&#39;
        Name of target of interest, to be used on exported files
    export_dir : str or pathlib.Path object, default = None
        Directory to export output files
    verbose : bool, default = False
        Whether to print updates/results
    log : bool, default = False
        Whether to send parameters and outputs to log file, to be saved in export_dir, or the same directory as well_data if export_dir not defined.
    **kw_params
        Keyword parameters used by any of the functions throughout the process. See list of functions above, and the API documentation for their possible parameters
    &#34;&#34;&#34;

    if verbose:
        verbose_print(run, locals())

    #Get data (files or otherwise)
    file_setup_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.file_setup).parameters.keys()}
    
    #Check how well_data and metadata were defined
    if isinstance(well_data, pathlib.PurePath) or isinstance(well_data, str):
        #Convert well_data to pathlib.Path if not already
        if isinstance(well_data, str):
            well_data = pathlib.Path(well_data)

        if metadata is None:
            if well_data.is_dir():
                #If the two files are supposed to be in the same directory (or just want well_data found)
                well_dataPath, metadataPath = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             
            elif well_data.exists():
                #If well_data is a file, and metadata is not used
                well_dataPath, _ = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             
                metadataPath = None
            else:
                #Need for well_data to exist at the very least
                raise IOError(&#39;well_data file does not exist:{}&#39;.format(well_data))
        elif isinstance(metadata, pathlib.PurePath) or isinstance(metadata, str):
            #Metdata has specifically been specified by a filepath
            if isinstance(metadata, str):
                metadata = pathlib.Path(metadata)    
            well_dataPath, metadataPath = w4h.file_setup(well_data=well_data, metadata=metadata, **file_setup_kwargs)                
        else:
            if isinstance(metadata, pd.DataFrame):
                well_dataPath, _ = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             
                metadataPath = metadata
            elif metadata is None:
                well_dataPath, _ = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             

    elif isinstance(well_data, pd.DataFrame):
        if isinstance(metadata, pd.DataFrame):
            well_dataPath = well_data
            metadataPath = metadata
        elif isinstance(metadata, pathlib.PurePath) or isinstance(metadata, str):
            _, metadataPath = w4h.file_setup(well_data=metadata, metadata=metadata, verbose=verbose, log=log, **file_setup_kwargs)                
            well_dataPath = well_data
        else:
            print(&#39;ERROR: metadata must be a string filepath, a pathlib.Path object, or pandas.DataFrame&#39;)
    else:
        print(&#39;ERROR: well_data must be a string filepath, a pathlib.Path object, or pandas.DataFrame&#39;)

    if not export_dir:
        if export_dir is False:
            pass
        else:
            nowTime = datetime.datetime.now()
            nowTime = str(nowTime).replace(&#39;:&#39;, &#39;-&#39;).replace(&#39; &#39;,&#39;_&#39;).split(&#39;.&#39;)[0]
            nowTimeStr = &#39;_&#39;+str(nowTime)
            outDir = &#39;Output_&#39;+nowTimeStr
            if isinstance(well_dataPath, pd.DataFrame) or isinstance(well_dataPath, gpd.GeoDataFrame):
                export_dir = pathlib.Path(outDir)
            elif isinstance(well_dataPath, pathlib.PurePath):
                if well_dataPath.is_dir():
                    export_dir = well_dataPath.joinpath(outDir)
                else:
                    export_dir = well_dataPath.parent.joinpath(outDir)
            else:
                raise IOError(&#39;export_dir should be explicitly defined if well_data is not a filepath&#39;)

            if not export_dir.exists():
                try:
                    export_dir.mkdir()
                except Exception:
                    print(&#39;Export Directory not created&#39;)

    #Get pandas dataframes from input
    read_raw_txt_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.read_raw_csv).parameters.keys()}
    well_data_IN, metadata_IN = w4h.read_raw_csv(data_filepath=well_dataPath, metadata_filepath=metadataPath, verbose=verbose, log=log, **read_raw_txt_kwargs)
    #Functions to read data into dataframes. Also excludes extraneous columns, and drops header data with no location information

    #Define data types (file will need to be udpated)
    well_data_DF = w4h.define_dtypes(undefined_df=well_data_IN, datatypes=&#39;./resources/downholeDataTypes.txt&#39;, verbose=verbose, log=log)
    metadata_DF = w4h.define_dtypes(undefined_df=metadata_IN, datatypes=&#39;./resources/headerDataTypes.txt&#39;, verbose=verbose, log=log)

    if metadata_DF is None:
        well_data_xyz = well_data_DF
    else:
        merge_metadata_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.merge_metadata).parameters.keys()}
        well_data_xyz = w4h.merge_metadata(data_df=well_data_DF, header_df=metadata_DF, data_cols=None, header_cols=None, auto_pick_cols=False, drop_duplicate_cols=True, log=False, **merge_metadata_kwargs)

    #Convert well_data_xyz to have geometry
    coords2geometry_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.coords2geometry).parameters.keys()}
    well_data_xyz = w4h.coords2geometry(df_no_geometry=well_data_xyz, xcol=xcol, ycol=ycol, zcol=zcol, verbose=verbose, log=log, **coords2geometry_kwargs)

    #Get Study area
    read_study_area_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.read_study_area).parameters.keys()}
    if study_area is None:
        studyAreaIN = None
        use_study_area = False
    else:
        studyAreaIN = w4h.read_study_area(study_area_path=study_area, log=log, output_crs=output_crs, **read_study_area_kwargs)
        use_study_area = True

    clip_gdf2study_area_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.clip_gdf2study_area).parameters.keys()}
    well_data_xyz = w4h.clip_gdf2study_area(study_area=studyAreaIN, gdf=well_data_xyz,  verbose=verbose, log=log,**clip_gdf2study_area_kwargs)
    #Get surfaces and grid(s)
    read_grid_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.read_grid).parameters.keys()}

    modelGridPath = model_grid
    surfaceElevPath = surf_elev_grid
    bedrockElevPath = bedrock_elev_grid

    modelGrid = w4h.read_grid(grid_path=modelGridPath, grid_type=&#39;model&#39;, study_area=studyAreaIN, verbose=verbose, log=log, **read_grid_kwargs)
    surfaceElevGridIN = w4h.read_grid(grid_path=surfaceElevPath, grid_type=&#39;surface&#39;, study_area=studyAreaIN, verbose=verbose, log=log, **read_grid_kwargs)
    bedrockElevGridIN = w4h.read_grid(grid_path=bedrockElevPath, grid_type=&#39;bedrock&#39;, study_area=studyAreaIN, verbose=verbose, log=log, **read_grid_kwargs)

    #UPDATE: MAKE SURE CRS&#39;s all align ***
    #Add control points
    add_control_points_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.add_control_points).parameters.keys()}
    well_data_xyz = w4h.add_control_points(df_without_control=well_data_xyz, xcol=xcol, ycol=ycol, zcol=zcol, top_col=top_col, bottom_col=bottom_col, description_col=description_col, verbose=verbose, log=log, **add_control_points_kwargs)

    #Clean up data
    well_data_xyz = w4h.remove_nonlocated(df_with_locations=well_data_xyz, log=log, verbose=verbose)
    well_data_xyz = w4h.remove_no_topo(df_with_topo=well_data_xyz, zcol=zcol, verbose=verbose, log=log)

    remove_no_depth_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.remove_no_depth).parameters.keys()}
    well_data_xyz = w4h.remove_no_depth(well_data_xyz, verbose=verbose, top_col=top_col, bottom_col=bottom_col, log=log, **remove_no_depth_kwargs) #Drop records with no depth information

    remove_bad_depth_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.remove_bad_depth).parameters.keys()}
    well_data_xyz = w4h.remove_bad_depth(well_data_xyz, verbose=verbose, top_col=top_col, bottom_col=bottom_col, depth_type=depth_type, log=log, **remove_bad_depth_kwargs)#Drop records with bad depth information (i.e., top depth &gt; bottom depth) (Also calculates thickness of each record)

    remove_no_formation_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.remove_no_description).parameters.keys()}
    well_data_xyz = w4h.remove_no_description(well_data_xyz, description_col=description_col, verbose=verbose, log=log, **remove_no_formation_kwargs)

    #CLASSIFICATION
    #Read dictionary definitions and classify
    get_search_terms_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.get_search_terms).parameters.keys()}
    specTermsPATH, startTermsPATH, wildcardTermsPATH, = w4h.get_search_terms(spec_path=lith_dict, start_path=lith_dict_start, wildcard_path=lith_dict_wildcard, log=log, **get_search_terms_kwargs)
    read_dictionary_terms_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.read_dictionary_terms).parameters.keys()}
    if &#39;class_flag&#39; in read_dictionary_terms_kwargs.keys():
        del read_dictionary_terms_kwargs[&#39;class_flag&#39;] #This is specific to an invidiual dict terms file, so don&#39;t want to use for all
    specTerms = w4h.read_dictionary_terms(dict_file=specTermsPATH, log=log, **read_dictionary_terms_kwargs)
    startTerms = w4h.read_dictionary_terms(dict_file=startTermsPATH, log=log, **read_dictionary_terms_kwargs)
    wildcardTerms = w4h.read_dictionary_terms(dict_file=wildcardTermsPATH, log=log, **read_dictionary_terms_kwargs)

    #Clean up dictionary terms
    specTerms.drop_duplicates(subset=&#39;DESCRIPTION&#39;, inplace=True)
    specTerms.reset_index(inplace=True, drop=True)

    startTerms.drop_duplicates(subset=&#39;DESCRIPTION&#39;, inplace=True)
    startTerms.reset_index(inplace=True, drop=True)

    wildcardTerms.drop_duplicates(subset=&#39;DESCRIPTION&#39;, inplace=True)
    wildcardTerms.reset_index(inplace=True, drop=True)

    if verbose:
        print(&#39;Search terms to be used:&#39;)
        print(&#39;\t {} exact match term/definition pairs&#39;)
        print(&#39;\t {} starting match term/definition pairs&#39;)
        print(&#39;\t {} wildcard match term/definition pairs&#39;)

    #CLASSIFICATIONS
    #Exact match classifications
    well_data_xyz = w4h.specific_define(well_data_xyz, terms_df=specTerms, description_col=description_col, verbose=verbose, log=log)
    
    #.startswith classifications
    if lith_dict_start is not None:
        classifedDF, searchDF = w4h.split_defined(well_data_xyz, verbose=verbose, log=log)
        searchDF = w4h.start_define(df=searchDF, terms_df=startTerms, description_col=description_col, verbose=verbose, log=log)
        well_data_xyz = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF) #UPDATE: Needed? ***    

    #wildcard/any substring match classifications
    if lith_dict_wildcard is not None:
        classifedDF, searchDF = w4h.split_defined(well_data_xyz, verbose=verbose, log=log)
        searchDF = w4h.wildcard_define(df=searchDF, terms_df=wildcardTerms, description_col=description_col, verbose=verbose, log=log)
        well_data_xyz = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF) #UPDATE: Needed? ***    

    #Depth classification
    classifedDF, searchDF = w4h.split_defined(well_data_xyz, verbose=verbose, log=log)
    searchDF = w4h.depth_define(df=searchDF, thresh=550, verbose=verbose, log=log)
    well_data_xyz = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF) #UPDATE: Needed? ***
    
    #Fill unclassified data
    well_data_xyz = w4h.fill_unclassified(well_data_xyz, classification_col=&#39;CLASS_FLAG&#39;)

    #Add target interpratations
    read_lithologies_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.read_lithologies).parameters.keys()}
    targetInterpDF = w4h.read_lithologies(lith_file=target_dict, log=log, **read_lithologies_kwargs)
    well_data_xyz = w4h.merge_lithologies(well_data_df=well_data_xyz, targinterps_df=targetInterpDF, target_col=&#39;TARGET&#39;, target_class=&#39;bool&#39;)

    #Sort dataframe to prepare for next steps
    #well_data_xyz = w4h.sort_dataframe(df=well_data_xyz, sort_cols=[&#39;API_NUMBER&#39;,&#39;TOP&#39;], remove_nans=True)
    well_data_xyz = well_data_xyz.sort_values(by=[well_id_col, top_col])
    well_data_xyz.reset_index(inplace=True, drop=True)
    #UPDATE: Option to remove nans?
    well_data_xyz = well_data_xyz[pd.notna(well_data_xyz[&#34;LITHOLOGY&#34;])]

    #Analyze Surface(s) and grid(s)
    bedrockGrid, surfaceGrid = w4h.align_rasters(grids_unaligned=[bedrockElevGridIN, surfaceElevGridIN], model_grid=modelGrid, no_data_val_grid=0, log=log)
    driftThickGrid, layerThickGrid = w4h.get_drift_thick(surface_elev=surfaceGrid, bedrock_elev=bedrockGrid, layers=layers, plot=verbose, log=log)
    
    well_data_xyz = w4h.sample_raster_points(raster=bedrockGrid, points_df=well_data_xyz, xcol=xcol, ycol=ycol, new_col=&#39;BEDROCK_ELEV&#39;, verbose=verbose, log=log)
    well_data_xyz = w4h.sample_raster_points(raster=surfaceGrid, points_df=well_data_xyz, xcol=xcol, ycol=ycol, new_col=&#39;SURFACE_ELEV&#39;, verbose=verbose, log=log)
    well_data_xyz[&#39;BEDROCK_DEPTH&#39;] = well_data_xyz[&#39;SURFACE_ELEV&#39;] - well_data_xyz[&#39;BEDROCK_ELEV&#39;]
    well_data_xyz[&#39;LAYER_THICK&#39;] = well_data_xyz[&#39;BEDROCK_DEPTH&#39;] / layers
    
    well_data_xyz = w4h.get_layer_depths(df_with_depths=well_data_xyz, layers=layers, log=log)

    layer_target_thick_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.layer_target_thick).parameters.keys()}
    if &#39;return_all&#39; in layer_target_thick_kwargs.keys():
        del layer_target_thick_kwargs[&#39;return_all&#39;] #This needs to be set to False, so we don&#39;t want it reading in twice

    resdf = w4h.layer_target_thick(df=well_data_xyz, layers=layers, return_all=False, export_dir=export_dir, depth_top_col=top_col, depth_bot_col=bottom_col, log=log, **layer_target_thick_kwargs)
    
    layer_interp_kwargs = {k: v for k, v in locals()[&#39;kw_params&#39;].items() if k in inspect.signature(w4h.layer_interp).parameters.keys()}
    layers_data = w4h.layer_interp(points=resdf, grid=modelGrid, layers=9, verbose=verbose, log=log, **layer_interp_kwargs)

    nowTime = datetime.datetime.now()
    nowTime = str(nowTime).replace(&#39;:&#39;, &#39;-&#39;).replace(&#39; &#39;,&#39;_&#39;).split(&#39;.&#39;)[0]
    nowTimeStr = &#39;_&#39;+str(nowTime)

    #THIS MAY BE REPEAT OF LAST LINES OF layer_interp()
    w4h.export_grids(grid_data=layers_data, out_path=export_dir, file_id=target_name,filetype=&#39;tif&#39;, variable_sep=True, date_stamp=True, verbose=verbose, log=log)

    return resdf, layers_data</code></pre>
</details>
</dd>
<dt id="w4h.sample_raster_points"><code class="name flex">
<span>def <span class="ident">sample_raster_points</span></span>(<span>raster=None, points_df=None, well_id_col='API_NUMBER', xcol='LONGITUDE', ycol='LATITUDE', new_col='SAMPLED', verbose=True, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample raster values to points from geopandas geodataframe.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>raster</code></strong> :&ensp;<code>rioxarray data array</code></dt>
<dd>Raster containing values to be sampled.</dd>
<dt><strong><code>points_df</code></strong> :&ensp;<code>geopandas.geodataframe</code></dt>
<dd>Geopandas dataframe with geometry column containing point values to sample.</dd>
<dt><strong><code>well_id_col</code></strong> :&ensp;<code>str</code>, default=<code>"API_NUMBER"</code></dt>
<dd>Column that uniquely identifies each well so multiple sampling points are not taken per well</dd>
<dt><strong><code>xcol</code></strong> :&ensp;<code>str</code>, default=<code>'LONGITUDE'</code></dt>
<dd>Column containing name for x-column, by default 'LONGITUDE.'
This is used to output (potentially) reprojected point coordinates so as not to overwrite the original.</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default=<code>'LATITUDE'</code></dt>
<dd>Column containing name for y-column, by default 'LATITUDE.'
This is used to output (potentially) reprojected point coordinates so as not to overwrite the original.
new_col : str, optional</dd>
<dt><strong><code>new_col</code></strong> :&ensp;<code>str</code>, default=<code>'SAMPLED'</code></dt>
<dd>Name for name of new column containing points sampled from the raster, by default 'SAMPLED'.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to send to print() information about progress of function, by default True.</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>points_df</code></strong> :&ensp;<code>geopandas.geodataframe</code></dt>
<dd>Same as points_df, but with sampled values and potentially with reprojected coordinates.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_raster_points(raster=None, points_df=None, well_id_col=&#39;API_NUMBER&#39;, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, new_col=&#39;SAMPLED&#39;, verbose=True, log=False):  
    &#34;&#34;&#34;Sample raster values to points from geopandas geodataframe.

    Parameters
    ----------
    raster : rioxarray data array
        Raster containing values to be sampled.
    points_df : geopandas.geodataframe
        Geopandas dataframe with geometry column containing point values to sample.
    well_id_col : str, default=&#34;API_NUMBER&#34;
        Column that uniquely identifies each well so multiple sampling points are not taken per well
    xcol : str, default=&#39;LONGITUDE&#39;
        Column containing name for x-column, by default &#39;LONGITUDE.&#39;
        This is used to output (potentially) reprojected point coordinates so as not to overwrite the original.
    ycol : str, default=&#39;LATITUDE&#39;
        Column containing name for y-column, by default &#39;LATITUDE.&#39;
        This is used to output (potentially) reprojected point coordinates so as not to overwrite the original.    new_col : str, optional
    new_col : str, default=&#39;SAMPLED&#39;
        Name for name of new column containing points sampled from the raster, by default &#39;SAMPLED&#39;.
    verbose : bool, default=True
        Whether to send to print() information about progress of function, by default True.
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    points_df : geopandas.geodataframe
        Same as points_df, but with sampled values and potentially with reprojected coordinates.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if raster is None:
        raster = w4h.get_resources()[&#39;surf_elev&#39;]
    if points_df is None:
        points_df = w4h.get_resources()[&#39;well_data&#39;]

    if verbose:
        verbose_print(sample_raster_points, locals(), exclude_params=[&#39;raster&#39;, &#39;points_df&#39;])
        print(f&#34;\tSampling {new_col} grid for at all well locations.&#34;)

    #Project points to raster CRS
    rastercrsWKT=raster.spatial_ref.crs_wkt
    if rastercrsWKT != points_df.crs:
        if verbose:
            print(&#34;\tTemporarily reprojecting raster data to point data&#39;s CRS.&#34;)
        pointCRS = points_df.crs.to_epsg()
        if pointCRS is None:
            pointCRS = points_df.crs.to_wkt()

        raster = raster.rio.reproject(pointCRS)
        #points_df = points_df.to_crs(rastercrsWKT)

    xCOLOUT = xcol+&#39;_PROJ&#39;
    yCOLOUT = ycol+&#39;_PROJ&#39;
    points_df[xCOLOUT] = points_df[&#39;geometry&#39;].x
    points_df[yCOLOUT] = points_df[&#39;geometry&#39;].y
    xData = np.array(points_df[xCOLOUT].values)
    yData = np.array(points_df[yCOLOUT].values)
    zData = []
    zID = []
    zInd = []

    # Get unique well values to reduce sampling time
    uniqueWells = points_df.drop_duplicates(subset=[well_id_col])
    if verbose:
        print(f&#34;\t{uniqueWells.shape[0]} unique wells idenfied using {well_id_col} column&#34;)
    
    # Loop over DataFrame rows
    for i, row in uniqueWells.iterrows():
        # Select data from DataArray at current coordinates and append to list
        zInd.append(i)
        zData.append([row[well_id_col], raster.sel(x=row[xCOLOUT], y=row[yCOLOUT], method=&#39;nearest&#39;).item()])
    
    inputtype = points_df.dtypes[well_id_col]
    wellZDF = pd.DataFrame(zData, columns=[well_id_col, new_col], index=pd.Index(zInd))

    # Merge each unique well&#39;s data with all well intervals
    wellZDF[well_id_col].astype(inputtype, copy=False)
    points_df = points_df.merge(wellZDF, how=&#39;left&#39;, on=well_id_col)
    #points_df[new_col] = zData#sampleDF[new_col]
    return points_df</code></pre>
</details>
</dd>
<dt id="w4h.sort_dataframe"><code class="name flex">
<span>def <span class="ident">sort_dataframe</span></span>(<span>df, sort_cols=['API_NUMBER', 'TOP'], remove_nans=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to sort dataframe by one or more columns.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe to be sorted</dd>
<dt><strong><code>sort_cols</code></strong> :&ensp;<code>str</code> or <code>list</code> of <code>str</code>, default <code>= ['API_NUMBER','TOP']</code></dt>
<dd>Name(s) of columns by which to sort dataframe, by default ['API_NUMBER','TOP']</dd>
<dt><strong><code>remove_nans</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether or not to remove nans in the process, by default True</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df_sorted</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Sorted dataframe</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sort_dataframe(df, sort_cols=[&#39;API_NUMBER&#39;,&#39;TOP&#39;], remove_nans=True):
    &#34;&#34;&#34;Function to sort dataframe by one or more columns.

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe to be sorted
    sort_cols : str or list of str, default = [&#39;API_NUMBER&#39;,&#39;TOP&#39;]
        Name(s) of columns by which to sort dataframe, by default [&#39;API_NUMBER&#39;,&#39;TOP&#39;]
    remove_nans : bool, default = True
        Whether or not to remove nans in the process, by default True

    Returns
    -------
    df_sorted : pandas.DataFrame
        Sorted dataframe
    &#34;&#34;&#34;
    #Sort columns for better processing later
    df_sorted = df.sort_values(sort_cols)
    df_sorted.reset_index(inplace=True, drop=True)
    if remove_nans:
        df_sorted = df_sorted[pd.notna(df_sorted[&#34;LITHOLOGY&#34;])]
    return df_sorted</code></pre>
</details>
</dd>
<dt id="w4h.specific_define"><code class="name flex">
<span>def <span class="ident">specific_define</span></span>(<span>df, terms_df, description_col='FORMATION', terms_col='DESCRIPTION', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to classify terms that have been specifically defined in the terms_df.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Input dataframe with unclassified well descriptions.</dd>
<dt><strong><code>terms_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing the classifications</dd>
<dt><strong><code>description_col</code></strong> :&ensp;<code>str</code>, default=<code>'FORMATION'</code></dt>
<dd>Column name in df containing the well descriptions, by default 'FORMATION'.</dd>
<dt><strong><code>terms_col</code></strong> :&ensp;<code>str</code>, default=<code>'DESCRIPTION'</code></dt>
<dd>Column name in terms_df containing the classified descriptions, by default 'DESCRIPTION'.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to print up results, by default False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df_Interps</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing the well descriptions and their matched classifications.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def specific_define(df, terms_df, description_col=&#39;FORMATION&#39;, terms_col=&#39;DESCRIPTION&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to classify terms that have been specifically defined in the terms_df.

    Parameters
    ----------
    df : pandas.DataFrame
        Input dataframe with unclassified well descriptions.
    terms_df : pandas.DataFrame
        Dataframe containing the classifications
    description_col : str, default=&#39;FORMATION&#39;
        Column name in df containing the well descriptions, by default &#39;FORMATION&#39;.
    terms_col : str, default=&#39;DESCRIPTION&#39;
        Column name in terms_df containing the classified descriptions, by default &#39;DESCRIPTION&#39;.
    verbose : bool, default=False
        Whether to print up results, by default False.

    Returns
    -------
    df_Interps : pandas.DataFrame
        Dataframe containing the well descriptions and their matched classifications.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(specific_define, locals(), exclude_params=[&#39;df&#39;, &#39;terms_df&#39;])

    if description_col != terms_col:
        terms_df.rename(columns={terms_col:description_col}, inplace=True)
        terms_col = description_col

    df[description_col] = df[description_col].astype(str)
    terms_df[terms_col] = terms_df[terms_col].astype(str)

    df[description_col] = df[description_col].str.casefold()
    terms_df[terms_col] = terms_df[terms_col].str.casefold()
    #df[&#39;FORMATION&#39;] = df[&#39;FORMATION&#39;].str.strip([&#39;.,:?\t\s&#39;])
    #terms_df[&#39;FORMATION&#39;] = terms_df[&#39;FORMATION&#39;].str.strip([&#39;.,:?\t\s&#39;])

    terms_df.drop_duplicates(subset=terms_col, keep=&#39;last&#39;, inplace=True)
    terms_df.reset_index(drop=True, inplace=True)
    
    df_Interps = pd.merge(left=df, right=terms_df.set_index(terms_col), on=description_col, how=&#39;left&#39;)
    df_Interps.rename(columns={description_col:&#39;FORMATION&#39;}, inplace=True)
    df_Interps[&#39;BEDROCK_FLAG&#39;] = df_Interps[&#39;LITHOLOGY&#39;] == &#39;BEDROCK&#39;
    
    if verbose:
        print(&#39;Classified well records using exact matches&#39;)
        numRecsClass = int(df_Interps[df_Interps[&#39;CLASS_FLAG&#39;]==1][&#39;CLASS_FLAG&#39;].sum())
        recsRemainig = int(df_Interps.shape[0]-numRecsClass)
        percRecsClass =round((df_Interps[df_Interps[&#39;CLASS_FLAG&#39;]==1][&#39;CLASS_FLAG&#39;].sum()/df_Interps.shape[0])*100,2)
        print(&#34;\t{} records classified using exact matches ({}% of unclassified data)&#34;.format(numRecsClass, percRecsClass))
        print(&#39;\t{} records remain unclassified ({}% of unclassified data).&#39;.format(recsRemainig, 1-percRecsClass))

    return df_Interps</code></pre>
</details>
</dd>
<dt id="w4h.split_defined"><code class="name flex">
<span>def <span class="ident">split_defined</span></span>(<span>df, classification_col='CLASS_FLAG', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to split dataframe with well descriptions into two dataframes based on whether a row has been classified.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all the well descriptions</dd>
<dt><strong><code>classification_col</code></strong> :&ensp;<code>str</code>, default <code>= 'CLASS_FLAG'</code></dt>
<dd>Name of column containing the classification flag, by default 'CLASS_FLAG'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print results, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Two-item tuple</code> of <code>pandas.Dataframe</code></dt>
<dd>tuple[0] is dataframe containing classified data, tuple[1] is dataframe containing unclassified data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split_defined(df, classification_col=&#39;CLASS_FLAG&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to split dataframe with well descriptions into two dataframes based on whether a row has been classified.

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe containing all the well descriptions
    classification_col : str, default = &#39;CLASS_FLAG&#39;
        Name of column containing the classification flag, by default &#39;CLASS_FLAG&#39;
    verbose : bool, default = False
        Whether to print results, by default False
    log : bool, default = False
        Whether to log results to log file

    Returns
    -------
    Two-item tuple of pandas.Dataframe
        tuple[0] is dataframe containing classified data, tuple[1] is dataframe containing unclassified data.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    classifedDF= df[df[classification_col].notna()] #Already-classifed data
    searchDF = df[df[classification_col].isna()] #Unclassified data
        
    return classifedDF, searchDF</code></pre>
</details>
</dd>
<dt id="w4h.start_define"><code class="name flex">
<span>def <span class="ident">start_define</span></span>(<span>df, terms_df, description_col='FORMATION', terms_col='DESCRIPTION', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to classify descriptions according to starting substring. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all the well descriptions</dd>
<dt><strong><code>terms_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all the startswith substrings to use for searching</dd>
<dt><strong><code>description_col</code></strong> :&ensp;<code>str</code>, default <code>= 'FORMATION'</code></dt>
<dd>Name of column in df containing descriptions, by default 'FORMATION'</dd>
<dt><strong><code>terms_col</code></strong> :&ensp;<code>str</code>, default <code>= 'FORMATION'</code></dt>
<dd>Name of column in terms_df containing startswith substring to match with description_col, by default 'FORMATION'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print out results, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log results to log file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing the original data and new classifications</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_define(df, terms_df, description_col=&#39;FORMATION&#39;, terms_col=&#39;DESCRIPTION&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to classify descriptions according to starting substring. 

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe containing all the well descriptions
    terms_df : pandas.DataFrame
        Dataframe containing all the startswith substrings to use for searching
    description_col : str, default = &#39;FORMATION&#39;
        Name of column in df containing descriptions, by default &#39;FORMATION&#39;
    terms_col : str, default = &#39;FORMATION&#39;
        Name of column in terms_df containing startswith substring to match with description_col, by default &#39;FORMATION&#39;
    verbose : bool, default = False
        Whether to print out results, by default False
    log : bool, default = True
        Whether to log results to log file

    Returns
    -------
    df : pandas.DataFrame
        Dataframe containing the original data and new classifications
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(start_define, locals(), exclude_params=[&#39;df&#39;, &#39;terms_df&#39;])
    #if verbose:
    #    #Estimate when it will end, based on test run
    #    estTime = df.shape[0]/3054409 * 6 #It took about 6 minutes to classify data with entire dataframe. This estimates the fraction of that it will take
    #    nowTime = datetime.datetime.now()
    #    endTime = nowTime+datetime.timedelta(minutes=estTime)
    #    print(&#34;Start Term process should be done by {:d}:{:02d}&#34;.format(endTime.hour, endTime.minute))

    #First, for each startterm, find all results in df that start with, add classification flag, and add interpretation.
    for i,s in enumerate(terms_df[terms_col]):
        df[&#39;CLASS_FLAG&#39;].where(~df[description_col].str.startswith(s,na=False),4,inplace=True)
        df[&#39;LITHOLOGY&#39;].where(~df[description_col].str.startswith(s,na=False),terms_df.loc[i,&#39;LITHOLOGY&#39;],inplace=True)
    df[&#39;BEDROCK_FLAG&#39;].loc[df[&#34;LITHOLOGY&#34;] == &#39;BEDROCK&#39;]
    
    if verbose:
        numRecsClass = int(df[df[&#39;CLASS_FLAG&#39;]==4][&#39;CLASS_FLAG&#39;].sum())
        percRecsClass= round((df[df[&#39;CLASS_FLAG&#39;]==4][&#39;CLASS_FLAG&#39;].sum()/df.shape[0])*100,2)
        recsRemainig = int(df.shape[0]-numRecsClass)

        print(&#39;Classified well records using initial substring matches&#39;)
        print(&#34;\t{} records classified using initial substring matches ({}% of unclassified  data)&#34;.format(numRecsClass, percRecsClass))
        print(&#39;\t{} records remain unclassified ({}% of unclassified  data).&#39;.format(recsRemainig, 1-percRecsClass))
    return df</code></pre>
</details>
</dd>
<dt id="w4h.verbose_print"><code class="name flex">
<span>def <span class="ident">verbose_print</span></span>(<span>func, local_variables, exclude_params=[])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def verbose_print(func, local_variables, exclude_params=[]):
    print_list = [&#39;\n&#39;]
    sTime = datetime.datetime.now()
    print_list.append(f&#34;{func.__name__}&#34;)
    print_list.append(f&#34;\tStarted at {sTime}.&#34;)
    print_list.append(f&#34;\tParameters:&#34;)
    for k, v in local_variables.items():
        if k in inspect.signature(func).parameters:
            if &#39;kwargs&#39; in k:
                print_list.append(f&#34;\t\t{k}&#34;)
                for kk, vv in local_variables[k].items():
                    print_list.append(f&#34;\t\t\t{kk}={vv}&#34;)
            elif k in exclude_params:
                print_list.append(f&#34;\t\t{k}=&lt;input object&gt;&#34;)
            else:
                print_list.append(f&#34;\t\t{k}={v}&#34;)

    for line in print_list:
        print(line)
    return print_list</code></pre>
</details>
</dd>
<dt id="w4h.wildcard_define"><code class="name flex">
<span>def <span class="ident">wildcard_define</span></span>(<span>df, terms_df, description_col='FORMATION', terms_col='DESCRIPTION', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to classify descriptions according to any substring. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all the well descriptions</dd>
<dt><strong><code>terms_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all the startswith substrings to use for searching</dd>
<dt><strong><code>description_col</code></strong> :&ensp;<code>str</code>, default <code>= 'FORMATION'</code></dt>
<dd>Name of column in df containing descriptions, by default 'FORMATION'</dd>
<dt><strong><code>terms_col</code></strong> :&ensp;<code>str</code>, default <code>= 'FORMATION'</code></dt>
<dd>Name of column in terms_df containing startswith substring to match with description_col, by default 'FORMATION'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print out results, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log results to log file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing the original data and new classifications</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wildcard_define(df, terms_df, description_col=&#39;FORMATION&#39;, terms_col=&#39;DESCRIPTION&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to classify descriptions according to any substring. 

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe containing all the well descriptions
    terms_df : pandas.DataFrame
        Dataframe containing all the startswith substrings to use for searching
    description_col : str, default = &#39;FORMATION&#39;
        Name of column in df containing descriptions, by default &#39;FORMATION&#39;
    terms_col : str, default = &#39;FORMATION&#39;
        Name of column in terms_df containing startswith substring to match with description_col, by default &#39;FORMATION&#39;
    verbose : bool, default = False
        Whether to print out results, by default False
    log : bool, default = True
        Whether to log results to log file

    Returns
    -------
    df : pandas.DataFrame
        Dataframe containing the original data and new classifications
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(wildcard_define, locals(), exclude_params=[&#39;df&#39;, &#39;terms_df&#39;])
    #if verbose:
    #    #Estimate when it will end, based on test run
    #    estTime = df.shape[0]/3054409 * 6 #It took about 6 minutes to classify data with entire dataframe. This estimates the fraction of that it will take
    #    nowTime = datetime.datetime.now()
    #    endTime = nowTime+datetime.timedelta(minutes=estTime)
    #    print(&#34;Wildcard Term process should be done by (?) {:d}:{:02d}&#34;.format(endTime.hour, endTime.minute))

    #First, for each startterm, find all results in df that start with, add classification flag, and add interpretation.
    for i,s in enumerate(terms_df[terms_col]):
        df[&#39;CLASS_FLAG&#39;].where(~df[description_col].str.contains(s, case=False, regex=False, na=False), 5, inplace=True)
        df[&#39;LITHOLOGY&#39;].where(~df[description_col].str.contains(s, case=False, regex=False, na=False),terms_df.loc[i,&#39;LITHOLOGY&#39;],inplace=True)
    df[&#39;BEDROCK_FLAG&#39;].loc[df[&#34;LITHOLOGY&#34;] == &#39;BEDROCK&#39;]
    
    if verbose:
        numRecsClass = int(df[df[&#39;CLASS_FLAG&#39;]==5][&#39;CLASS_FLAG&#39;].sum())
        percRecsClass= round((df[df[&#39;CLASS_FLAG&#39;]==5][&#39;CLASS_FLAG&#39;].sum()/df.shape[0])*100,2)
        recsRemainig = int(df.shape[0]-numRecsClass)

        print(&#39;Classified well records using any substring (wildcard) match&#39;)
        print(&#34;\t{} records classified using any substring match ({}% of unclassified  data)&#34;.format(numRecsClass, percRecsClass))
        print(&#39;\t{} records remain unclassified ({}% of unclassified  data).&#39;.format(recsRemainig, 1-percRecsClass))
    return df</code></pre>
</details>
</dd>
<dt id="w4h.xyz_metadata_merge"><code class="name flex">
<span>def <span class="ident">xyz_metadata_merge</span></span>(<span>xyz, metadata, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Add elevation to header data file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>xyz</code></strong> :&ensp;<code>pandas.Dataframe</code></dt>
<dd>Contains elevation for the points</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>pandas dataframe</code></dt>
<dd>Header data file</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>headerXYZData</code></strong> :&ensp;<code>pandas.Dataframe</code></dt>
<dd>Header dataset merged to get elevation values</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def xyz_metadata_merge(xyz, metadata, verbose=False, log=False):
    &#34;&#34;&#34;Add elevation to header data file.

    Parameters
    ----------
    xyz : pandas.Dataframe
        Contains elevation for the points
    metadata : pandas dataframe
        Header data file
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    headerXYZData : pandas.Dataframe
        Header dataset merged to get elevation values

    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    if verbose:
        verbose_print(xyz_metadata_merge, locals(), exclude_params=[&#39;xyz&#39;])
    headerXYZData = metadata.merge(xyz, how=&#39;left&#39;, on=&#39;API_NUMBER&#39;)
    headerXYZData.drop([&#39;LATITUDE_x&#39;, &#39;LONGITUDE_x&#39;], axis=1, inplace=True)
    headerXYZData.rename({&#39;LATITUDE_y&#39;:&#39;LATITUDE&#39;, &#39;LONGITUDE_y&#39;:&#39;LONGITUDE&#39;}, axis=1, inplace=True)
    return headerXYZData</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="w4h.classify" href="classify.html">w4h.classify</a></code></li>
<li><code><a title="w4h.clean" href="clean.html">w4h.clean</a></code></li>
<li><code><a title="w4h.core" href="core.html">w4h.core</a></code></li>
<li><code><a title="w4h.export" href="export.html">w4h.export</a></code></li>
<li><code><a title="w4h.layers" href="layers.html">w4h.layers</a></code></li>
<li><code><a title="w4h.mapping" href="mapping.html">w4h.mapping</a></code></li>
<li><code><a title="w4h.read" href="read.html">w4h.read</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="w4h.add_control_points" href="#w4h.add_control_points">add_control_points</a></code></li>
<li><code><a title="w4h.align_rasters" href="#w4h.align_rasters">align_rasters</a></code></li>
<li><code><a title="w4h.clip_gdf2study_area" href="#w4h.clip_gdf2study_area">clip_gdf2study_area</a></code></li>
<li><code><a title="w4h.combine_dataset" href="#w4h.combine_dataset">combine_dataset</a></code></li>
<li><code><a title="w4h.coords2geometry" href="#w4h.coords2geometry">coords2geometry</a></code></li>
<li><code><a title="w4h.define_dtypes" href="#w4h.define_dtypes">define_dtypes</a></code></li>
<li><code><a title="w4h.depth_define" href="#w4h.depth_define">depth_define</a></code></li>
<li><code><a title="w4h.export_dataframe" href="#w4h.export_dataframe">export_dataframe</a></code></li>
<li><code><a title="w4h.export_grids" href="#w4h.export_grids">export_grids</a></code></li>
<li><code><a title="w4h.export_undefined" href="#w4h.export_undefined">export_undefined</a></code></li>
<li><code><a title="w4h.file_setup" href="#w4h.file_setup">file_setup</a></code></li>
<li><code><a title="w4h.fill_unclassified" href="#w4h.fill_unclassified">fill_unclassified</a></code></li>
<li><code><a title="w4h.get_current_date" href="#w4h.get_current_date">get_current_date</a></code></li>
<li><code><a title="w4h.get_drift_thick" href="#w4h.get_drift_thick">get_drift_thick</a></code></li>
<li><code><a title="w4h.get_layer_depths" href="#w4h.get_layer_depths">get_layer_depths</a></code></li>
<li><code><a title="w4h.get_most_recent" href="#w4h.get_most_recent">get_most_recent</a></code></li>
<li><code><a title="w4h.get_resources" href="#w4h.get_resources">get_resources</a></code></li>
<li><code><a title="w4h.get_search_terms" href="#w4h.get_search_terms">get_search_terms</a></code></li>
<li><code><a title="w4h.get_unique_wells" href="#w4h.get_unique_wells">get_unique_wells</a></code></li>
<li><code><a title="w4h.grid2study_area" href="#w4h.grid2study_area">grid2study_area</a></code></li>
<li><code><a title="w4h.layer_interp" href="#w4h.layer_interp">layer_interp</a></code></li>
<li><code><a title="w4h.layer_target_thick" href="#w4h.layer_target_thick">layer_target_thick</a></code></li>
<li><code><a title="w4h.logger_function" href="#w4h.logger_function">logger_function</a></code></li>
<li><code><a title="w4h.merge_lithologies" href="#w4h.merge_lithologies">merge_lithologies</a></code></li>
<li><code><a title="w4h.merge_metadata" href="#w4h.merge_metadata">merge_metadata</a></code></li>
<li><code><a title="w4h.read_dict" href="#w4h.read_dict">read_dict</a></code></li>
<li><code><a title="w4h.read_dictionary_terms" href="#w4h.read_dictionary_terms">read_dictionary_terms</a></code></li>
<li><code><a title="w4h.read_grid" href="#w4h.read_grid">read_grid</a></code></li>
<li><code><a title="w4h.read_lithologies" href="#w4h.read_lithologies">read_lithologies</a></code></li>
<li><code><a title="w4h.read_model_grid" href="#w4h.read_model_grid">read_model_grid</a></code></li>
<li><code><a title="w4h.read_raw_csv" href="#w4h.read_raw_csv">read_raw_csv</a></code></li>
<li><code><a title="w4h.read_study_area" href="#w4h.read_study_area">read_study_area</a></code></li>
<li><code><a title="w4h.read_wcs" href="#w4h.read_wcs">read_wcs</a></code></li>
<li><code><a title="w4h.read_wms" href="#w4h.read_wms">read_wms</a></code></li>
<li><code><a title="w4h.read_xyz" href="#w4h.read_xyz">read_xyz</a></code></li>
<li><code><a title="w4h.remerge_data" href="#w4h.remerge_data">remerge_data</a></code></li>
<li><code><a title="w4h.remove_bad_depth" href="#w4h.remove_bad_depth">remove_bad_depth</a></code></li>
<li><code><a title="w4h.remove_no_depth" href="#w4h.remove_no_depth">remove_no_depth</a></code></li>
<li><code><a title="w4h.remove_no_description" href="#w4h.remove_no_description">remove_no_description</a></code></li>
<li><code><a title="w4h.remove_no_topo" href="#w4h.remove_no_topo">remove_no_topo</a></code></li>
<li><code><a title="w4h.remove_nonlocated" href="#w4h.remove_nonlocated">remove_nonlocated</a></code></li>
<li><code><a title="w4h.run" href="#w4h.run">run</a></code></li>
<li><code><a title="w4h.sample_raster_points" href="#w4h.sample_raster_points">sample_raster_points</a></code></li>
<li><code><a title="w4h.sort_dataframe" href="#w4h.sort_dataframe">sort_dataframe</a></code></li>
<li><code><a title="w4h.specific_define" href="#w4h.specific_define">specific_define</a></code></li>
<li><code><a title="w4h.split_defined" href="#w4h.split_defined">split_defined</a></code></li>
<li><code><a title="w4h.start_define" href="#w4h.start_define">start_define</a></code></li>
<li><code><a title="w4h.verbose_print" href="#w4h.verbose_print">verbose_print</a></code></li>
<li><code><a title="w4h.wildcard_define" href="#w4h.wildcard_define">wildcard_define</a></code></li>
<li><code><a title="w4h.xyz_metadata_merge" href="#w4h.xyz_metadata_merge">xyz_metadata_merge</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>