<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>w4h API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>w4h</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#__init__.py


#from w4h import classify, clean, export, layers, mapping, read

from w4h.core import(logger_function,
                          run)

from w4h.classify import (specific_define, 
                          split_defined, 
                          start_define,
                          wildcard_define,
                          remerge_data, 
                          depth_define, 
                          export_undefined, 
                          fill_unclassified, 
                          merge_lithologies, 
                          get_unique_wells,
                          sort_dataframe)

from w4h.clean import (remove_nonlocated, 
                       remove_no_topo, 
                       remove_no_depth, 
                       remove_bad_depth, 
                       remove_no_formation)

from w4h.export import (export_dataframe,
                        export_grids)

from w4h.layers import (get_layer_depths,
                        merge_tables, 
                        layer_target_thick, 
                        layer_interp,
                        combine_dataset)

from w4h.mapping import (read_study_area, 
                         coords2geometry, 
                         clip_gdf2study_area, 
                         sample_raster_points, 
                         xyz_metadata_merge, 
                         read_wms,
                         read_wcs, 
                         grid2study_area,
                         read_model_grid,
                         read_grid,
                         align_rasters,
                         get_drift_thick)

from w4h.read import (get_current_date,
                      get_most_recent,
                      file_setup,
                      read_raw_csv,
                      read_xyz,
                      read_dict,
                      define_dtypes,
                      get_search_terms,
                      read_dictionary_terms,
                      read_lithologies)


__all__=(
        &#39;specific_define&#39;, 
        &#39;split_defined&#39;, 
        &#39;start_define&#39;,
        &#39;wildcard_define&#39;,
        &#39;remerge_data&#39;, 
        &#39;depth_define&#39;, 
        &#39;export_undefined&#39;, 
        &#39;fill_unclassified&#39;, 
        &#39;merge_lithologies&#39;, 
        &#39;get_unique_wells&#39;,
        &#39;sort_dataframe&#39;,
         &#39;remove_nonlocated&#39;, 
         &#39;remove_no_topo&#39;, 
         &#39;remove_no_depth&#39;, 
         &#39;remove_bad_depth&#39;, 
         &#39;remove_no_formation&#39;,
        &#39;export_dataframe&#39;,
        &#39;export_grids&#39;,
         &#39;get_layer_depths&#39;,
         &#39;merge_tables&#39;, 
         &#39;layer_target_thick&#39;, 
         &#39;layer_interp&#39;,
         &#39;combine_dataset&#39;,
        &#39;read_study_area&#39;, 
        &#39;coords2geometry&#39;, 
        &#39;clip_gdf2study_area&#39;, 
        &#39;sample_raster_points&#39;, 
        &#39;xyz_metadata_merge&#39;, 
        &#39;read_wms&#39;,
        &#39;read_wcs&#39;, 
        &#39;grid2study_area&#39;,
        &#39;read_model_grid&#39;,
        &#39;read_grid&#39;,
        &#39;align_rasters&#39;,
        &#39;get_drift_thick&#39;,
         &#39;get_current_date&#39;,
         &#39;get_most_recent&#39;,
         &#39;file_setup&#39;,
         &#39;read_raw_csv&#39;,
         &#39;read_xyz&#39;,
         &#39;read_dict&#39;,
         &#39;define_dtypes&#39;,
         &#39;get_search_terms&#39;,
         &#39;read_dictionary_terms&#39;,
         &#39;read_lithologies&#39;,
        &#39;logger_function&#39;,
        &#39;run&#39;)

__author__=&#39;Riley Balikian, Joe Franke, Allan Jones, Mike Krasowski&#39;</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="w4h.classify" href="classify.html">w4h.classify</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="w4h.clean" href="clean.html">w4h.clean</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="w4h.core" href="core.html">w4h.core</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="w4h.export" href="export.html">w4h.export</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="w4h.layers" href="layers.html">w4h.layers</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="w4h.mapping" href="mapping.html">w4h.mapping</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="w4h.read" href="read.html">w4h.read</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="w4h.align_rasters"><code class="name flex">
<span>def <span class="ident">align_rasters</span></span>(<span>grids_unaligned, modelgrid, no_data_val_grid=0, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Reprojects two rasters and aligns their pixels</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>grids_unaligned</code></strong> :&ensp;<code>list</code> or <code>xarray.DataArray</code></dt>
<dd>Contains a list of grids or one unaligned grid</dd>
<dt><strong><code>modelgrid</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>Contains model grid</dd>
<dt><strong><code>no_data_val_grid</code></strong> :&ensp;<code>int</code>, default=<code>0</code></dt>
<dd>Sets value of no data pixels</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>alignedGrids</code></strong> :&ensp;<code>list</code> or <code>xarray.DataArray</code></dt>
<dd>Contains aligned grids</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def align_rasters(grids_unaligned, modelgrid, no_data_val_grid=0, log=False):
    &#34;&#34;&#34;Reprojects two rasters and aligns their pixels

    Parameters
    ----------
    grids_unaligned : list or xarray.DataArray
        Contains a list of grids or one unaligned grid
    modelgrid : xarray.DataArray
        Contains model grid
    no_data_val_grid : int, default=0
        Sets value of no data pixels
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    alignedGrids : list or xarray.DataArray
        Contains aligned grids
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if type(grids_unaligned) is list:
        alignedGrids=[]
        for g in grids_unaligned:
            alignedGrid = g.rio.reproject_match(modelgrid)

            try:
                no_data_val_grid = alignedGrid.attrs[&#39;_FillValue&#39;] #Extract from dataset itself
            except:
                pass
            
            alignedGrid = alignedGrid.where(alignedGrid != no_data_val_grid)  #Replace no data values with NaNs
            
            alignedGrids.append(alignedGrid)
    else:
        alignedGrid = grids_unaligned.rio.reproject_match(modelgrid)

        try:
            noDataVal = alignedGrid.attrs[&#39;_FillValue&#39;] #Extract from dataset itself
        except:
            pass

        alignedGrids = alignedGrid.where(alignedGrid != noDataVal, other=np.nan)  #Replace no data values with NaNs
        
    return alignedGrids</code></pre>
</details>
</dd>
<dt id="w4h.clip_gdf2study_area"><code class="name flex">
<span>def <span class="ident">clip_gdf2study_area</span></span>(<span>study_area, gdf, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Clips dataframe to only include things within study area.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>study_area</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>Inputs study area polygon</dd>
<dt><strong><code>gdf</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>Inputs point data</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>gdfClip</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>Contains only points within the study area</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clip_gdf2study_area(study_area, gdf, log=False):
    &#34;&#34;&#34;Clips dataframe to only include things within study area.

    Parameters
    ----------
    study_area : geopandas.GeoDataFrame
        Inputs study area polygon
    gdf : geopandas.GeoDataFrame
        Inputs point data
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    gdfClip : geopandas.GeoDataFrame
        Contains only points within the study area
    
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    studyArea_proj = study_area.to_crs(gdf.crs).copy()
    gdfClip = gpd.clip(gdf, studyArea_proj) #Easier to project just study area to ensure data fit
    gdfClip.reset_index(inplace=True, drop=True) #Reset index
    
    return gdfClip</code></pre>
</details>
</dd>
<dt id="w4h.combine_dataset"><code class="name flex">
<span>def <span class="ident">combine_dataset</span></span>(<span>layer_dataset, surface_elev, bedrock_elev, layer_thick, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to combine xarray datasets or datarrays into a single xr.Dataset. Useful to add surface, bedrock, layer thick, and layer datasets all into one variable, for pickling, for example.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>layer_dataset</code></strong> :&ensp;<code>xr.DataArray </code></dt>
<dd>DataArray contining all the interpolated layer information.</dd>
<dt><strong><code>surface_elev</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>DataArray containing surface elevation data</dd>
<dt><strong><code>bedrock_elev</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>DataArray containing bedrock elevation data</dd>
<dt><strong><code>layer_thick</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>DataArray containing the layer thickness at each point in the model grid</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>xr.Dataset</code></dt>
<dd>Dataset with all input arrays set to different variables within the dataset.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_dataset(layer_dataset, surface_elev, bedrock_elev, layer_thick, log=False):
    &#34;&#34;&#34;Function to combine xarray datasets or datarrays into a single xr.Dataset. Useful to add surface, bedrock, layer thick, and layer datasets all into one variable, for pickling, for example.

    Parameters
    ----------
    layer_dataset : xr.DataArray 
        DataArray contining all the interpolated layer information.
    surface_elev : xr.DataArray
        DataArray containing surface elevation data
    bedrock_elev : xr.DataArray
        DataArray containing bedrock elevation data
    layer_thick : xr.DataArray
        DataArray containing the layer thickness at each point in the model grid
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    xr.Dataset
        Dataset with all input arrays set to different variables within the dataset.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    
    daDict = {}
    daDict[&#39;Layers&#39;] = layer_dataset
    daDict[&#39;Surface_Elev&#39;] = surface_elev
    daDict[&#39;Bedrock_Elev&#39;] = bedrock_elev
    daDict[&#39;Layer_Thickness&#39;] = layer_thick

    combined_dataset = xr.Dataset(daDict)

    return combined_dataset</code></pre>
</details>
</dd>
<dt id="w4h.coords2geometry"><code class="name flex">
<span>def <span class="ident">coords2geometry</span></span>(<span>df_no_geometry, xcol='LONGITUDE', ycol='LATITUDE', zcol='ELEV_FT', input_coords_crs='EPSG:4269', use_z=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds geometry to points with xy coordinates in the specified coordinate reference system.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df_no_geometry</code></strong> :&ensp;<code>pandas.Dataframe</code></dt>
<dd>a Pandas dataframe containing points</dd>
<dt><strong><code>xcol</code></strong> :&ensp;<code>str</code>, default=<code>'LONGITUDE'</code></dt>
<dd>Name of column holding x coordinate data in df_no_geometry</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default=<code>'LATITUDE'</code></dt>
<dd>Name of column holding y coordinate data in df_no_geometry</dd>
<dt><strong><code>zcol</code></strong> :&ensp;<code>str</code>, default=<code>'ELEV_FT'</code></dt>
<dd>Name of column holding z coordinate data in df_no_geometry</dd>
<dt><strong><code>input_coords_crs</code></strong> :&ensp;<code>str</code>, default=<code>'EPSG:4269</code></dt>
<dd>Name of crs used for geometry</dd>
<dt><strong><code>use_z</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to use z column in calculation</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>gdf</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>Geopandas dataframe with points and their geometry values</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coords2geometry(df_no_geometry, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, zcol=&#39;ELEV_FT&#39;, input_coords_crs=&#39;EPSG:4269&#39;, use_z=False, log=False):
    &#34;&#34;&#34;Adds geometry to points with xy coordinates in the specified coordinate reference system.

    Parameters
    ----------
    df_no_geometry : pandas.Dataframe
        a Pandas dataframe containing points
    xcol : str, default=&#39;LONGITUDE&#39;
        Name of column holding x coordinate data in df_no_geometry
    ycol : str, default=&#39;LATITUDE&#39;
        Name of column holding y coordinate data in df_no_geometry
    zcol : str, default=&#39;ELEV_FT&#39;
        Name of column holding z coordinate data in df_no_geometry
    input_coords_crs : str, default=&#39;EPSG:4269
        Name of crs used for geometry
    use_z : bool, default=False
        Whether to use z column in calculation
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    gdf : geopandas.GeoDataFrame
        Geopandas dataframe with points and their geometry values

    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    df = df_no_geometry.copy()

    ptCRS=input_coords_crs

    x = df[xcol].to_numpy()
    y = df[ycol].to_numpy()
    z = df[zcol].to_numpy()

    #coords = pd.concat([y, x], axis=1)
    if use_z:
        df[&#34;geometry&#34;] = gpd.points_from_xy(x, y, z=z, crs=ptCRS)
    else:
        df[&#34;geometry&#34;] = gpd.points_from_xy(x, y, crs=ptCRS)
        
    gdf = gpd.GeoDataFrame(df, crs=ptCRS)
    return gdf</code></pre>
</details>
</dd>
<dt id="w4h.define_dtypes"><code class="name flex">
<span>def <span class="ident">define_dtypes</span></span>(<span>undefined_df, datatypes=None, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to define datatypes of a dataframe, especially with file-indicated dyptes</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>undefined_df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Pandas dataframe with columns whose datatypes need to be (re)defined</dd>
<dt><strong><code>datatypes</code></strong> :&ensp;<code>dict, str, pathlib.PurePath() object,</code> or <code>None</code>, default <code>= None</code></dt>
<dd>Dictionary containing datatypes, to be used in pandas.DataFrame.astype() function. If None, will read from file indicated by dtype_file (which must be defined, along with dtype_dir), by default None</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dfout</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing redefined columns</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def define_dtypes(undefined_df, datatypes=None, verbose=False, log=False):
    &#34;&#34;&#34;Function to define datatypes of a dataframe, especially with file-indicated dyptes

    Parameters
    ----------
    undefined_df : pd.DataFrame
        Pandas dataframe with columns whose datatypes need to be (re)defined
    datatypes : dict, str, pathlib.PurePath() object, or None, default = None
        Dictionary containing datatypes, to be used in pandas.DataFrame.astype() function. If None, will read from file indicated by dtype_file (which must be defined, along with dtype_dir), by default None
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    dfout : pandas.DataFrame
        Pandas dataframe containing redefined columns
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    dfout = undefined_df.copy()
    
    if isinstance(datatypes, pathlib.PurePath) or isinstance(datatypes, str):
        datatypes = pathlib.Path(datatypes)

        if not datatypes.exists():
            if verbose:
                print(&#39;ERROR: datatypes ({}) does not exist&#39;.format(datatypes))
            return dfout
        elif datatypes.is_dir():
            if verbose:
                print(&#39;ERROR: datatypes must be either dict or filepath (path to directories not allowed)&#39;)
            return dfout

        datatypes = read_dict(file=datatypes)
        dfout = dfout.astype(datatypes)

    elif isinstance(datatypes, dict):
        if verbose:
            print(&#39;datatypes is None, not updating datatypes&#39;)
        dfout = dfout.astype(datatypes)
    else:
        if verbose:
            print(&#39;ERROR: datatypes must be either dict or a filepath, not {}&#39;.format(type(datatypes)))
        return dfout
    
    #This is likely redundant
    dfcols = dfout.columns
    for i in range(0, np.shape(dfout)[1]):
        dfout.iloc[:,i] = undefined_df.iloc[:,i].astype(datatypes[dfcols[i]])

    return dfout</code></pre>
</details>
</dd>
<dt id="w4h.depth_define"><code class="name flex">
<span>def <span class="ident">depth_define</span></span>(<span>dfIN, top_col='TOP', thresh=550.0, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to define all intervals lower than thresh as bedrock</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dfIN</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe to classify</dd>
<dt><strong><code>top_col</code></strong> :&ensp;<code>str</code>, default <code>= 'TOP'</code></dt>
<dd>Name of column that contains the depth information, likely of the top of the well interval, by default 'TOP'</dd>
<dt><strong><code>thresh</code></strong> :&ensp;<code>float</code>, default <code>= 550.0</code></dt>
<dd>Depth (in units used in dfIN['top_col']) below which all intervals will be classified as bedrock, by default 550.0.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print results, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log results to log file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing intervals classified as bedrock due to depth</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def depth_define(dfIN, top_col=&#39;TOP&#39;, thresh=550.0, verbose=False, log=False):
    &#34;&#34;&#34;Function to define all intervals lower than thresh as bedrock

    Parameters
    ----------
    dfIN : pandas.DataFrame
        Dataframe to classify
    top_col : str, default = &#39;TOP&#39;
        Name of column that contains the depth information, likely of the top of the well interval, by default &#39;TOP&#39;
    thresh : float, default = 550.0
        Depth (in units used in dfIN[&#39;top_col&#39;]) below which all intervals will be classified as bedrock, by default 550.0.
    verbose : bool, default = False
        Whether to print results, by default False
    log : bool, default = True
        Whether to log results to log file

    Returns
    -------
    df : pandas.DataFrame
        Dataframe containing intervals classified as bedrock due to depth
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    df = dfIN.copy()
    df[&#39;CLASS_FLAG&#39;].mask(df[top_col]&gt;thresh, 3 ,inplace=True) #Add a Classification Flag of 3 (bedrock b/c it&#39;s deepter than 550&#39;) to all records where the top of the interval is &gt;550&#39;
    df[&#39;BEDROCK_FLAG&#39;].mask(df[top_col]&gt;thresh, True, inplace=True)

    if verbose:
        if df.CLASS_FLAG.notnull().sum() == 0:
            brDepthClass = 0
        else:
            brDepthClass = df[&#39;CLASS_FLAG&#39;].value_counts()[3.0]
        total = dfIN.shape[0]
        print(&#34;Records classified as bedrock that were deeper than &#34;+str(thresh)+ &#34;&#39;: &#34; + str(brDepthClass))
        print(&#34;This represents &#34;+str(round((brDepthClass)*100/total,2))+&#34;% of the unclassified data in this dataframe.&#34;)
        
    return df</code></pre>
</details>
</dd>
<dt id="w4h.export_dataframe"><code class="name flex">
<span>def <span class="ident">export_dataframe</span></span>(<span>df, out_dir, filename, date_stamp=True, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to export dataframes</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas dataframe,</code> or <code>list</code> of <code>pandas dataframes</code></dt>
<dd>Data frame or list of dataframes to be exported</dd>
<dt><strong><code>out_dir</code></strong> :&ensp;<code>string</code> or <code>pathlib.Path object</code></dt>
<dd>Directory to which to export dataframe object(s) as .csv</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code> or <code>list</code> of <code>strings</code></dt>
<dd>Filename(s) of output files</dd>
<dt><strong><code>date_stamp</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to include a datestamp in the filename. If true, file ends with _yyyy-mm-dd.csv of current date, by default True.</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_dataframe(df, out_dir, filename, date_stamp=True, log=False):
    &#34;&#34;&#34;Function to export dataframes

    Parameters
    ----------
    df : pandas dataframe, or list of pandas dataframes
        Data frame or list of dataframes to be exported
    out_dir : string or pathlib.Path object
        Directory to which to export dataframe object(s) as .csv
    filename : str or list of strings
        Filename(s) of output files
    date_stamp : bool, default=True
        Whether to include a datestamp in the filename. If true, file ends with _yyyy-mm-dd.csv of current date, by default True.
    log : bool, default = True
        Whether to log inputs and outputs to log file.        
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if date_stamp:
        todayDate = datetime.date.today()
        todayDateStr = &#39;_&#39;+str(todayDate)
    else:
        todayDateStr=&#39;&#39;

    if type(out_dir) is str or isinstance(out_dir, pathlib.PurePath):
        out_dir = str(out_dir)
        out_dir = out_dir.replace(&#39;\\&#39;, &#39;/&#39;).replace(&#39;\\&#39;[-1], &#39;/&#39;)
        if out_dir[-1] != &#39;/&#39;:
            out_dir = out_dir + &#39;/&#39;
    else:
        print(&#39;Please input string or pathlib object for out_dir parameters&#39;)
        return

    if type(filename) is str:
        dfOutFile =  out_dir+filename+todayDateStr+&#39;.csv&#39;
        df.to_csv(dfOutFile, index_label=&#39;ID&#39;)
        print(&#39;Exported &#39;+filename+todayDateStr+&#39;.csv&#39;)
    elif type(filename) is list and type(df) is list and len(df) == len(filename):
        for i, f in enumerate(df):
            fname = filename[i]
            dfOutFile =  out_dir+fname+todayDateStr+&#39;.csv&#39;
            f.to_csv(dfOutFile, index_label=&#39;ID&#39;)
            print(&#39;Exported &#39;+fname+todayDateStr+&#39;.csv&#39;)</code></pre>
</details>
</dd>
<dt id="w4h.export_grids"><code class="name flex">
<span>def <span class="ident">export_grids</span></span>(<span>grid_data, out_path, file_id='', filetype='tif', variable_sep=True, date_stamp=True, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to export grids to files.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>grid_data</code></strong> :&ensp;<code>xarray DataArray</code> or <code>xarray Dataset</code></dt>
<dd>Dataset or dataarray to be exported</dd>
<dt><strong><code>out_path</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>Output location for data export. If variable_sep=True, this should be a directory. Otherwise, this should also include the filename. The file extension should not be included here.</dd>
<dt><strong><code>file_id</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>If specified, will add this after 'LayerXX' or 'AllLayers' in the filename, just before datestamp, if used. Example filename for file_id='Coarse': Layer1_Coarse_2023-04-18.tif.</dd>
<dt><strong><code>filetype</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Output filetype. Can either be pickle or any file extension supported by rioxarray.rio.to_raster(). Can either include period or not., by default 'tif'</dd>
<dt><strong><code>variable_sep</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If grid_data is an xarray Dataset, this will export each variable in the dataset as a separate file, including the variable name in the filename, by default False</dd>
<dt><strong><code>date_stamp</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to include a date stamp in the file name., by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_grids(grid_data, out_path, file_id=&#39;&#39;,filetype=&#39;tif&#39;, variable_sep=True, date_stamp=True, log=False):
    &#34;&#34;&#34;Function to export grids to files.

    Parameters
    ----------
    grid_data : xarray DataArray or xarray Dataset
        Dataset or dataarray to be exported
    out_path : str or pathlib.Path object
        Output location for data export. If variable_sep=True, this should be a directory. Otherwise, this should also include the filename. The file extension should not be included here.
    file_id : str, optional
        If specified, will add this after &#39;LayerXX&#39; or &#39;AllLayers&#39; in the filename, just before datestamp, if used. Example filename for file_id=&#39;Coarse&#39;: Layer1_Coarse_2023-04-18.tif.
    filetype : str, optional
        Output filetype. Can either be pickle or any file extension supported by rioxarray.rio.to_raster(). Can either include period or not., by default &#39;tif&#39;
    variable_sep : bool, optional
        If grid_data is an xarray Dataset, this will export each variable in the dataset as a separate file, including the variable name in the filename, by default False
    date_stamp : bool, optional
        Whether to include a date stamp in the file name., by default True
    log : bool, default = True
        Whether to log inputs and outputs to log file.        
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    #Initialize lists to determine which filetype will be used for export
    ncdfList = [&#39;netcdf&#39;, &#39;ncdf&#39;, &#39;n&#39;]
    tifList = [&#39;tif&#39;, &#39;tiff&#39;, &#39;geotiff&#39;, &#39;geotif&#39;, &#39;t&#39;]
    pickleList = [&#39;pickle&#39;, &#39;pkl&#39;, &#39;p&#39;]

    #Format output string(s)
    #Format output filepath
    if type(out_path) is str or isinstance(out_path, pathlib.PurePath):
        if isinstance(out_path, pathlib.PurePath):
            pass
        else:
            out_path = pathlib.Path(out_path)
        if out_path.parent.exists()==False:
            print(&#39;Directory does not exist. Please enter a different value for the out_path parameter.&#39;)
            return        

        if out_path.is_dir():
            if isinstance(grid_data, xr.DataArray):
                if variable_sep:
                    lyrs = grid_data.coords[&#39;Layer&#39;].values
                    filenames = []
                    for l in lyrs:
                        filenames.append(&#39;Layer&#39;+str(l))
                else:
                    filenames = [&#39;AllLayers&#39;]
            if isinstance(grid_data, xr.Dataset):
                if variable_sep:
                    filenames = []
                    for var in grid_data:
                        filenames.append(var)
                else:
                    filenames = [&#39;AllLayers&#39;]    
        else:
            filenames = [out_path.stem]
            out_path = out_path.parent

    else:
        print(&#39;Please input string or pathlib object for out_path parameters&#39;)
        return
    
    #Format datestamp, if desired in output filename
    if date_stamp:
        todayDate = datetime.date.today()
        todayDateStr = &#39;_&#39;+str(todayDate)
    else:
        todayDateStr=&#39;&#39;

    #Ensure the file suffix includes .
    if filetype[0] == &#39;.&#39;:
        pass
    else:
        filetype = &#39;.&#39; + filetype

    if file_id != &#39;&#39;:
        file_id = &#39;_&#39;+file_id

    out_path = out_path.as_posix()+&#39;/&#39;
    outPaths = []
    for f in filenames:
        outPaths.append(out_path+f+file_id+todayDateStr+filetype)

    #Do export
    if filetype.lower() in pickleList:
        import pickle
        for op in outPaths:
            try:
                with open(op, &#39;wb&#39;) as f:
                    pickle.dump(grid_data, f)
            except:
                print(&#39;An error occured during export.&#39;)
                print(op, &#39;could not be exported as a pickle object.&#39;)
                print(&#39;Try again using different parameters.&#39;)
    else:
        import rioxarray as rxr
        try:
            if isinstance(grid_data, xr.Dataset):
                if variable_sep:
                    for i, var in enumerate(grid_data.data_vars):
                        grid_data[var].rio.to_raster(outPaths[i])
                else:
                    grid_data.rio.to_raster(outPaths[0])
            elif isinstance(grid_data, xr.DataArray):
                if variable_sep:
                    lyrs = grid_data.coords[&#39;Layer&#39;].values
                    for i, l in enumerate(lyrs):
                        out_grid = grid_data.sel(Layer = l).copy()
                        out_grid.rio.to_raster(outPaths[i])
                else:
                    grid_data.rio.to_raster(outPaths[0])
            else:
                grid_data.rio.to_raster(outPaths[0])
        except:
            print(&#39;An error occured during export.&#39;)
            print(&#39;{} could not be exported as {} file.&#39;.format(outPaths, filetype))
            print(&#39;Try again using different parameters.&#39;)

    return</code></pre>
</details>
</dd>
<dt id="w4h.export_undefined"><code class="name flex">
<span>def <span class="ident">export_undefined</span></span>(<span>df, outdir)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to export terms that still need to be defined.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing at least some unclassified data</dd>
<dt><strong><code>outdir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code></dt>
<dd>Directory to save file. Filename will be generated automatically based on today's date.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>stillNeededDF</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing only unclassified terms, and the number of times they occur</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_undefined(df, outdir):
    &#34;&#34;&#34;Function to export terms that still need to be defined.

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe containing at least some unclassified data
    outdir : str or pathlib.Path
        Directory to save file. Filename will be generated automatically based on today&#39;s date.

    Returns
    -------
    stillNeededDF : pandas.DataFrame
        Dataframe containing only unclassified terms, and the number of times they occur
    &#34;&#34;&#34;
    import pathlib
    if isinstance(outdir, pathlib.PurePath):
        if not outdir.is_dir() or not outdir.exists():
            print(&#39;Please specify a valid directory for export. Filename is generated automatically.&#39;)
            return
        outdir = outdir.as_posix()
    else:
        outdir.replace(&#39;\\&#39;,&#39;/&#39;)
        outdir.replace(&#39;\\&#39;[-1], &#39;/&#39;)

    #Get directory path correct        
    if outdir[-1] != &#39;/&#39;:
        outdir = outdir+&#39;/&#39;

    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)
    searchDF = df[df[&#39;CLASS_FLAG&#39;].isna()]
    
    stillNeededDF=searchDF[&#39;FORMATION&#39;].value_counts()
    stillNeededDF.to_csv(outdir+&#39;Undefined_&#39;+todayDateStr+&#39;.csv&#39;)
    return stillNeededDF</code></pre>
</details>
</dd>
<dt id="w4h.file_setup"><code class="name flex">
<span>def <span class="ident">file_setup</span></span>(<span>well_data, metadata=None, data_filename='*ISGS_DOWNHOLE_DATA*.txt', metadata_filename='*ISGS_HEADER*.txt', log_dir=None, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to setup files, assuming data, metadata, and elevation/location are in separate files (there should be one "key"/identifying column consistent across all files to join/merge them later)</p>
<p>This function may not be useful if files are organized differently than this structure.
If that is the case, it is recommended to use the get_most_recent() function for each individual file if needed.
It may also be of use to simply skip this function altogether and directly define each filepath in a manner that can be used by pandas.read_csv()</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>well_data</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>Str or pathlib.Path to directory containing input files, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, optional</dt>
<dd>Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>data_filename</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Pattern used by pathlib.glob() to get the most recent data file, by default '<em>ISGS_DOWNHOLE_DATA</em>.txt'</dd>
<dt><strong><code>metadata_filename</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Pattern used by pathlib.glob() to get the most recent metadata file, by default '<em>ISGS_HEADER</em>.txt'</dd>
<dt><strong><code>log_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.PurePath()</code> or <code>None</code>, default=<code>None</code></dt>
<dd>Directory to place log file in. This is not read directly, but is used indirectly by w4h.logger_function()</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print name of files to terminal, by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Tuple with paths to (well_data, metadata)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def file_setup(well_data, metadata=None, data_filename=&#39;*ISGS_DOWNHOLE_DATA*.txt&#39;, metadata_filename=&#39;*ISGS_HEADER*.txt&#39;, log_dir=None, verbose=False, log=False):
    &#34;&#34;&#34;Function to setup files, assuming data, metadata, and elevation/location are in separate files (there should be one &#34;key&#34;/identifying column consistent across all files to join/merge them later)

    This function may not be useful if files are organized differently than this structure. 
    If that is the case, it is recommended to use the get_most_recent() function for each individual file if needed.
    It may also be of use to simply skip this function altogether and directly define each filepath in a manner that can be used by pandas.read_csv()

    Parameters
    ----------
    well_data : str or pathlib.Path object
        Str or pathlib.Path to directory containing input files, by default str(repoDir)+&#39;/resources&#39;
    metadata : str or pathlib.Path object, optional
        Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+&#39;/resources&#39;
    data_filename : str, optional
        Pattern used by pathlib.glob() to get the most recent data file, by default &#39;*ISGS_DOWNHOLE_DATA*.txt&#39;
    metadata_filename : str, optional
        Pattern used by pathlib.glob() to get the most recent metadata file, by default &#39;*ISGS_HEADER*.txt&#39;
    log_dir : str or pathlib.PurePath() or None, default=None
        Directory to place log file in. This is not read directly, but is used indirectly by w4h.logger_function()
    verbose : bool, default = False
        Whether to print name of files to terminal, by default True
    log : bool, default = True
        Whether to log inputs and outputs to log file.

    Returns
    -------
    tuple
        Tuple with paths to (well_data, metadata)
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    #Define  filepath variables to be used later for reading/writing files
    data_path = pathlib.Path(well_data)
    if metadata is None:
        origMetaPath = None
        metadata=data_path
    else:
        origMetaPath = metadata
        metadata=pathlib.Path(metadata)

    #If input path is a directory, find most recent version of the file. If file, just read the file
    if data_path.is_dir():
        downholeDataFILE = get_most_recent(data_path, data_filename, verbose=verbose)
    else:
        downholeDataFILE = data_path
    
    if metadata.is_dir():
        headerDataFILE = get_most_recent(metadata, metadata_filename, verbose=verbose)
        if headerDataFILE == []:
            headerDataFILE = downholeDataFILE
    else:
        if origMetaPath is None:
            headerDataFILE = downholeDataFILE
        else:
            headerDataFILE = metadata
       #Set all input as pathlib.Path objects (may be redundant, but just in case)
    downholeDataPATH = pathlib.Path(downholeDataFILE)
    headerDataPATH = pathlib.Path(headerDataFILE)

    if verbose:
        print(&#39;Using the following files:&#39;)
        print(&#39;\t&#39;, downholeDataFILE)
        print(&#39;\t&#39;, headerDataFILE)

    #Define datatypes, to use later
    #downholeDataDTYPES = {&#39;ID&#39;:np.uint32, &#34;API_NUMBER&#34;:np.uint64,&#34;TABLE_NAME&#34;:str,&#34;WHO&#34;:str,&#34;INTERPRET_DATE&#34;:str,&#34;FORMATION&#34;:str,&#34;THICKNESS&#34;:np.float64,&#34;TOP&#34;:np.float64,&#34;BOTTOM&#34;:np.float64}
    #headerDataDTYPES = {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#34;TDFORMATION&#34;:str,&#34;PRODFORM&#34;:str,&#34;TOTAL_DEPTH&#34;:np.float64,&#34;SECTION&#34;:np.float64,&#34;TWP&#34;:np.float64,&#34;TDIR&#34;:str,&#34;RNG&#34;:np.float64,&#34;RDIR&#34;:str,&#34;MERIDIAN&#34;:np.float64,&#34;FARM_NAME&#34;:str,&#34;NSFOOT&#34;:np.float64,&#34;NSDIR&#34;:str,&#34;EWFOOT&#34;:np.float64,&#34;EWDIR&#34;:str,&#34;QUARTERS&#34;:str,&#34;ELEVATION&#34;:np.float64,&#34;ELEVREF&#34;:str,&#34;COMP_DATE&#34;:str,&#34;STATUS&#34;:str,&#34;FARM_NUM&#34;:str,&#34;COUNTY_CODE&#34;:np.float64,&#34;PERMIT_NUMBER&#34;:str,&#34;COMPANY_NAME&#34;:str,&#34;COMPANY_CODE&#34;:str,&#34;PERMIT_DATE&#34;:str,&#34;CORNER&#34;:str,&#34;LATITUDE&#34;:np.float64,&#34;LONGITUDE&#34;:np.float64,&#34;ENTERED_BY&#34;:str,&#34;UPDDATE&#34;:str,&#34;ELEVSOURCE&#34;:str, &#34;ELEV_FT&#34;:np.float64}
    return downholeDataPATH, headerDataPATH</code></pre>
</details>
</dd>
<dt id="w4h.fill_unclassified"><code class="name flex">
<span>def <span class="ident">fill_unclassified</span></span>(<span>df, classification_col='CLASS_FLAG')</span>
</code></dt>
<dd>
<div class="desc"><p>Fills unclassified rows in 'CLASS_FLAG' column with np.nan</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe on which to perform operation</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe on which operation has been performed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fill_unclassified(df, classification_col=&#39;CLASS_FLAG&#39;):
    &#34;&#34;&#34;Fills unclassified rows in &#39;CLASS_FLAG&#39; column with np.nan

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe on which to perform operation

    Returns
    -------
    df : pandas.DataFrame
        Dataframe on which operation has been performed
    &#34;&#34;&#34;
    df[classification_col].fillna(0, inplace=True)
    return df</code></pre>
</details>
</dd>
<dt id="w4h.get_current_date"><code class="name flex">
<span>def <span class="ident">get_current_date</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="gets-the-current-date-to-help-with-finding-the-most-recent-file">Gets The Current Date To Help With Finding The Most Recent File</h2>
<h2 id="parameters">Parameters</h2>
<p>None</p>
<hr>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>todayDate
</code></dt>
<dd>datetime object with today's date</dd>
<dt><code>dateSuffix
</code></dt>
<dd>str to use for naming output files</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_current_date():
    &#34;&#34;&#34; Gets the current date to help with finding the most recent file
        ---------------------
        Parameters:
            None

        ---------------------
        Returns:
            todayDate   : datetime object with today&#39;s date
            dateSuffix  : str to use for naming output files
    &#34;&#34;&#34;
    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)
    dateSuffix = &#39;_&#39;+todayDateStr
    return todayDate, dateSuffix</code></pre>
</details>
</dd>
<dt id="w4h.get_drift_thick"><code class="name flex">
<span>def <span class="ident">get_drift_thick</span></span>(<span>surface, bedrock, layers=9, plot=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the distance from surface to bedrock and then divides by number of layers to get layer thickness.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>surface</code></strong> :&ensp;<code>rioxarray.DataArray</code></dt>
<dd>array holding surface elevation</dd>
<dt><strong><code>bedrock</code></strong> :&ensp;<code>rioxarray.DataArray</code></dt>
<dd>array holding bedrock elevation</dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>int</code>, default=<code>9</code></dt>
<dd>number of layers needed to calculate thickness for</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>tells function to either plot the data or not</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>driftThick</code></strong> :&ensp;<code>rioxarray.DataArray</code></dt>
<dd>Contains data array containing depth to bedrock at each point</dd>
<dt><strong><code>layerThick</code></strong> :&ensp;<code>rioxarray.DataArray</code></dt>
<dd>Contains data array with layer thickness at each point</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_drift_thick(surface, bedrock, layers=9, plot=False, log=False):
    &#34;&#34;&#34;Finds the distance from surface to bedrock and then divides by number of layers to get layer thickness.

    Parameters
    ----------
    surface : rioxarray.DataArray
        array holding surface elevation
    bedrock : rioxarray.DataArray
        array holding bedrock elevation
    layers : int, default=9
        number of layers needed to calculate thickness for
    plot : bool, default=False
        tells function to either plot the data or not

    Returns
    -------
    driftThick : rioxarray.DataArray
        Contains data array containing depth to bedrock at each point
    layerThick : rioxarray.DataArray
        Contains data array with layer thickness at each point

    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    xr.set_options(keep_attrs=True)

    driftThick = surface - bedrock
    driftThick = driftThick.clip(0,max=5000,keep_attrs=True)
    if plot:
        driftThick.plot()

    try:
        noDataVal = driftThick.attrs[&#39;_FillValue&#39;] #Extract from dataset itself
    except:
        noDataVal = 100001
    
    driftThick = driftThick.where(driftThick &lt;100000, other=np.nan)  #Replace no data values with NaNs
    driftThick = driftThick.where(driftThick &gt;-100000, other=np.nan)  #Replace no data values with NaNs

    layerThick = driftThick/layers
    
    xr.set_options(keep_attrs=&#39;default&#39;)

    return driftThick, layerThick</code></pre>
</details>
</dd>
<dt id="w4h.get_layer_depths"><code class="name flex">
<span>def <span class="ident">get_layer_depths</span></span>(<span>well_metadata, no_layers=9, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to calculate depths and elevations of each model layer at each well based on surface elevation, bedrock elevation, and number of layers/layer thickness</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>well_metadata</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing well metdata</dd>
<dt><strong><code>no_layers</code></strong> :&ensp;<code>int</code>, default=<code>9</code></dt>
<dd>Number of layers. This should correlate with get_drift_thick() input parameter, if drift thickness was calculated using that function, by default 9.</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Dataframe</code></dt>
<dd>Dataframe containing new columns for depth to layers and elevation of layers.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_layer_depths(well_metadata, no_layers=9, log=False):
    &#34;&#34;&#34;Function to calculate depths and elevations of each model layer at each well based on surface elevation, bedrock elevation, and number of layers/layer thickness

    Parameters
    ----------
    well_metadata : pandas.DataFrame
        Dataframe containing well metdata
    no_layers : int, default=9
        Number of layers. This should correlate with get_drift_thick() input parameter, if drift thickness was calculated using that function, by default 9.
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    pandas.Dataframe
        Dataframe containing new columns for depth to layers and elevation of layers.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    for layer in range(0, no_layers): #For each layer
        #Make column names
        depthColName  = &#39;DEPTH_FT_LAYER&#39;+str(layer+1)
        #depthMcolName = &#39;Depth_M_LAYER&#39;+str(layer) 

        #Calculate depth to each layer at each well, in feet and meters
        well_metadata[depthColName]  = well_metadata[&#39;LAYER_THICK_FT&#39;] * layer
        #headerData[depthMcolName] = headerData[depthColName] * 0.3048

    for layer in range(0, no_layers): #For each layer
        elevColName = &#39;ELEV_FT_LAYER&#39;+str(layer+1)
        #elevMColName = &#39;ELEV_M_LAYER&#39;+str(layer)
            
        well_metadata[elevColName]  = well_metadata[&#39;SURFACE_ELEV_FT&#39;] - well_metadata[&#39;LAYER_THICK_FT&#39;] * layer
        #headerData[elevMColName]  = headerData[&#39;SURFACE_ELEV_M&#39;] - headerData[&#39;LAYER_THICK_M&#39;] * layer
    return well_metadata</code></pre>
</details>
</dd>
<dt id="w4h.get_most_recent"><code class="name flex">
<span>def <span class="ident">get_most_recent</span></span>(<span>dir='C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology/resources', glob_pattern='*', verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to find the most recent file with the indicated pattern, using pathlib.glob function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, optional</dt>
<dd>Directory in which to find the most recent file, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>glob_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>String used by the pathlib.glob() function/method for searching, by default '*'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pathlib.Path object</code></dt>
<dd>Pathlib Path object of the most recent file fitting the glob pattern indicated in the glob_pattern parameter.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_most_recent(dir=str(repoDir)+&#39;/resources&#39;, glob_pattern=&#39;*&#39;, verbose=True):
    &#34;&#34;&#34;Function to find the most recent file with the indicated pattern, using pathlib.glob function.

    Parameters
    ----------
    dir : str or pathlib.Path object, optional
        Directory in which to find the most recent file, by default str(repoDir)+&#39;/resources&#39;
    glob_pattern : str, optional
        String used by the pathlib.glob() function/method for searching, by default &#39;*&#39;

    Returns
    -------
    pathlib.Path object
        Pathlib Path object of the most recent file fitting the glob pattern indicated in the glob_pattern parameter.
    &#34;&#34;&#34;
    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)

    files = pathlib.Path(dir).rglob(glob_pattern) #Get all the files that fit the pattern
    fileDates = []
    for f in files: #Get the file dates from their file modification times
        fileDates.append(np.datetime64(datetime.datetime.fromtimestamp(os.path.getmtime(f))))
    
    if fileDates == []:
        #If no files found that match pattern, return an empty pathlib.Path()
        if verbose:
            print(&#39;No file found in {} matching {} pattern&#39;.format(dir, glob_pattern))
        mostRecentFile = pathlib.Path()
        return mostRecentFile
    else:
        globInd = np.argmin(np.datetime64(todayDateStr) - np.array(fileDates)) #Find the index of the most recent file

    #Iterate through glob/files again (need to recreate glob)
    files = pathlib.Path(dir).rglob(glob_pattern)
    for j, f in enumerate(files):
        if j == globInd:
            mostRecentFile=f
            break
    
    if verbose:
        print(&#39;Most Recent version of this file is : &#39;+mostRecentFile.name)

    return mostRecentFile</code></pre>
</details>
</dd>
<dt id="w4h.get_search_terms"><code class="name flex">
<span>def <span class="ident">get_search_terms</span></span>(<span>spec_path='C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology/resources/', spec_glob_pattern='*SearchTerms-Specific*', start_path=None, start_glob_pattern='*SearchTerms-Start*', wildcard_path=None, wildcard_glob_pattern='*SearchTerms-Wildcard', log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Read in dictionary files for downhole data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spec_path</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, optional</dt>
<dd>Directory where the file containing the specific search terms is located, by default str(repoDir)+'/resources/'</dd>
<dt><strong><code>spec_glob_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default '<em>SearchTerms-Specific</em>'</dd>
<dt><strong><code>start_path</code></strong> :&ensp;<code>str</code> or <code>None</code>, optional</dt>
<dd>Directory where the file containing the start search terms is located, by default None</dd>
<dt><strong><code>start_glob_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default '<em>SearchTerms-Start</em>'</dd>
<dt><strong><code>wildcard_path</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, default <code>= None</code></dt>
<dd>Directory where the file containing the wildcard search terms is located, by default None</dd>
<dt><strong><code>wildcard_glob_pattern</code></strong> :&ensp;<code>str</code>, default <code>= '*SearchTerms-Wildcard'</code></dt>
<dd>Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default '<em>SearchTerms-Wildcard</em>'</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(specTermsPath, startTermsPath, wilcardTermsPath) : tuple
Tuple containing the pandas dataframes with specific search terms,
with start search terms, and with wildcard search terms</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_search_terms(spec_path=str(repoDir)+&#39;/resources/&#39;, spec_glob_pattern=&#39;*SearchTerms-Specific*&#39;, 
                     start_path=None, start_glob_pattern = &#39;*SearchTerms-Start*&#39;, 
                     wildcard_path=None, wildcard_glob_pattern=&#39;*SearchTerms-Wildcard&#39;,
                     log=False):
    &#34;&#34;&#34;Read in dictionary files for downhole data

    Parameters
    ----------
    spec_path : str or pathlib.Path, optional
        Directory where the file containing the specific search terms is located, by default str(repoDir)+&#39;/resources/&#39;
    spec_glob_pattern : str, optional
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Specific*&#39;
    start_path : str or None, optional
        Directory where the file containing the start search terms is located, by default None
    start_glob_pattern : str, optional
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Start*&#39;
    wildcard_path : str or pathlib.Path, default = None
        Directory where the file containing the wildcard search terms is located, by default None    
    wildcard_glob_pattern : str, default = &#39;*SearchTerms-Wildcard&#39;
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Wildcard*&#39;
    log : bool, default = True
        Whether to log inputs and outputs to log file.        

    Returns
    -------
    (specTermsPath, startTermsPath, wilcardTermsPath) : tuple
        Tuple containing the pandas dataframes with specific search terms,  with start search terms, and with wildcard search terms
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    
    #specTermsFile = &#34;SearchTerms-Specific_BedrockOrNo_2022-09.csv&#34; #Specific matches
    #startTermsFile = &#34;SearchTerms-Start_BedrockOrNo.csv&#34; #Wildcard matches for the start of the description

    #Exact match path
    if spec_path is None:
        specTermsPath = spec_path        
    else:
        spec_path = pathlib.Path(spec_path)

        if spec_path.is_dir():
            specTermsPath = get_most_recent(spec_path, spec_glob_pattern)
        else:
            specTermsPath = spec_path

    #Startswith path
    if start_path is None:
        startTermsPath = start_path        
    else:
        start_path = pathlib.Path(start_path)

        if start_path.is_dir():
            startTermsPath = get_most_recent(start_path, start_glob_pattern)
        else:
            startTermsPath = start_path

    #Wildcard Path
    if wildcard_path is None:
        wilcardTermsPath = wildcard_path        
    else:
        wildcard_path = pathlib.Path(wildcard_path)

        if wildcard_path.is_dir():
            wilcardTermsPath = get_most_recent(wildcard_path, wildcard_glob_pattern)
        else:
            wilcardTermsPath = wildcard_path
    
    return specTermsPath, startTermsPath, wilcardTermsPath</code></pre>
</details>
</dd>
<dt id="w4h.get_unique_wells"><code class="name flex">
<span>def <span class="ident">get_unique_wells</span></span>(<span>df, wellid_col='API_NUMBER', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets unique wells as a dataframe based on a given column name.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all wells and/or well intervals of interest</dd>
<dt><strong><code>wellid_col</code></strong> :&ensp;<code>str</code>, default=<code>"API_NUMBER"</code></dt>
<dd>Name of column in df containing a unique identifier for each well, by default 'API_NUMBER'. .unique() will be run on this column to get the unique values.</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>wellsDF</code></dt>
<dd>DataFrame containing only the unique well IDs</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_unique_wells(df, wellid_col=&#39;API_NUMBER&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Gets unique wells as a dataframe based on a given column name.

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe containing all wells and/or well intervals of interest
    wellid_col : str, default=&#34;API_NUMBER&#34;
        Name of column in df containing a unique identifier for each well, by default &#39;API_NUMBER&#39;. .unique() will be run on this column to get the unique values.
    log : bool, default = False
        Whether to log results to log file

    Returns
    -------
    wellsDF
        DataFrame containing only the unique well IDs
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    #Get Unique well APIs
    uniqueWells = df[wellid_col].unique()
    wellsDF = pd.DataFrame(uniqueWells)
    if verbose:
        print(&#39;Number of unique wells in downholeData: &#39;+str(wellsDF.shape[0]))
    wellsDF.columns = [&#39;UNIQUE_ID&#39;]
    
    return wellsDF</code></pre>
</details>
</dd>
<dt id="w4h.grid2study_area"><code class="name flex">
<span>def <span class="ident">grid2study_area</span></span>(<span>study_area, grid, study_area_crs='', grid_crs='', log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Clips grid to study area.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>study_area</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>inputs study area polygon</dd>
<dt><strong><code>grid</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>inputs grid array</dd>
<dt><strong><code>study_area_crs</code></strong> :&ensp;<code>str</code>, default=<code>''</code></dt>
<dd>inputs the coordinate reference system for the study area</dd>
<dt><strong><code>grid_crs</code></strong> :&ensp;<code>str</code>, default=<code>''</code></dt>
<dd>inputs the coordinate reference system for the grid</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>grid</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>returns xarray containing grid clipped only to area within study area</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def grid2study_area(study_area, grid, study_area_crs=&#39;&#39;, grid_crs=&#39;&#39;, log=False):
    &#34;&#34;&#34;Clips grid to study area.

    Parameters
    ----------
    study_area : geopandas.GeoDataFrame
        inputs study area polygon
    grid : xarray.DataArray
        inputs grid array
    study_area_crs : str, default=&#39;&#39;
        inputs the coordinate reference system for the study area
    grid_crs : str, default=&#39;&#39;
        inputs the coordinate reference system for the grid
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    grid : xarray.DataArray
        returns xarray containing grid clipped only to area within study area

    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    
    if study_area_crs==&#39;&#39;:
        study_area_crs=study_area.crs

    if grid_crs==&#39;&#39;:
        #Get EPSG of model grid
        subtext = grid.spatial_ref.crs_wkt[-20:]
        starInd = subtext.find(&#39;EPSG&#39;)
        grid_crs = subtext[starInd:-2].replace(&#39;&#34;&#39;,&#39;&#39;).replace(&#39;,&#39;,&#39;:&#39;)   
        #print(grid_crs)
    
    if study_area_crs != grid_crs:
        studyAreaUnproject = study_area.copy()
        study_area = study_area.to_crs(grid_crs)   
    else:
        study_area = study_area

    saExtent = study_area.total_bounds

    if grid[&#39;y&#39;][-1].values - grid[&#39;y&#39;][0].values &gt; 0:
        miny=saExtent[1]
        maxy=saExtent[3]
    else:
        miny=saExtent[3]
        maxy=saExtent[1]        
        
    if grid[&#39;x&#39;][-1].values - grid[&#39;x&#39;][0].values &gt; 0:
        minx=saExtent[0]
        maxx=saExtent[2]
    else:
        minx=saExtent[2]
        maxx=saExtent[0]
    grid = grid.sel(x=slice(minx, maxx), y=slice(miny, maxy)).sel(band=1)     

    return grid</code></pre>
</details>
</dd>
<dt id="w4h.layer_interp"><code class="name flex">
<span>def <span class="ident">layer_interp</span></span>(<span>points, grid, layers=None, method='nearest', return_type='dataarray', export_dir=None, targetcol='TARG_THICK_PER', lyrcol='LAYER', xcol=None, ycol=None, xcoord='x', ycoord='y', log=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to interpolate results, going from points to grid data. Uses scipy.interpolate module.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>points</code></strong> :&ensp;<code>list</code></dt>
<dd>List containing pandas dataframes or geopandas geoadataframes containing the point data. Should be resDF_list output from layer_target_thick().</dd>
<dt><strong><code>grid</code></strong> :&ensp;<code>xr.DataArray</code> or <code>xr.Dataset</code></dt>
<dd>Xarray dataarray with the coordinates/spatial reference of the output grid to interpolate to</dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>int</code>, default=<code>None</code></dt>
<dd>Number of layers for interpolation. If None, uses the length ofthe points list to determine number of layers. By default None.</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str, {'nearest', 'interp2d','linear', 'cloughtocher', 'radial basis function'}</code></dt>
<dd>Type of interpolation to use. See scipy.interpolate N-D scattered. Values can be any of the following (also shown in "kind" column of N-D scattered section of table here: <a href="https://docs.scipy.org/doc/scipy/tutorial/interpolate.html">https://docs.scipy.org/doc/scipy/tutorial/interpolate.html</a>). By default 'nearest'</dd>
<dt><strong><code>return_type</code></strong> :&ensp;<code>str, {'dataarray', 'dataset'}</code></dt>
<dd>Type of xarray object to return, either xr.DataArray or xr.Dataset, by default 'dataarray.'</dd>
<dt><strong><code>export_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, default=<code>None</code></dt>
<dd>Export directory for interpolated grids, using w4h.export_grids(). If None, does not export, by default None.</dd>
<dt><strong><code>targetcol</code></strong> :&ensp;<code>str</code>, default <code>= 'TARG_THICK_PER'</code></dt>
<dd>Name of column in points containing data to be interpolated, by default 'TARG_THICK_PER'.</dd>
<dt><strong><code>lyrcol</code></strong> :&ensp;<code>str</code>, default <code>= 'Layer'</code></dt>
<dd>Name of column containing layer number. Not currently used, by default 'LAYER'</dd>
<dt><strong><code>xcol</code></strong> :&ensp;<code>str</code>, default <code>= 'None'</code></dt>
<dd>Name of column containing x coordinates. If None, will look for 'geometry' column, as in a geopandas.GeoDataframe. By default None</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default <code>= 'None'</code></dt>
<dd>Name of column containing y coordinates. If None, will look for 'geometry' column, as in a geopandas.GeoDataframe. By default None</dd>
<dt><strong><code>xcoord</code></strong> :&ensp;<code>str</code>, default=<code>'x'</code></dt>
<dd>Name of x coordinate in grid, used to extract x values of grid, by default 'x'</dd>
<dt><strong><code>ycoord</code></strong> :&ensp;<code>str</code>, default=<code>'y'</code></dt>
<dd>Name of y coordinate in grid, used to extract x values of grid, by default 'y'</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments to be read directly into whichever scipy.interpolate function is designated by the method parameter.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>interp_data</code></strong> :&ensp;<code>xr.DataArray</code> or <code>xr.Dataset, depending on return_type</code></dt>
<dd>By default, returns an xr.DataArray object with the layers added as a new dimension called Layer. Can also specify return_type='dataset' to return an xr.Dataset with each layer as a separate variable.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def layer_interp(points, grid, layers=None, method=&#39;nearest&#39;, return_type=&#39;dataarray&#39;, export_dir=None, targetcol=&#39;TARG_THICK_PER&#39;, lyrcol=&#39;LAYER&#39;, xcol=None, ycol=None, xcoord=&#39;x&#39;, ycoord=&#39;y&#39;, log=False, **kwargs):
    &#34;&#34;&#34;Function to interpolate results, going from points to grid data. Uses scipy.interpolate module.

    Parameters
    ----------
    points : list
        List containing pandas dataframes or geopandas geoadataframes containing the point data. Should be resDF_list output from layer_target_thick().
    grid : xr.DataArray or xr.Dataset
        Xarray dataarray with the coordinates/spatial reference of the output grid to interpolate to
    layers : int, default=None
        Number of layers for interpolation. If None, uses the length ofthe points list to determine number of layers. By default None.
    method : str, {&#39;nearest&#39;, &#39;interp2d&#39;,&#39;linear&#39;, &#39;cloughtocher&#39;, &#39;radial basis function&#39;}
        Type of interpolation to use. See scipy.interpolate N-D scattered. Values can be any of the following (also shown in &#34;kind&#34; column of N-D scattered section of table here: https://docs.scipy.org/doc/scipy/tutorial/interpolate.html). By default &#39;nearest&#39;
    return_type : str, {&#39;dataarray&#39;, &#39;dataset&#39;}
        Type of xarray object to return, either xr.DataArray or xr.Dataset, by default &#39;dataarray.&#39;
    export_dir : str or pathlib.Path, default=None
        Export directory for interpolated grids, using w4h.export_grids(). If None, does not export, by default None.
    targetcol : str, default = &#39;TARG_THICK_PER&#39;
        Name of column in points containing data to be interpolated, by default &#39;TARG_THICK_PER&#39;.
    lyrcol : str, default = &#39;Layer&#39;
        Name of column containing layer number. Not currently used, by default &#39;LAYER&#39;
    xcol : str, default = &#39;None&#39;
        Name of column containing x coordinates. If None, will look for &#39;geometry&#39; column, as in a geopandas.GeoDataframe. By default None
    ycol : str, default = &#39;None&#39;
        Name of column containing y coordinates. If None, will look for &#39;geometry&#39; column, as in a geopandas.GeoDataframe. By default None
    xcoord : str, default=&#39;x&#39;
        Name of x coordinate in grid, used to extract x values of grid, by default &#39;x&#39;
    ycoord : str, default=&#39;y&#39;
        Name of y coordinate in grid, used to extract x values of grid, by default &#39;y&#39;
    log : bool, default = True
        Whether to log inputs and outputs to log file.
    **kwargs
        Keyword arguments to be read directly into whichever scipy.interpolate function is designated by the method parameter.

    Returns
    -------
    interp_data : xr.DataArray or xr.Dataset, depending on return_type
        By default, returns an xr.DataArray object with the layers added as a new dimension called Layer. Can also specify return_type=&#39;dataset&#39; to return an xr.Dataset with each layer as a separate variable.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    nnList = [&#39;nearest&#39;, &#39;nearest neighbor&#39;, &#39;nearestneighbor&#39;,&#39;neighbor&#39;,  &#39;nn&#39;,&#39;n&#39;]
    splineList = [&#39;interp2d&#39;, &#39;interp2&#39;, &#39;interp&#39;, &#39;spline&#39;, &#39;spl&#39;, &#39;sp&#39;, &#39;s&#39;]
    linList = [&#39;linear&#39;, &#39;lin&#39;, &#39;l&#39;]
    ctList = [&#39;clough tocher&#39;, &#39;clough&#39;, &#39;cloughtocher&#39;, &#39;ct&#39;, &#39;c&#39;]
    rbfList = [&#39;rbf&#39;, &#39;radial basis&#39;, &#39;radial basis function&#39;, &#39;r&#39;, &#39;radial&#39;]
    #k-nearest neighbors from scikit-learn?
    #kriging? (from pykrige or maybe also from scikit-learn)
    
        
    X = np.round(grid[xcoord].values, 3)# #Extract xcoords from grid
    Y = np.round(grid[ycoord].values, 3)# #Extract ycoords from grid
    
    if layers is None and (type(points) is list or type(points) is dict):
        layers = len(points)

    if len(points) != layers:
        print(&#39;You have specified a different number of layers than what is iterable in the points argument. This may not work properly.&#39;)

    daDict = {}
    for lyr in range(1, layers+1):
        if type(points) is list or type(points) is dict:
            pts = points[lyr-1]
            dataX = pts
        else:
            pts = points

        if xcol is None:
            if &#39;geometry&#39; in pts.columns:
                dataX = pts[&#39;geometry&#39;].x
            else:
                print(&#39;xcol not specified and geometry column not detected (points is not/does not contain geopandas.GeoDataFrame)&#39; )
                return
        else:
            dataX = pts[xcol]
        
        if ycol is None:
            if &#39;geometry&#39; in pts.columns:
                dataY = pts[&#39;geometry&#39;].y
            else:
                print(&#39;ycol not specified and geometry column not detected (points is not/does not contain geopandas.GeoDataFrame)&#39; )
                return
        else:
            dataY = pts[ycol]

        #layer = pts[lyrcol]        
        interpVal = pts[targetcol]
        if method.lower() in nnList:
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            dataPoints = np.array(list(zip(dataX, dataY)))
            interp = interpolate.NearestNDInterpolator(dataPoints, interpVal, **kwargs)
            Z = interp(X, Y)
        elif method.lower() in linList:
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            interp = interpolate.LinearNDInterpolator(list(zip(dataX, dataY)), interpVal, **kwargs)
            Z = interp(X, Y)
        elif method.lower() in ctList:
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            if &#39;tol&#39; not in kwargs:
                kwargs[&#39;tol&#39;] = 1e10
            interp = interpolate.CloughTocher2DInterpolator(list(zip(dataX, dataY)), interpVal, **kwargs)
            Z = interp(X, Y) 
        elif method.lower() in rbfList:
            dataXY=  np.column_stack((dataX, dataY))
            interp = interpolate.RBFInterpolator(dataXY, interpVal, **kwargs)
            print(&#34;Radial Basis Function does not work well with many well-based datasets. Consider instead specifying &#39;nearest&#39;, &#39;linear&#39;, &#39;spline&#39;, or &#39;clough tocher&#39; for interpolation method.&#34;)
            Z = interp(np.column_stack((X.ravel(), Y.ravel()))).reshape(X.shape)
        elif method.lower() in splineList:
            Z = interpolate.bisplrep(dataX, dataY, interpVal, **kwargs)
                #interp = interpolate.interp2d(dataX, dataY, interpVal, kind=lin_kind, **kwargs)
                #Z = interp(X, Y)
        else:
            print(&#39;Specified interpolation method not recognized, using nearest neighbor.&#39;)
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            interp = interpolate.NearestNDInterpolator(list(zip(dataX, dataY)), interpVal, **kwargs)
            Z = interp(X, Y)

        #global ZTest
        #ZTest = Z

        interp_grid = xr.DataArray( #Create new datarray with new data values, but everything else the same
                    data=Z,
                    dims=grid.dims,
                    coords=grid.coords)
        
        if &#39;band&#39; in interp_grid.coords:
            interp_grid = interp_grid.drop_vars(&#39;band&#39;)
        interp_grid = interp_grid.clip(min=0, max=1, keep_attrs=True)

        interp_grid = interp_grid.expand_dims(dim=&#39;Layer&#39;)
        interp_grid = interp_grid.assign_coords(Layer=[lyr])

        del Z
        del dataX
        del dataY
        del interpVal
        del interp

        #interp_grid=interp_grid.interpolate_na(dim=x)
        zFillDigs = len(str(layers))
        daDict[&#39;Layer&#39;+str(lyr).zfill(zFillDigs)] = interp_grid
        del interp_grid
        print(&#39;Completed interpolation for Layer &#39;+str(lyr).zfill(zFillDigs))

    dataAList = [&#39;dataarray&#39;, &#39;da&#39;, &#39;a&#39;, &#39;array&#39;]
    dataSList = [&#39;dataset&#39;, &#39;ds&#39;, &#39;set&#39;]
    if return_type.lower() in dataAList:
        interp_data = xr.concat(daDict.values(), dim=&#39;Layer&#39;)
        interp_data = interp_data.assign_coords(Layer=np.arange(1,10))
    elif return_type.lower() in dataSList:
        interp_data = xr.Dataset(daDict)
        print(&#39;Done with interpolation, getting global attrs&#39;)
        common_attrs = {}
        for i, (var_name, data_array) in enumerate(interp_data.data_vars.items()):
            if i == 0:
                common_attrs = data_array.attrs
            else:
                common_attrs = {k: v for k, v in common_attrs.items() if k in data_array.attrs and data_array.attrs[k] == v}
        interp_data.attrs.update(common_attrs)
    else:
        print(&#34;{} is not a valid input for return_type. Please set return_type to either &#39;dataarray&#39; or &#39;dataset&#39;&#34;.format(return_type))
        return

    if export_dir is None:
        pass
    else:
        w4h.export_grids(grid_data=interp_data, out_path=export_dir, file_id=&#39;&#39;,filetype=&#39;tif&#39;, variable_sep=True, date_stamp=True)
        print(&#39;Exported to {}&#39;.format(export_dir))

    return interp_data</code></pre>
</details>
</dd>
<dt id="w4h.layer_target_thick"><code class="name flex">
<span>def <span class="ident">layer_target_thick</span></span>(<span>df, layers=9, return_all=False, export_dir=None, outfile_prefix='', depth_top_col='TOP', depth_bot_col='BOTTOM', log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to calculate thickness of target material in each layer at each well point</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>geopandas.geodataframe</code></dt>
<dd>Geodataframe containing classified data, surface elevation, bedrock elevation, layer depths, geometry.</dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>int</code>, default=<code>9</code></dt>
<dd>Number of layers in model, by default 9</dd>
<dt><strong><code>return_all</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>If True, return list of original geodataframes with extra column added for target thick for each layer.
If False, return list of geopandas.geodataframes with only essential information for each layer.</dd>
<dt><strong><code>export_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, default=<code>None</code></dt>
<dd>If str or pathlib.Path, should be directory to which to export dataframes built in function.</dd>
<dt><strong><code>outfile_prefix</code></strong> :&ensp;<code>str</code>, default=<code>''</code></dt>
<dd>Only used if export_dir is set. Will be used at the start of the exported filenames</dd>
<dt><strong><code>depth_top_col</code></strong> :&ensp;<code>str</code>, default=<code>'TOP'</code></dt>
<dd>Name of column containing data for depth to top of described well intervals</dd>
<dt><strong><code>depth_bot_col</code></strong> :&ensp;<code>str</code>, default=<code>'BOTTOM'</code></dt>
<dd>Name of column containing data for depth to bottom of described well intervals</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>res_df</code> or <code>res : geopandas.geodataframe</code></dt>
<dd>Geopandas geodataframe containing only important information needed for next stage of analysis.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def layer_target_thick(df, layers=9, return_all=False, export_dir=None, outfile_prefix=&#39;&#39;, depth_top_col=&#39;TOP&#39;, depth_bot_col=&#39;BOTTOM&#39;, log=False):
    &#34;&#34;&#34;Function to calculate thickness of target material in each layer at each well point

    Parameters
    ----------
    df : geopandas.geodataframe
        Geodataframe containing classified data, surface elevation, bedrock elevation, layer depths, geometry.
    layers : int, default=9
        Number of layers in model, by default 9
    return_all : bool, default=False
        If True, return list of original geodataframes with extra column added for target thick for each layer.
        If False, return list of geopandas.geodataframes with only essential information for each layer.
    export_dir : str or pathlib.Path, default=None
        If str or pathlib.Path, should be directory to which to export dataframes built in function.
    outfile_prefix : str, default=&#39;&#39;
        Only used if export_dir is set. Will be used at the start of the exported filenames
    depth_top_col : str, default=&#39;TOP&#39;
        Name of column containing data for depth to top of described well intervals
    depth_bot_col : str, default=&#39;BOTTOM&#39;
        Name of column containing data for depth to bottom of described well intervals
    log : bool, default = True
        Whether to log inputs and outputs to log file.
    
    Returns
    -------
    res_df or res : geopandas.geodataframe
        Geopandas geodataframe containing only important information needed for next stage of analysis.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    df[&#39;TOP_ELEV_FT&#39;] = df[&#39;SURFACE_ELEV_FT&#39;] - df[depth_top_col]
    df[&#39;BOT_ELEV_FT&#39;] = df[&#39;SURFACE_ELEV_FT&#39;] - df[depth_bot_col]

    layerList = range(1,layers+1)
    res_list = []
    resdf_list = []
    #Generate Column names based on (looped) integers
    for layer in layerList:
        zStr = &#39;ELEV&#39;
        zColT = &#39;TOP_ELEV_FT&#39;
        zColB = &#39;BOT_ELEV_FT&#39;
        topCol = zStr+&#39;_FT_LAYER&#39;+str(layer)
        if layer != 9: #For all layers except the bottom layer....
            botCol = zStr+&#39;_FT_LAYER&#39;+str(layer+1) #use the layer below it to 
        else: #Otherwise, ...
            botCol = &#34;BEDROCK_&#34;+zStr+&#34;_FT&#34; #Use the (corrected) bedrock depth

        #Divide records into 4 separate categories for ease of calculation, to be joined back together later  
            #Category 1: Well interval starts above layer top, ends within model layer
            #Category 2: Well interval is entirely contained withing model layer
            #Category 3: Well interval starts within model layer, continues through bottom of model layer
            #Category 4: well interval begins and ends on either side of model layer (model layer is contained within well layer)

        #records1 = intervals that go through the top of the layer and bottom is within layer
        records1 = df.loc[(df[zColT] &gt;= df[topCol]) &amp; #Top of the well is above or equal to the top of the layer
                        (df[zColB] &lt;= df[topCol]) &amp; # &amp; #Bottom is below the top of the layer
                        (df[zColB] &lt;= df[zColT])].copy() #Bottom is deeper than top (should already be the case)
        records1[&#39;TARG_THICK_FT&#39;] = pd.DataFrame(np.round((records1.loc[:,topCol]-records1.loc[: , zColB]) * records1[&#39;TARGET&#39;],3)).copy() #Multiply &#34;target&#34; (1 or 0) by length within layer            
        
        #records2 = entire interval is within layer
        records2 = df.loc[(df[zColT] &lt;= df[topCol]) &amp; #Top of the well is lower than top of the layer 
                    (df[zColB] &gt;= df[botCol]) &amp; #Bottom of the well is above bottom of the layer 
                    (df[zColB] &lt;= df[zColT])].copy() #Bottom ofthe well is deeper than or equal to top (should already be the case)
        records2[&#39;TARG_THICK_FT&#39;] = pd.DataFrame(np.round((records2.loc[: , zColT] - records2.loc[: , zColB]) * records2[&#39;TARGET&#39;],3)).copy()

        #records3 = intervals with top within layer and bottom of interval going through bottom of layer
        records3 = df.loc[(df[zColT] &gt; df[botCol]) &amp; #Top of the well is above bottom of layer
                    (df[zColB] &lt; df[botCol]) &amp; #Bottom of the well is below bottom of layer
                    (df[zColT] &lt;= df[topCol]) &amp; #Top of well is below top of layer
                    (df[zColB] &lt;= df[zColT])].copy() #Bottom is deeper than top (should already be the case)
        records3[&#39;TARG_THICK_FT&#39;] = pd.DataFrame(np.round((records3.loc[: , zColT] - (records3.loc[:,botCol]))*records3[&#39;TARGET&#39;],3)).copy()

        #records4 = interval goes through entire layer
        records4 = df.loc[(df[zColT] &gt;= df[topCol]) &amp; #Top of well is above top of layer
                    (df[zColB] &lt; df[botCol]) &amp; #Bottom of well is below bottom of layer
                    (df[zColB] &lt;= df[zColT])].copy() #Bottom of well is below top of well
        records4[&#39;TARG_THICK_FT&#39;] = pd.DataFrame(np.round((records4.loc[: , topCol]-records4.loc[: , botCol]) * records4[&#39;TARGET&#39;],3)).copy()
        
        #Put the four calculated record categories back together into single dataframe
        res = pd.concat([records1, records2, records3, records4])

        #The sign may be reversed if using depth rather than elevation
        if (res[&#39;TARG_THICK_FT&#39;] &lt; 0).all():
            res[&#39;TARG_THICK_FT&#39;] = res[&#39;TARG_THICK_FT&#39;] * -1
        
        #Cannot have negative thicknesses
        res[&#39;TARG_THICK_FT&#39;].clip(lower=0, inplace=True)
        res[&#39;LAYER_THICK_FT&#39;].clip(lower=0, inplace=True)
        
        #Get geometrys for each unique API/well
        res_df = res.groupby(by=[&#39;API_NUMBER&#39;,&#39;LATITUDE&#39;,&#39;LONGITUDE&#39;], as_index=False).sum(numeric_only=True)#Calculate thickness for each well interval in the layer indicated (e.g., if there are two well intervals from same well in one model layer)
        uniqInd = pd.DataFrame([v.values[0] for k, v in res.groupby(&#39;API_NUMBER&#39;).groups.items()]).loc[:,0]
        geomCol = res.loc[uniqInd, &#39;geometry&#39;]
        geomCol = pd.DataFrame(geomCol[~geomCol.index.duplicated(keep=&#39;first&#39;)]).reset_index()
        

        res_df[&#39;TARG_THICK_PER&#39;] =  pd.DataFrame(np.round(res_df[&#39;TARG_THICK_FT&#39;]/res_df[&#39;LAYER_THICK_FT&#39;],3)) #Calculate thickness as percent of total layer thickness
        res_df[&#39;TARG_THICK_PER&#39;] = res_df[&#39;TARG_THICK_PER&#39;].where(res_df[&#39;TARG_THICK_PER&#39;]!=np.inf, other=0) 

        res_df[&#34;LAYER&#34;] = layer #Just to have as part of the output file, include the present layer in the file itself as a separate column
        res_df = res_df[[&#39;API_NUMBER&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;LATITUDE_PROJ&#39;, &#39;LONGITUDE_PROJ&#39;,&#39;TOP&#39;, &#39;BOTTOM&#39;, &#39;TOP_ELEV_FT&#39;, &#39;BOT_ELEV_FT&#39;, &#39;SURFACE_ELEV_FT&#39;, topCol, botCol,&#39;LAYER_THICK_FT&#39;,&#39;TARG_THICK_FT&#39;, &#39;TARG_THICK_PER&#39;, &#39;LAYER&#39;]].copy() #Format dataframe for output
        res_df = gpd.GeoDataFrame(res_df, geometry=geomCol.loc[:,&#39;geometry&#39;])
        resdf_list.append(res_df)
        res_list.append(res)

        if isinstance(export_dir, pathlib.PurePath) or type(export_dir) is str:
            export_dir = pathlib.Path(export_dir)
            if export_dir.is_dir():
                pass
            else:
                try:
                    os.mkdir(export_dir)
                except:
                    print(&#39;Specified export directory does not exist and cannot be created. Function will continue run, but data will not be exported.&#39;)

            #Format and build export filepath
            export_dir = str(export_dir).replace(&#39;\\&#39;, &#39;/&#39;).replace(&#39;\\&#39;[0], &#39;/&#39;)
            zFillDigs = len(str(len(layerList)))
            if outfile_prefix[-1]==&#39;_&#39;:
                outfile_prefix = outfile_prefix[:-1]
            if export_dir[-1] ==&#39;/&#39;:
                export_dir = export_dir[:-1]
            nowStr = str(datetime.datetime.today().date())+&#39;_&#39;+str(datetime.datetime.today().hour)+&#39;-&#39;+str(datetime.datetime.today().minute)+&#39;-&#39;+str(datetime.datetime.today().second)
            outPath = export_dir+&#39;/&#39;+outfile_prefix+&#39;_Lyr&#39;+str(layer).zfill(zFillDigs)+&#39;_&#39;+nowStr+&#39;.csv&#39;

            if return_all:
                res.to_csv(outPath, sep=&#39;,&#39;, na_rep=np.nan, index_label=&#39;ID&#39;)
                res_df.to_csv(outPath, sep=&#39;,&#39;, na_rep=np.nan, index_label=&#39;ID&#39;)
            else:
                res_df.to_csv(outPath, sep=&#39;,&#39;, na_rep=np.nan, index_label=&#39;ID&#39;)

    if return_all:
        return res_list, resdf_list
    else:
        return resdf_list</code></pre>
</details>
</dd>
<dt id="w4h.logger_function"><code class="name flex">
<span>def <span class="ident">logger_function</span></span>(<span>logtocommence, parameters, func_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to log other functions, to be called from within other functions</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>logtocommence</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to perform logging steps</dd>
<dt><strong><code>parameters</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary containing parameters and their values, from function</dd>
<dt><strong><code>func_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of function within which this is called</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def logger_function(logtocommence, parameters, func_name):
    &#34;&#34;&#34;Function to log other functions, to be called from within other functions

    Parameters
    ----------
    logtocommence : bool
        Whether to perform logging steps
    parameters : dict
        Dictionary containing parameters and their values, from function
    func_name : str
        Name of function within which this is called
    &#34;&#34;&#34;
    if logtocommence:
        global log_filename
        #log parameter should be false by default on all. If true, will show up in kwargs
        
        #Get the log parameter value
        if &#39;log&#39; in parameters.keys():
            log_file = parameters.pop(&#39;log&#39;, None)
        else:
            #If it wasn&#39;t set, default to None
            log_file = None
        
        #Get currenet time and setup format for log messages
        curr_time = datetime.datetime.now()
        FORMAT = &#39;%(asctime)s  %(message)s&#39;

        #Check if we are starting a new logfile (only does this during run of file_setup() or (currently non-existent) new_logfile() functions)
        if log_file == True and (func_name == &#39;file_setup&#39; or func_name == &#39;new_logfile&#39;):

            #Get the log_dir variable set as a file_setup() parameter, or default to None if not specified
            out_dir = parameters.pop(&#39;log_dir&#39;, None)
            if out_dir is None:
                #If output directory not specified, default to the input directory
                out_dir = parameters[&#39;well_data&#39;]
            
            #Get the timestamp for the filename (this won&#39;t change, so represents the start of logging)
            timestamp = curr_time.strftime(&#39;%Y-%m-%d_%H-%M-%S&#39;)
            log_filename = pathlib.Path(out_dir).joinpath(f&#34;log_{timestamp}.txt&#34;)
            if &#39;verbose&#39; in parameters.keys():
                print(&#39;Logging data to&#39;, log_filename)

            #Set up logging stream using logging module
            logging.basicConfig(filename=log_filename, level=logging.INFO, format=FORMAT, filemode=&#39;w&#39;)

            #Log 
            logging.info(f&#34;{func_name} CALLED WITH PARAMETERS:\n\t {parameters}&#34;)
        elif log_file == True:
            #Run this for functions that aren&#39;t setting up logging file
            if log_filename:
                #Get the log stream and log this function&#39;s call with parameters
                logging.basicConfig(filename=log_filename, level=logging.INFO, format=FORMAT)
                logging.info(f&#34;{func_name} CALLED WITH PARAMETERS: \n\t{parameters}&#34;)
            else:
                #If log file has not already been set up, set it up
                timestamp = curr_time.strftime(&#39;%Y-%m-%d_%H-%M-%S&#39;)
                log_filename = f&#34;log_{timestamp}.txt&#34;

                #Now, get the log stream and log this function&#39;s call with parameters
                logging.basicConfig(filename=log_filename, level=logging.INFO, format=FORMAT)
                logging.info(f&#34;{func_name} CALLED WITH PARAMETERS: \n\t{parameters}&#34;)
        else:
            #Don&#39;t log if log=False
            pass
    return</code></pre>
</details>
</dd>
<dt id="w4h.merge_lithologies"><code class="name flex">
<span>def <span class="ident">merge_lithologies</span></span>(<span>df, targinterps_df, target_col='TARGET', target_class='bool')</span>
</code></dt>
<dd>
<div class="desc"><p>Function to merge lithologies and target booleans based on classifications</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing classified well data</dd>
<dt><strong><code>targinterps_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing lithologies and their target interpretations, depending on what the target is for this analysis (often, coarse materials=1, fine=0)</dd>
<dt><strong><code>target_col</code></strong> :&ensp;<code>str</code>, default <code>= 'TARGET'</code></dt>
<dd>Name of column in targinterps_df containing the target interpretations</dd>
</dl>
<p>target_class, default = 'bool'
Whether the input column is using boolean values as its target indicator</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df_targ</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing merged lithologies/targets</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_lithologies(df, targinterps_df, target_col=&#39;TARGET&#39;, target_class=&#39;bool&#39;):
    &#34;&#34;&#34;Function to merge lithologies and target booleans based on classifications
    
    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe containing classified well data
    targinterps_df : pandas.DataFrame
        Dataframe containing lithologies and their target interpretations, depending on what the target is for this analysis (often, coarse materials=1, fine=0)
    target_col : str, default = &#39;TARGET&#39;
        Name of column in targinterps_df containing the target interpretations
    target_class, default = &#39;bool&#39;
        Whether the input column is using boolean values as its target indicator
        
    Returns
    -------
    df_targ : pandas.DataFrame
        Dataframe containing merged lithologies/targets
    
    &#34;&#34;&#34;    
    
    #by default, use the boolean input 
    if target_class==&#39;bool&#39;:
        targinterps_df[target_col] = targinterps_df[target_col].where(targinterps_df[target_col]==&#39;1&#39;, other=&#39;0&#39;).astype(int)
        targinterps_df[target_col].fillna(value=0, inplace=True)
    else:
        targinterps_df[target_col].replace(&#39;DoNotUse&#39;, value=-1, inplace=True)
        targinterps_df[target_col].fillna(value=-2, inplace=True)
        targinterps_df[target_col].astype(np.int8)

    df_targ = pd.merge(df, targinterps_df.set_index(&#39;INTERPRETATION&#39;), right_on=&#39;INTERPRETATION&#39;,left_on=&#39;LITHOLOGY&#39;, how=&#39;left&#39;)
    
    return df_targ</code></pre>
</details>
</dd>
<dt id="w4h.merge_tables"><code class="name flex">
<span>def <span class="ident">merge_tables</span></span>(<span>data_df, header_df, data_cols=None, header_cols=None, auto_pick_cols=False, drop_duplicate_cols=True, log=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to merge tables, intended for merging metadata table with data table</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>"Left" dataframe, intended for this purpose to be dataframe with main data, but can be anything</dd>
<dt><strong><code>header_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>"Right" dataframe, intended for this purpose to be dataframe with metadata, but can be anything</dd>
<dt><strong><code>data_cols</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>List of strings of column names, for columns to be included after join from "left" table (data table). If None, all columns are kept, by default None</dd>
<dt><strong><code>header_cols</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>List of strings of columns names, for columns to be included in merged table after merge from "right" table (metadata). If None, all columns are kept, by default None</dd>
<dt><strong><code>auto_pick_cols</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to autopick the columns from the metadata table. If True, the following column names are kept:['API_NUMBER', 'LATITUDE', 'LONGITUDE', 'BEDROCK_ELEV_FT', 'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT'], by default False</dd>
<dt><strong><code>drop_duplicate_cols</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, drops duplicate columns from the tables so that columns do not get renamed upon merge, by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>kwargs that are passed directly to pd.merge(). By default, the 'on' and 'how' parameters are defined as on='API_NUMBER' and how='inner'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mergedTable</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Merged dataframe</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_tables(data_df, header_df, data_cols=None, header_cols=None, auto_pick_cols=False, drop_duplicate_cols=True, log=False, **kwargs):
    &#34;&#34;&#34;Function to merge tables, intended for merging metadata table with data table

    Parameters
    ----------
    data_df : pandas.DataFrame
        &#34;Left&#34; dataframe, intended for this purpose to be dataframe with main data, but can be anything
    header_df : pandas.DataFrame
        &#34;Right&#34; dataframe, intended for this purpose to be dataframe with metadata, but can be anything
    data_cols : list, optional
        List of strings of column names, for columns to be included after join from &#34;left&#34; table (data table). If None, all columns are kept, by default None
    header_cols : list, optional
        List of strings of columns names, for columns to be included in merged table after merge from &#34;right&#34; table (metadata). If None, all columns are kept, by default None
    auto_pick_cols : bool, default = False
        Whether to autopick the columns from the metadata table. If True, the following column names are kept:[&#39;API_NUMBER&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;BEDROCK_ELEV_FT&#39;, &#39;SURFACE_ELEV_FT&#39;, &#39;BEDROCK_DEPTH_FT&#39;, &#39;LAYER_THICK_FT&#39;], by default False
    drop_duplicate_cols : bool, optional
        If True, drops duplicate columns from the tables so that columns do not get renamed upon merge, by default True
    log : bool, default = False
        Whether to log inputs and outputs to log file.
    **kwargs
        kwargs that are passed directly to pd.merge(). By default, the &#39;on&#39; and &#39;how&#39; parameters are defined as on=&#39;API_NUMBER&#39; and how=&#39;inner&#39;

    Returns
    -------
    mergedTable : pandas.DataFrame
        Merged dataframe
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if auto_pick_cols:
        header_cols = [&#39;API_NUMBER&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;BEDROCK_ELEV_FT&#39;, &#39;SURFACE_ELEV_FT&#39;, &#39;BEDROCK_DEPTH_FT&#39;, &#39;LAYER_THICK_FT&#39;]
        for c in header_df.columns:
            if c.startswith(&#39;ELEV_&#39;) or c.startswith(&#39;DEPTH&#39;):
                header_cols.append(c)
            if &#39;_PROJ&#39; in c:
                header_cols.append(c)
        header_cols.append(&#39;geometry&#39;)
    elif header_cols is None:
        header_cols = header_df.columns
    else:
        header_cols = header_cols

    #If not specified, get all the cols
    if data_cols is None:
        data_cols = data_df.columns

    #Defults for on and how
    if &#39;on&#39; not in kwargs.keys():
        kwargs[&#39;on&#39;]=&#39;API_NUMBER&#39;

    if &#39;how&#39; not in kwargs.keys():
        kwargs[&#39;how&#39;]=&#39;inner&#39;

    #Drop duplicate columns
    if drop_duplicate_cols:
        header_colCopy= header_cols.copy()
        remCount = 0
        for i, c in enumerate(header_colCopy):
            if c in data_cols and c != kwargs[&#39;on&#39;]:
                print(&#39;REMOVING&#39;, header_cols[i-remCount])
                header_cols.pop(i - remCount)
                remCount += 1

    leftTable_join = data_df[data_cols]
    rightTable_join = header_df[header_cols]

    mergedTable = pd.merge(left=leftTable_join, right=rightTable_join, **kwargs)
    return mergedTable</code></pre>
</details>
</dd>
<dt id="w4h.read_dict"><code class="name flex">
<span>def <span class="ident">read_dict</span></span>(<span>file, keytype='np')</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read a text file with a dictionary in it into a python dictionary</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>Filepath to the file of interest containing the dictionary text</dd>
<dt><strong><code>keytype</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>String indicating the datatypes used in the text, currently only 'np' is implemented, by default 'np'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Dictionary translated from text file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_dict(file, keytype=&#39;np&#39;):
    &#34;&#34;&#34;Function to read a text file with a dictionary in it into a python dictionary

    Parameters
    ----------
    file : str or pathlib.Path object
        Filepath to the file of interest containing the dictionary text
    keytype : str, optional
        String indicating the datatypes used in the text, currently only &#39;np&#39; is implemented, by default &#39;np&#39;

    Returns
    -------
    dict
        Dictionary translated from text file.
    &#34;&#34;&#34;

    with open(file, &#39;r&#39;) as f:
        data= f.read()

    jsDict = json.loads(data)
    if keytype==&#39;np&#39;:
        for k in jsDict.keys():
            jsDict[k] = getattr(np, jsDict[k])
    
    return jsDict</code></pre>
</details>
</dd>
<dt id="w4h.read_dictionary_terms"><code class="name flex">
<span>def <span class="ident">read_dictionary_terms</span></span>(<span>dict_file, id_col='ID', search_col='DESCRIPTION', definition_col='LITHOLOGY', class_flag_col='CLASS_FLAG', dictionary_type=None, class_flag=6, rem_extra_cols=True, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read dictionary terms from file into pandas dataframe</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dict_file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>list</code> of <code>these</code></dt>
<dd>File or list of files to be read</dd>
<dt><strong><code>search_col</code></strong> :&ensp;<code>str</code>, default <code>= 'DESCRIPTION'</code></dt>
<dd>Name of column containing search terms (geologic formations)</dd>
<dt><strong><code>definition_col</code></strong> :&ensp;<code>str</code>, default <code>= 'LITHOLOGY'</code></dt>
<dd>Name of column containing interpretations of search terms (lithologies)</dd>
<dt><strong><code>dictionary_type</code></strong> :&ensp;<code>str</code> or <code>None, {None, 'exact', 'start', 'wildcard',}</code></dt>
<dd>Indicator of which kind of dictionary terms to be read in: None, 'exact', 'start', or 'wildcard' by default None.
- If None, uses name of file to try to determine. If it cannot, it will default to using the classification flag from class_flag
- If 'exact', will be used to search for exact matches to geologic descriptions
- If 'start', will be used as with the .startswith() string method to find inexact matches to geologic descriptions
- If 'wildcard', will be used to find any matching substring for inexact geologic matches</dd>
<dt><strong><code>class_flag</code></strong> :&ensp;<code>int</code>, default <code>= 1</code></dt>
<dd>Classification flag to be used if dictionary_type is None and cannot be otherwise determined, by default 1</dd>
<dt><strong><code>rem_extra_cols</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to remove the extra columns from the input file after it is read in as a pandas dataframe, by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict_terms</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with formatting ready to be used in the classification steps of this package</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_dictionary_terms(dict_file, id_col=&#39;ID&#39;, search_col=&#39;DESCRIPTION&#39;, definition_col=&#39;LITHOLOGY&#39;, class_flag_col=&#39;CLASS_FLAG&#39;, dictionary_type=None, class_flag=6, rem_extra_cols=True, log=False):
    &#34;&#34;&#34;Function to read dictionary terms from file into pandas dataframe

    Parameters
    ----------
    dict_file : str or pathlib.Path object, or list of these
        File or list of files to be read
    search_col : str, default = &#39;DESCRIPTION&#39;
        Name of column containing search terms (geologic formations)
    definition_col : str, default = &#39;LITHOLOGY&#39;
        Name of column containing interpretations of search terms (lithologies)
    dictionary_type : str or None, {None, &#39;exact&#39;, &#39;start&#39;, &#39;wildcard&#39;,}
        Indicator of which kind of dictionary terms to be read in: None, &#39;exact&#39;, &#39;start&#39;, or &#39;wildcard&#39; by default None.
            - If None, uses name of file to try to determine. If it cannot, it will default to using the classification flag from class_flag
            - If &#39;exact&#39;, will be used to search for exact matches to geologic descriptions
            - If &#39;start&#39;, will be used as with the .startswith() string method to find inexact matches to geologic descriptions
            - If &#39;wildcard&#39;, will be used to find any matching substring for inexact geologic matches
    class_flag : int, default = 1
        Classification flag to be used if dictionary_type is None and cannot be otherwise determined, by default 1
    rem_extra_cols : bool, default = True
        Whether to remove the extra columns from the input file after it is read in as a pandas dataframe, by default True
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    dict_terms : pandas.DataFrame
        Pandas dataframe with formatting ready to be used in the classification steps of this package
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    #Read files into pandas dataframes
    dict_terms = []
    if dict_file is None:
        df = pd.DataFrame(columns=[&#39;ID&#39;, &#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#39;CLASS_FLAGS&#39;])
        dict_terms.append(df)
        dict_file = [&#39;&#39;]
    elif type(dict_file) is list:
        for f in dict_file:
            if not f.exists():
                df = pd.DataFrame(columns=[&#39;ID&#39;, &#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#39;CLASS_FLAGS&#39;])
                dict_terms.append(df)
            else:
                dict_terms.append(pd.read_csv(f))

            if id_col in dict_terms.columns:
                dict_terms.set_index(id_col, drop=True, inplace=True)
    else:
        dict_file = pathlib.Path(dict_file)
        if dict_file.exists() and dict_file.is_file():
            dict_terms.append(pd.read_csv(dict_file, low_memory=False))
            if id_col in dict_terms[-1].columns:
                dict_terms[-1].set_index(id_col, drop=True, inplace=True)
            dict_file = [dict_file]
        else:
            print(&#39;ERROR: dict_file ({}) does not exist.&#39;.format(dict_file))
            #Create empty dataframe to return
            dict_terms = pd.DataFrame(columns=[&#39;ID&#39;, &#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#34;CLASS_FLAGS&#34;])
            return dict_terms

    #Rename important columns
    searchTermList = [&#39;searchterm&#39;, &#39;search&#39;, &#39;exact&#39;]
    startTermList = [&#39;startterm&#39;, &#39;start&#39;, &#39;startswith&#39;]
    wildcardTermList = [&#39;wildcard&#39;, &#39;substring&#39;, ]

    #Recast all columns to datatypes of headerData to defined types
    dict_termDtypes = {search_col:str, definition_col:str, class_flag_col:np.uint8}

    if dictionary_type is None:
        dictionary_type=&#39;&#39; #Allow string methods on this variable

    #Iterating, to allow reading of multiple dict file at once (also works with just one at at time)
    for i, d in enumerate(dict_terms):
        if dictionary_type.lower() in searchTermList or (dictionary_type==&#39;&#39; and &#39;spec&#39; in str(dict_file[i]).lower()):
            d[class_flag_col] = 1
        elif dictionary_type.lower() in startTermList or (dictionary_type==&#39;&#39; and &#39;start&#39; in str(dict_file[i]).lower()):
            d[class_flag_col] = 4 #Start term classification flag
        elif dictionary_type.lower() in wildcardTermList or (dictionary_type==&#39;&#39; and &#39;wildcard&#39; in str(dict_file[i]).lower()):
            d[class_flag_col] = 5 #Wildcard term classification flag
        else:
            d[class_flag_col] = class_flag #Custom classification flag, defined as argument
        #1: exact classification match, 2: (not defined...ML?), 3: bedrock classification for obvious bedrock, 4: start term, 5: wildcard/substring, 6: Undefined

        #Rename columns so it is consistent through rest of code
        if search_col != &#39;DESCRIPTION&#39;:
            d.rename(columns={search_col:&#39;DESCRIPTION&#39;}, inplace=True)
        if definition_col != &#39;LITHOLOGY&#39;:
            d.rename(columns={definition_col:&#39;LITHOLOGY&#39;}, inplace=True)
        if class_flag_col != &#39;CLASS_FLAG&#39;:
            d.rename(columns={class_flag_col:&#39;CLASS_FLAG&#39;}, inplace=True)

        #Cast all columns as type str, if not already
        for i in range(0, np.shape(d)[1]):
            if d.iloc[:,i].name in list(dict_termDtypes.keys()):
                d.iloc[:,i] = d.iloc[:,i].astype(dict_termDtypes[d.iloc[:,i].name])
        
        #Delete duplicate definitions
        d.drop_duplicates(subset=&#39;DESCRIPTION&#39;,inplace=True) #Apparently, there are some duplicate definitions, which need to be deleted first
        d.reset_index(inplace=True, drop=True)

        #Whether to remove extra columns that aren&#39;t needed from dataframe
        if rem_extra_cols:
            d = d[[&#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#39;CLASS_FLAG&#39;]]

    #If only one file designated, convert it so it&#39;s no longer a list, but just a dataframe

    if len(dict_terms) == 1:
        dict_terms = dict_terms[0]

    return dict_terms</code></pre>
</details>
</dd>
<dt id="w4h.read_grid"><code class="name flex">
<span>def <span class="ident">read_grid</span></span>(<span>grid_path=None, grid_type='model', no_data_val_grid=0, use_service=False, study_area=None, study_area_crs=None, grid_crs=None, verbose=False, log=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads in grid</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>grid_path</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, default=<code>None</code></dt>
<dd>Path to a grid file</dd>
<dt><strong><code>grid_type</code></strong> :&ensp;<code>str</code>, default=<code>'model'</code></dt>
<dd>Sets what type of grid to load in</dd>
<dt><strong><code>no_data_val_grid</code></strong> :&ensp;<code>int</code>, default=<code>0</code></dt>
<dd>Sets the no data value of the grid</dd>
<dt><strong><code>use_service</code></strong> :&ensp;<code>str</code>, default=<code>False</code></dt>
<dd>Sets which service the function uses</dd>
<dt><strong><code>study_area</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code>, default=<code>None</code></dt>
<dd>Dataframe containing study area polygon</dd>
<dt><strong><code>study_area_crs</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>Sets specific crs if current crs is not wanted</dd>
<dt><strong><code>grid_crs</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>Sets crs to use if clipping to study area</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>gridIN</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>Returns grid</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_grid(grid_path=None, grid_type=&#39;model&#39;, no_data_val_grid=0, use_service=False, study_area=None, study_area_crs=None, grid_crs=None, verbose=False, log=False, **kwargs):
    &#34;&#34;&#34;Reads in grid

    Parameters
    ----------
    grid_path : str or pathlib.Path, default=None
        Path to a grid file
    grid_type : str, default=&#39;model&#39;
        Sets what type of grid to load in
    no_data_val_grid : int, default=0
        Sets the no data value of the grid
    use_service : str, default=False
        Sets which service the function uses
    study_area : geopandas.GeoDataFrame, default=None
        Dataframe containing study area polygon
    study_area_crs : str, default=None
        Sets specific crs if current crs is not wanted
    grid_crs : str, default=None
        Sets crs to use if clipping to study area
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    gridIN : xarray.DataArray
        Returns grid
    
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if grid_type==&#39;model&#39;:
        if &#39;read_grid&#39; in list(kwargs.keys()):
            rgrid = kwargs[&#39;read_grid&#39;]
        else:
            rgrid=True
        gridIN = read_model_grid(model_grid_path=grid_path, study_area=study_area,  no_data_val_grid=0, read_grid=rgrid, study_area_crs=study_area_crs, grid_crs=grid_crs, verbose=verbose)
    else:
        if use_service==False:
            gridIN = rxr.open_rasterio(grid_path)
        elif use_service.lower()==&#39;wcs&#39;:
            gridIN = read_wcs(study_area, wcs_url=lidarURL, **kwargs)
        elif use_service.lower()==&#39;wms&#39;:
            pass
            gridIN = read_wms(study_area, wcs_url=lidarURL, **kwargs)
            
        if study_area is not None:
            if grid_crs is None:
                try:
                    grid_crs=gridIN.spatial_ref.crs_wkt
                except:
                    iswsCRS = w4h.read_dict(r&#39;../resources/isws_crs&#39;)
                    gridIN.rio.write_crs(iswsCRS)
            elif grid_crs.lower()==&#39;isws&#39;:
                iswsCRS = w4h.read_dict(r&#39;../resources/isws_crs&#39;)
                gridIN.rio.write_crs(iswsCRS)
                
            if study_area_crs is None:
                study_area_crs=study_area.crs
            study_area = study_area.to_crs(grid_crs)
            study_area_crs=study_area.crs
            
            gridIN = grid2study_area(study_area=study_area, grid=gridIN, study_area_crs=study_area_crs, grid_crs=grid_crs)

        try:
            no_data_val_grid = gridIN.attrs[&#39;_FillValue&#39;] #Extract from dataset itself
        except:
            pass
                
        gridIN = gridIN.where(gridIN != no_data_val_grid, other=np.nan)  #Replace no data values with NaNs

    return gridIN</code></pre>
</details>
</dd>
<dt id="w4h.read_lithologies"><code class="name flex">
<span>def <span class="ident">read_lithologies</span></span>(<span>lith_file=None, interp_col='LITHOLOGY', target_col='CODE', use_cols=None, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read lithology file into pandas dataframe</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lith_file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, default <code>= None</code></dt>
<dd>Filename of lithology file. If None, default is contained within repository, by default None</dd>
<dt><strong><code>interp_col</code></strong> :&ensp;<code>str</code>, default <code>= 'LITHOLOGY'</code></dt>
<dd>Column to used to match interpretations</dd>
<dt><strong><code>target_col</code></strong> :&ensp;<code>str</code>, default <code>= 'CODE'</code></dt>
<dd>Column to be used as target code</dd>
<dt><strong><code>use_cols</code></strong> :&ensp;<code>list</code>, default <code>= None</code></dt>
<dd>Which columns to use when reading in dataframe. If None, defaults to ['LITHOLOGY', 'CODE'].</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with lithology information</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_lithologies(lith_file=None, interp_col=&#39;LITHOLOGY&#39;, target_col=&#39;CODE&#39;, use_cols=None, log=False):
    &#34;&#34;&#34;Function to read lithology file into pandas dataframe

    Parameters
    ----------
    lith_file : str or pathlib.Path object, default = None
        Filename of lithology file. If None, default is contained within repository, by default None
    interp_col : str, default = &#39;LITHOLOGY&#39;
        Column to used to match interpretations
    target_col : str, default = &#39;CODE&#39;
        Column to be used as target code
    use_cols : list, default = None
        Which columns to use when reading in dataframe. If None, defaults to [&#39;LITHOLOGY&#39;, &#39;CODE&#39;].
    log : bool, default = True
        Whether to log inputs and outputs to log file.

    Returns
    -------
    pandas.DataFrame
        Pandas dataframe with lithology information
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if lith_file is None:
        #Find resources
        lith_file=&#39;Lithology_Interp_FineCoarse.csv&#39;
    
    if not isinstance(lith_file, pathlib.PurePath):
        lith_file = pathlib.Path(lith_file)

    if use_cols is None:
        use_cols = [&#39;LITHOLOGY&#39;, &#39;CODE&#39;]

    lithoDF = pd.read_csv(lith_file, usecols=use_cols)

    lithoDF.rename(columns={interp_col:&#39;INTERPRETATION&#39;, target_col:&#39;TARGET&#39;}, inplace=True)

    return lithoDF</code></pre>
</details>
</dd>
<dt id="w4h.read_model_grid"><code class="name flex">
<span>def <span class="ident">read_model_grid</span></span>(<span>model_grid_path, study_area=None, no_data_val_grid=0, read_grid=True, node_byspace=True, study_area_crs=None, grid_crs=None, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads in model grid to xarray data array</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>grid_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to model grid file</dd>
<dt><strong><code>study_area</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code>, default=<code>None</code></dt>
<dd>Dataframe containing study area polygon</dd>
<dt><strong><code>no_data_val_grid</code></strong> :&ensp;<code>int</code>, default=<code>0</code></dt>
<dd>value assigned to areas with no data</dd>
<dt><strong><code>readGrid</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether function to either read grid or create grid</dd>
<dt><strong><code>node_byspace</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Denotes how to create grid</dd>
<dt><strong><code>study_area_crs</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>Inputs study area crs</dd>
<dt><strong><code>grid_crs</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>Inputs grid crs</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>modelGrid</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>Data array containing model grid</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_model_grid(model_grid_path, study_area=None, no_data_val_grid=0, read_grid=True, node_byspace=True, study_area_crs=None, grid_crs=None, verbose=False, log=False):
    &#34;&#34;&#34;Reads in model grid to xarray data array

    Parameters
    ----------
    grid_path : str
        Path to model grid file
    study_area : geopandas.GeoDataFrame, default=None
        Dataframe containing study area polygon
    no_data_val_grid : int, default=0
        value assigned to areas with no data
    readGrid : bool, default=True
        Whether function to either read grid or create grid
    node_byspace : bool, default=False
        Denotes how to create grid
    study_area_crs : str, default=None
        Inputs study area crs
    grid_crs : str, default=None
        Inputs grid crs
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    modelGrid : xarray.DataArray
        Data array containing model grid
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    
    if read_grid and model_grid_path is not None:
        modelGridIN = rxr.open_rasterio(model_grid_path)

        file = w4h.read.__get_resource_path(&#39;isws_crs.txt&#39;)
        iswsCRS = w4h.read_dict(file, keytype=None)

        if grid_crs is None:
            try:
                grid_crs=modelGridIN.spatial_ref.crs_wkt
            except:
                modelGridIN.rio.write_crs(iswsCRS)
        elif grid_crs.lower()==&#39;isws&#39;:
            modelGridIN.rio.write_crs(iswsCRS)
        
        if study_area is not None:                
            if study_area_crs is None:
                study_area_crs=study_area.crs
            study_area = study_area.to_crs(grid_crs)
            study_area_crs=study_area.crs            
            modelGrid = grid2study_area(study_area=study_area, grid=modelGridIN, study_area_crs=study_area_crs, grid_crs=grid_crs)
        else:
            modelGrid = modelGridIN

        try:
            noDataVal = float(modelGrid.attrs[&#39;_FillValue&#39;]) #Extract from dataset itsel
        except:
            noDataVal = -5000000

        modelGrid = modelGrid.where(modelGrid != noDataVal, other=np.nan)   #Replace no data values with NaNs
        modelGrid.rio.reproject(iswsCRS, inplace=True)
    elif model_grid_path is None and study_area is None:
        if verbose:
            print(&#34;ERROR: Either model_grid_path or study_area must be defined.&#34;)
    else:
        spatRefDict = {&#39;crs_wkt&#39;: &#39;PROJCS[&#34;Clarke_1866_Lambert_Conformal_Conic&#34;,GEOGCS[&#34;NAD27&#34;,DATUM[&#34;North_American_Datum_1927&#34;,SPHEROID[&#34;Clarke 1866&#34;,6378206.4,294.978698199999,AUTHORITY[&#34;EPSG&#34;,&#34;7008&#34;]],AUTHORITY[&#34;EPSG&#34;,&#34;6267&#34;]],PRIMEM[&#34;Greenwich&#34;,0],UNIT[&#34;degree&#34;,0.0174532925199433,AUTHORITY[&#34;EPSG&#34;,&#34;9122&#34;]],AUTHORITY[&#34;EPSG&#34;,&#34;4267&#34;]],PROJECTION[&#34;Lambert_Conformal_Conic_2SP&#34;],PARAMETER[&#34;latitude_of_origin&#34;,33],PARAMETER[&#34;central_meridian&#34;,-89.5],PARAMETER[&#34;standard_parallel_1&#34;,33],PARAMETER[&#34;standard_parallel_2&#34;,45],PARAMETER[&#34;false_easting&#34;,2999994],PARAMETER[&#34;false_northing&#34;,0],UNIT[&#34;US survey foot&#34;,0.304800609601219,AUTHORITY[&#34;EPSG&#34;,&#34;9003&#34;]],AXIS[&#34;Easting&#34;,EAST],AXIS[&#34;Northing&#34;,NORTH]]&#39;,
            &#39;semi_major_axis&#39;: 6378206.4,
            &#39;semi_minor_axis&#39;: 6356583.799998981,
            &#39;inverse_flattening&#39;: 294.978698199999,
            &#39;reference_ellipsoid_name&#39;: &#39;Clarke 1866&#39;,
            &#39;longitude_of_prime_meridian&#39;: 0.0,
            &#39;prime_meridian_name&#39;: &#39;Greenwich&#39;,
            &#39;geographic_crs_name&#39;: &#39;NAD27&#39;,
            &#39;horizontal_datum_name&#39;: &#39;North American Datum 1927&#39;,
            &#39;projected_crs_name&#39;: &#39;Clarke_1866_Lambert_Conformal_Conic&#39;,
            &#39;grid_mapping_name&#39;: &#39;lambert_conformal_conic&#39;,
            &#39;standard_parallel&#39;: (33.0, 45.0),
            &#39;latitude_of_projection_origin&#39;: 33.0,
            &#39;longitude_of_central_meridian&#39;: -89.5,
            &#39;false_easting&#39;: 2999994.0,
            &#39;false_northing&#39;: 0.0,
            &#39;spatial_ref&#39;: &#39;PROJCS[&#34;Clarke_1866_Lambert_Conformal_Conic&#34;,GEOGCS[&#34;NAD27&#34;,DATUM[&#34;North_American_Datum_1927&#34;,SPHEROID[&#34;Clarke 1866&#34;,6378206.4,294.978698199999,AUTHORITY[&#34;EPSG&#34;,&#34;7008&#34;]],AUTHORITY[&#34;EPSG&#34;,&#34;6267&#34;]],PRIMEM[&#34;Greenwich&#34;,0],UNIT[&#34;degree&#34;,0.0174532925199433,AUTHORITY[&#34;EPSG&#34;,&#34;9122&#34;]],AUTHORITY[&#34;EPSG&#34;,&#34;4267&#34;]],PROJECTION[&#34;Lambert_Conformal_Conic_2SP&#34;],PARAMETER[&#34;latitude_of_origin&#34;,33],PARAMETER[&#34;central_meridian&#34;,-89.5],PARAMETER[&#34;standard_parallel_1&#34;,33],PARAMETER[&#34;standard_parallel_2&#34;,45],PARAMETER[&#34;false_easting&#34;,2999994],PARAMETER[&#34;false_northing&#34;,0],UNIT[&#34;US survey foot&#34;,0.304800609601219,AUTHORITY[&#34;EPSG&#34;,&#34;9003&#34;]],AXIS[&#34;Easting&#34;,EAST],AXIS[&#34;Northing&#34;,NORTH]]&#39;,
            &#39;GeoTransform&#39;: &#39;2440250.0 625.0 0.0 3459750.0 0.0 -625.0&#39;}
        
        saExtent = study_area.total_bounds

        startX = saExtent[0] #Starting X Coordinate
        startY = saExtent[1] #starting Y Coordinate
        
        endX = saExtent[2]
        endY = saExtent[3]
        
        if node_byspace:
            xSpacing = 625 #X Node spacing 
            ySpacing = xSpacing #Y Node spacing  
            
            x = np.arange(startX, endX, xSpacing)
            y = np.arange(startY, endY, ySpacing)
        else:
            xNodes = 100 #Number of X Nodes
            yNodes = 100 #Number of Y Nodes

            x = np.linspace(startX, endX, num=xNodes)
            y = np.linspace(startY, endY, num=yNodes)        
        
        xx, yy = np.meshgrid(x, y)
        zz = np.ones_like(xx).transpose()

        yIn = np.flipud(y)

        coords = {&#39;x&#39;:x,&#39;y&#39;:yIn, &#39;spatial_ref&#39;:0}
        dims = {&#39;x&#39;:x,&#39;y&#39;:yIn}
        
        modelGrid = xr.DataArray(data=zz,coords=coords,attrs={&#39;_FillValue&#39;:3.402823466e+38}, dims=dims)
        modelGrid.spatial_ref.attrs[&#39;spatial_ref&#39;] = {}
        if grid_crs is None or grid_crs==&#39;isws&#39; or grid_crs==&#39;ISWS&#39;:
            for k in spatRefDict:
                modelGrid.spatial_ref.attrs[k] = spatRefDict[k]
    return modelGrid</code></pre>
</details>
</dd>
<dt id="w4h.read_raw_csv"><code class="name flex">
<span>def <span class="ident">read_raw_csv</span></span>(<span>data_filepath, metadata_filepath, data_cols=None, metadata_cols=None, xcol='LONGITUDE', ycol='LATITUDE', well_key='API_NUMBER', encoding='latin-1', verbose=False, log=False, **read_csv_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Easy function to read raw .txt files output from (for example), an Access database</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the file containing data, including the extension.</dd>
<dt><strong><code>metadata_filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the file containing metadata, including the extension.</dd>
<dt><strong><code>data_cols</code></strong> :&ensp;<code>list</code>, default <code>= None</code></dt>
<dd>List with strings with names of columns from txt file to keep after reading. If None, ["API_NUMBER","TABLE_NAME","FORMATION","THICKNESS","TOP","BOTTOM"], by default None.</dd>
<dt><strong><code>metadata_cols</code></strong> :&ensp;<code>list</code>, default <code>= None</code></dt>
<dd>List with strings with names of columns from txt file to keep after reading. If None, ['API_NUMBER',"TOTAL_DEPTH","SECTION","TWP","TDIR","RNG","RDIR","MERIDIAN","QUARTERS","ELEVATION","ELEVREF","COUNTY_CODE","LATITUDE","LONGITUDE","ELEVSOURCE"], by default None</dd>
<dt><strong><code>x_col</code></strong> :&ensp;<code>str</code>, default <code>= 'LONGITUDE'</code></dt>
<dd>Name of column in metadata file indicating the x-location of the well, by default 'LONGITUDE'</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default <code>= 'LATITUDE'</code></dt>
<dd>Name of the column in metadata file indicating the y-location of the well, by default 'LATITUDE'</dd>
<dt><strong><code>well_key</code></strong> :&ensp;<code>str</code>, default <code>= 'API_NUMBER'</code></dt>
<dd>Name of the column with the key/identifier that will be used to merge data later, by default 'API_NUMBER'</dd>
<dt><strong><code>encoding</code></strong> :&ensp;<code>str</code>, default <code>= 'latin-1'</code></dt>
<dd>Encoding of the data in the input files, by default 'latin-1'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print the number of rows in the input columns, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
<dt><strong><code>**read_csv_kwargs</code></strong></dt>
<dd>**kwargs that get passed to pd.read_csv()</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(pandas.DataFrame, pandas.DataFrame)
Tuple/list with two pandas dataframes: (data, metadata)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_raw_csv(data_filepath, metadata_filepath, data_cols=None, metadata_cols=None, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, well_key=&#39;API_NUMBER&#39;, encoding=&#39;latin-1&#39;, verbose=False, log=False, **read_csv_kwargs):
    &#34;&#34;&#34;Easy function to read raw .txt files output from (for example), an Access database

    Parameters
    ----------
    data_filepath : str
        Filename of the file containing data, including the extension.
    metadata_filepath : str
        Filename of the file containing metadata, including the extension.
    data_cols : list, default = None
        List with strings with names of columns from txt file to keep after reading. If None, [&#34;API_NUMBER&#34;,&#34;TABLE_NAME&#34;,&#34;FORMATION&#34;,&#34;THICKNESS&#34;,&#34;TOP&#34;,&#34;BOTTOM&#34;], by default None.
    metadata_cols : list, default = None
        List with strings with names of columns from txt file to keep after reading. If None, [&#39;API_NUMBER&#39;,&#34;TOTAL_DEPTH&#34;,&#34;SECTION&#34;,&#34;TWP&#34;,&#34;TDIR&#34;,&#34;RNG&#34;,&#34;RDIR&#34;,&#34;MERIDIAN&#34;,&#34;QUARTERS&#34;,&#34;ELEVATION&#34;,&#34;ELEVREF&#34;,&#34;COUNTY_CODE&#34;,&#34;LATITUDE&#34;,&#34;LONGITUDE&#34;,&#34;ELEVSOURCE&#34;], by default None
    x_col : str, default = &#39;LONGITUDE&#39;
        Name of column in metadata file indicating the x-location of the well, by default &#39;LONGITUDE&#39;
    ycol : str, default = &#39;LATITUDE&#39;
        Name of the column in metadata file indicating the y-location of the well, by default &#39;LATITUDE&#39;
    well_key : str, default = &#39;API_NUMBER&#39;
        Name of the column with the key/identifier that will be used to merge data later, by default &#39;API_NUMBER&#39;
    encoding : str, default = &#39;latin-1&#39;
        Encoding of the data in the input files, by default &#39;latin-1&#39;
    verbose : bool, default = False
        Whether to print the number of rows in the input columns, by default False
    log : bool, default = False
        Whether to log inputs and outputs to log file.
    **read_csv_kwargs
        **kwargs that get passed to pd.read_csv()

    Returns
    -------
    (pandas.DataFrame, pandas.DataFrame)
        Tuple/list with two pandas dataframes: (data, metadata)
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if data_cols is None:
        data_useCols = [&#34;API_NUMBER&#34;,&#34;TABLE_NAME&#34;,&#34;FORMATION&#34;,&#34;THICKNESS&#34;,&#34;TOP&#34;,&#34;BOTTOM&#34;]
    else:
        data_useCols= data_cols

    if metadata_cols is None:
        metadata_useCols = [&#39;API_NUMBER&#39;,&#34;TOTAL_DEPTH&#34;,&#34;SECTION&#34;,&#34;TWP&#34;,&#34;TDIR&#34;,&#34;RNG&#34;,&#34;RDIR&#34;,&#34;MERIDIAN&#34;,&#34;QUARTERS&#34;,&#34;ELEVATION&#34;,&#34;ELEVREF&#34;,&#34;COUNTY_CODE&#34;,&#34;LATITUDE&#34;,&#34;LONGITUDE&#34;,&#34;ELEVSOURCE&#34;]
    else:
        metadata_useCols= metadata_cols

    #Check if input data is already 
    if isinstance(data_filepath, pd.DataFrame):
        downholeDataIN = data_filepath[data_useCols]
    else:
        downholeDataIN = pd.read_csv(data_filepath, sep=&#39;,&#39;, header=&#39;infer&#39;, encoding=encoding, usecols=data_useCols, **read_csv_kwargs)
    
    if isinstance(metadata_filepath, pd.DataFrame):
        headerDataIN = metadata_filepath[metadata_useCols]
    else:
        headerDataIN = pd.read_csv(metadata_filepath, sep=&#39;,&#39;, header=&#39;infer&#39;, encoding=encoding, usecols=metadata_useCols, **read_csv_kwargs)

    #Drop data with no API        
    downholeDataIN = downholeDataIN.dropna(subset=[well_key]) #Drop data with no API
    headerDataIN = headerDataIN.dropna(subset=[well_key]) #Drop metadata with no API

    #Drop data with no or missing location information
    headerDataIN = headerDataIN.dropna(subset=[ycol]) 
    headerDataIN = headerDataIN.dropna(subset=[xcol])
    
    #Reset index so index goes from 0 in numerical/integer order
    headerDataIN.reset_index(inplace=True, drop=True)
    downholeDataIN.reset_index(inplace=True, drop=True)
    
    #Print outputs to terminal, if designated
    if verbose:
        print(&#39;Data file has &#39; + str(downholeDataIN.shape[0])+&#39; valid well records.&#39;)
        print(&#34;Metadata file has &#34;+str(headerDataIN.shape[0])+&#34; unique wells with valid location information.&#34;)
    
    return downholeDataIN, headerDataIN</code></pre>
</details>
</dd>
<dt id="w4h.read_study_area"><code class="name flex">
<span>def <span class="ident">read_study_area</span></span>(<span>study_area_path, study_area_crs='EPSG:4269', log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Read study area geospatial file into geopandas</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>study_area_path</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code></dt>
<dd>Filepath to any geospatial file readable by geopandas.
Polygon is best, but may work with other types if extent is correct.</dd>
<dt><strong><code>crs</code></strong> :&ensp;<code>str, tuple, dict</code>, optional</dt>
<dd>CRS designation readable by geopandas/pyproj</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>studyAreaIN</code></strong> :&ensp;<code>geopandas dataframe</code></dt>
<dd>Geopandas dataframe with polygon geometry.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_study_area(study_area_path, study_area_crs=&#39;EPSG:4269&#39;, log=False):
    &#34;&#34;&#34;Read study area geospatial file into geopandas

    Parameters
    ----------
    study_area_path : str or pathlib.Path
        Filepath to any geospatial file readable by geopandas. 
        Polygon is best, but may work with other types if extent is correct.
    crs : str, tuple, dict, optional
        CRS designation readable by geopandas/pyproj
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    studyAreaIN : geopandas dataframe
        Geopandas dataframe with polygon geometry.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    studyAreaIN = gpd.read_file(study_area_path)
    studyAreaIN.to_crs(study_area_crs, inplace=True)

    return studyAreaIN</code></pre>
</details>
</dd>
<dt id="w4h.read_wcs"><code class="name flex">
<span>def <span class="ident">read_wcs</span></span>(<span>study_area, wcs_url='https://data.isgs.illinois.edu/arcgis/services/Elevation/IL_Statewide_Lidar_DEM_WGS/ImageServer/WCSServer?request=GetCapabilities&service=WCS', res_x=30, res_y=30, log=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads a WebCoverageService from a url and returns a rioxarray dataset containing it.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>study_area</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>Dataframe containing study area polygon</dd>
<dt><strong><code>wcs_url</code></strong> :&ensp;<code>str</code>, default=<code>lidarURL</code></dt>
<dd>&nbsp;</dd>
<dt>Represents the url for the WCS</dt>
<dt><strong><code>res_x</code></strong> :&ensp;<code>int</code>, default=<code>30</code></dt>
<dd>Sets resolution for x axis</dd>
<dt><strong><code>res_y</code></strong> :&ensp;<code>int</code>, default=<code>30</code></dt>
<dd>Sets resolution for y axis</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>wcsData_rxr</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>A xarray dataarray holding the image from the WebCoverageService</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_wcs(study_area, wcs_url=lidarURL, res_x=30, res_y=30, log=False, **kwargs):
    &#34;&#34;&#34;Reads a WebCoverageService from a url and returns a rioxarray dataset containing it.

    Parameters
    ----------
    study_area : geopandas.GeoDataFrame
        Dataframe containing study area polygon
    wcs_url : str, default=lidarURL
    Represents the url for the WCS
    res_x : int, default=30
        Sets resolution for x axis
    res_y : int, default=30
        Sets resolution for y axis
    log : bool, default = False
        Whether to log results to log file, by default False
    **kwargs

    Returns
    -------
    wcsData_rxr : xarray.DataArray
        A xarray dataarray holding the image from the WebCoverageService
    &#34;&#34;&#34;
    #Drawn largely from: https://git.wur.nl/isric/soilgrids/soilgrids.notebooks/-/blob/master/01-WCS-basics.ipynb
    
    #30m DEM
    #wcs_url = r&#39;https://data.isgs.illinois.edu/arcgis/services/Elevation/IL_DEM_30M/ImageServer/WCSServer?request=GetCapabilities&amp;service=WCS&#39;
    #lidar url:
    #lidarURL = r&#39;https://data.isgs.illinois.edu/arcgis/services/Elevation/IL_Statewide_Lidar_DEM_WGS/ImageServer/WCSServer?request=GetCapabilities&amp;service=WCS&#39;

    #studyAreaPath = r&#34;\\isgs-sinkhole.ad.uillinois.edu\geophysics\Balikian\ISWS_HydroGeo\WellDataAutoClassification\SampleData\ESL_StudyArea_5mi.shp&#34;
    #study_area = gpd.read_file(studyAreaPath)
    if study_area is None:
        print(&#39;ERROR: study_area must be specified to use read_wcs (currently set to {})&#39;.format(study_area))
        return

    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if &#39;wcs_url&#39; in kwargs:
        wcs_url = kwargs[&#39;wcs_url&#39;]
    if &#39;res_x&#39; in kwargs:
        res_x = kwargs[&#39;res_x&#39;]
    if &#39;res_y&#39; in kwargs:
        res_y = kwargs[&#39;res_y&#39;]
    
    width_in = &#39;&#39;
    height_in= &#39;&#39;

    #Create coverage object
    my_wcs = WebCoverageService(wcs_url, version=&#39;1.0.0&#39;) 
    #names = [k for k in my_wcs.contents.keys()]
    #print(names)
    dataID = &#39;IL_Statewide_Lidar_DEM&#39;
    data = my_wcs.contents[dataID]
    dBBox = data.boundingboxes #Is this an error?
    
    study_area = study_area.to_crs(data.boundingboxes[0][&#39;nativeSrs&#39;])
    saBBox = study_area.total_bounds
    
    #In case study area bounding box goes outside data bounding box, use data bounding box values
    newBBox = []
    for i,c in enumerate(dBBox[0][&#39;bbox&#39;]):
        if i == 0 or i==2:
            if saBBox[i] &lt; c:
                newBBox.append(saBBox[i])
            else:
                newBBox.append(c)
        else:
            if saBBox[i] &gt; c:
                newBBox.append(saBBox[i])
            else:
                newBBox.append(c)

    #Recalculate resolution if it is too fine to read in
    #Start by getting the area of the study area bounding box
    saWidth = saBBox[2]-saBBox[0]
    saHeight = saBBox[3]-saBBox[1]
    saBBoxAreaM = saWidth*saHeight
    saBBoxAreaKM = saBBoxAreaM/(1000*1000) #Area in km^2

    if saBBoxAreaM/(res_x*res_y) &gt; (4100*15000)*0.457194: #What I think might be the max download size?
        print(&#34;Resolution inputs overriden, file request too large.&#34;)
        res_x=str(round(saWidth/2500, 2))

        width_in  = str(int(saWidth/float(res_x )))
        height_in = str(int(saHeight/float(res_x)))
        
        res_y=str(round(saHeight/height_in, 2))

        print(&#39;New resolution is: &#39;+res_x+&#39;m_x X &#39;+res_y+&#39;m_y&#39; )
        print(&#39;Dataset size: &#39;+width_in+&#39; pixels_x X &#39;+height_in+&#39; pixels_y&#39;)

    bBox = tuple(newBBox)
    bBox_str = str(tuple(newBBox)[1:-1]).replace(&#39; &#39;,&#39;&#39;)
    dataCRS = &#39;EPSG:3857&#39;

    #Format WCS request using owslib
    response = my_wcs.getCoverage(
        identifier=my_wcs[dataID].id, 
        crs=dataCRS,#&#39;urn:ogc:def:crs:EPSG::26716&#39;,
        bbox=bBox,
        resx=res_x, 
        resy=res_y,
        timeout=60,
        #width = width_in, height=height_in,
        format=&#39;GeoTIFF&#39;)
    response

    #If I can figure out url, this might be better?
    #baseURL = r&#39;https://data.isgs.illinois.edu/arcgis/services/Elevation/&#39;+dataID+&#39;/ImageServer/WCSServer&#39;
    #addonRequestURL = &#39;?request=GetCoverage&amp;service=WCS&amp;bbox=&#39;+bBox_str+&#39;&amp;srs=&#39;+dataCRS+&#39;&amp;format=GeoTIFF&#39;+&#39;&amp;WIDTH=&#39;+width_in+&#39;&amp;HEIGHT=&#39;+height_in+&#39;)&#39;
    #reqURL = baseURL+addonRequestURL
    #wcsData_rxr =  rxr.open_rasterio(reqURL)

    with MemoryFile(response) as memfile:
        with memfile.open() as dataset:
            wcsData_rxr =  rxr.open_rasterio(dataset)

    return wcsData_rxr</code></pre>
</details>
</dd>
<dt id="w4h.read_wms"><code class="name flex">
<span>def <span class="ident">read_wms</span></span>(<span>study_area, layer_name='IL_Statewide_Lidar_DEM_WGS:None', wms_url='https://data.isgs.illinois.edu/arcgis/services/Elevation/IL_Statewide_Lidar_DEM_WGS/ImageServer/WCSServer?request=GetCapabilities&service=WCS', srs='EPSG:3857', clip_to_studyarea=True, bbox=[-9889002.6155, 5134541.069716, -9737541.607038, 5239029.6274], res_x=30, res_y=30, size_x=512, size_y=512, format='image/tiff', log=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads a WebMapService from a url and returns a rioxarray dataset containing it.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>study_area</code></strong> :&ensp;<code>geopandas.GeoDataFrame</code></dt>
<dd>Dataframe containg study area polygon</dd>
<dt><strong><code>layer_name</code></strong> :&ensp;<code>str</code>, default=<code>'IL_Statewide_Lidar_DEM_WGS:None'</code></dt>
<dd>Represents the layer name in the WMS</dd>
<dt><strong><code>wms_url</code></strong> :&ensp;<code>str</code>, default=<code>lidarURL</code></dt>
<dd>Represents the url for the WMS</dd>
<dt><strong><code>srs</code></strong> :&ensp;<code>str</code>, default=<code>'EPSG:3857'</code></dt>
<dd>Sets the srs</dd>
<dt><strong><code>clip_to_studyarea</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to clip to study area or not</dd>
<dt><strong><code>res_x</code></strong> :&ensp;<code>int</code>, default=<code>30</code></dt>
<dd>Sets resolution for x axis</dd>
<dt><strong><code>res_y</code></strong> :&ensp;<code>int</code>, default=<code>512</code></dt>
<dd>Sets resolution for y axis</dd>
<dt><strong><code>size_x</code></strong> :&ensp;<code>int</code>, default=<code>512</code></dt>
<dd>Sets width of result</dd>
<dt><strong><code>size_y</code></strong> :&ensp;<code>int</code>, default=<code>512</code></dt>
<dd>Sets height of result</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>wmsData_rxr</code></strong> :&ensp;<code>xarray.DataArray</code></dt>
<dd>Holds the image from the WebMapService</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_wms(study_area, layer_name=&#39;IL_Statewide_Lidar_DEM_WGS:None&#39;, wms_url=lidarURL, srs=&#39;EPSG:3857&#39;, clip_to_studyarea=True, bbox=[-9889002.615500,5134541.069716,-9737541.607038,5239029.627400],res_x=30, res_y=30, size_x=512, size_y=512, format=&#39;image/tiff&#39;, log=False, **kwargs):
    &#34;&#34;&#34;
    Reads a WebMapService from a url and returns a rioxarray dataset containing it.

    Parameters
    ----------
    study_area : geopandas.GeoDataFrame
        Dataframe containg study area polygon
    layer_name : str, default=&#39;IL_Statewide_Lidar_DEM_WGS:None&#39;
        Represents the layer name in the WMS
    wms_url : str, default=lidarURL
        Represents the url for the WMS
    srs : str, default=&#39;EPSG:3857&#39;
        Sets the srs
    clip_to_studyarea : bool, default=True
        Whether to clip to study area or not
    res_x : int, default=30
        Sets resolution for x axis
    res_y : int, default=512
        Sets resolution for y axis
    size_x : int, default=512
        Sets width of result
    size_y : int, default=512
        Sets height of result
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    wmsData_rxr : xarray.DataArray
        Holds the image from the WebMapService
    &#34;&#34;&#34;
    if study_area is None:
        print(&#39;ERROR: study_area must be specified to use read_wms (currently set to {})&#39;.format(study_area))
        return
    
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    from owslib.wms import WebMapService
    # Define WMS endpoint URL
    if &#39;wms_url&#39; in kwargs:
        wms_url = kwargs[&#39;wms_url&#39;]
    else:
        wms_url = wms_url

    # Create WMS connection object
    wms = WebMapService(wms_url)
    # Print available layers
    #print(wms.contents)

    # Select desired layer
    if &#39;layer_name&#39; in kwargs:
        layer = kwargs[&#39;layer_name&#39;]
    else:
        layer = layer_name
    
    data = wms.contents#[layer]
    if &#39;srs&#39; in kwargs:
        studyArea_proj = study_area.to_crs(kwargs[&#39;srs&#39;])
        saBBox = studyArea_proj.total_bounds
    else:
        studyArea_proj = study_area.to_crs(srs)
    
    saBBox = studyArea_proj.total_bounds

    if layer == &#39;IL_Statewide_Lidar_DEM_WGS:None&#39;:
        dBBox = data[&#39;0&#39;].boundingBox #Is this an error?

        gpdDict = {&#39;Label&#39;: [&#39;Surf Data Box&#39;], &#39;geometry&#39;: [shapely.geometry.Polygon(((dBBox[0], dBBox[1]), (dBBox[0], dBBox[3]), (dBBox[2], dBBox[3]), (dBBox[2], dBBox[1]), (dBBox[0], dBBox[1])))]}
        dBBoxGDF = gpd.GeoDataFrame(gpdDict, crs=dBBox[4])
        dBBoxGDF.to_crs(srs)

        #In case study area bounding box goes outside data bounding box, use data bounding box values
        newBBox = []
        for i,c in enumerate(dBBox):
            if type(c) is str:
                pass
            elif i == 0 or i==2:
                if saBBox[i] &lt; c:
                    newBBox.append(saBBox[i])
                else:
                    newBBox.append(c)
            else:
                if saBBox[i] &gt; c:
                    newBBox.append(saBBox[i])
                else:
                    newBBox.append(c)

    saWidth = saBBox[2]-saBBox[0]
    saHeight = saBBox[3]-saBBox[1]    
    # Check kwargs for rest of parameters
    if &#39;size_x&#39; in kwargs:
        size_x = kwargs[&#39;size_x&#39;]
    if &#39;size_y&#39; in kwargs:
        size_y = kwargs[&#39;size_y&#39;]
    if &#39;format&#39; in kwargs:
        format = kwargs[&#39;format&#39;]
    if &#39;clip_to_studyarea&#39; in kwargs:
        clip_to_studyarea = kwargs[&#39;clip_to_studyarea&#39;]
   
    #get the wms
    if clip_to_studyarea:
        img = wms.getmap(layers=[layer], srs=srs, bbox=saBBox, size=(size_x, size_y), format=format, transparent=True, timeout=60)        
    else:
        img = wms.getmap(layers=[layer], srs=srs, bbox=bbox, size=(size_x, size_y), format=format, transparent=True, timeout=60)

    #Save wms in memory to a raster dataset
    with MemoryFile(img) as memfile:
        with memfile.open() as dataset:
            wmsData_rxr = rxr.open_rasterio(dataset)

    #if clip_to_studyarea:
    #    wmsData_rxr = wmsData_rxr.sel(x=slice(saBBox[0], saBBox[2]), y=slice(saBBox[3], saBBox[1]))#.sel(band=1)

    return wmsData_rxr</code></pre>
</details>
</dd>
<dt id="w4h.read_xyz"><code class="name flex">
<span>def <span class="ident">read_xyz</span></span>(<span>xyzpath, datatypes=None, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read file containing xyz data (elevation/location)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>xyzpath</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code></dt>
<dd>Filepath of the xyz file, including extension</dd>
<dt><strong><code>datatypes</code></strong> :&ensp;<code>dict</code>, default <code>= None</code></dt>
<dd>Dictionary containing the datatypes for the columns int he xyz file. If None, {'ID':np.uint32,'API_NUMBER':np.uint64,'LATITUDE':np.float64,'LONGITUDE':np.float64,'ELEV_FT':np.float64}, by default None</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print the number of xyz records to the terminal, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing the elevation and location data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_xyz(xyzpath, datatypes=None, verbose=False, log=False):
    &#34;&#34;&#34;Function to read file containing xyz data (elevation/location)

    Parameters
    ----------
    xyzpath : str or pathlib.Path
        Filepath of the xyz file, including extension
    datatypes : dict, default = None
        Dictionary containing the datatypes for the columns int he xyz file. If None, {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#39;LATITUDE&#39;:np.float64,&#39;LONGITUDE&#39;:np.float64,&#39;ELEV_FT&#39;:np.float64}, by default None
    verbose : bool, default = False
        Whether to print the number of xyz records to the terminal, by default False
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    pandas.DataFrame
        Pandas dataframe containing the elevation and location data
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if datatypes is None:
        xyzDTypes = {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#39;LATITUDE&#39;:np.float64,&#39;LONGITUDE&#39;:np.float64,&#39;ELEV_FT&#39;:np.float64}

    xyzDataIN = pd.read_csv(xyzpath, sep=&#39;,&#39;, header=&#39;infer&#39;, dtype=xyzDTypes, index_col=&#39;ID&#39;)
    
    if verbose:
        print(&#39;XYZ file has &#39; + str(xyzDataIN.shape[0])+&#39; records with elevation and location.&#39;)
    return xyzDataIN</code></pre>
</details>
</dd>
<dt id="w4h.remerge_data"><code class="name flex">
<span>def <span class="ident">remerge_data</span></span>(<span>classifieddf, searchdf)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to merge newly-classified (or not) and previously classified data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>classifieddf</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe that had already been classified previously</dd>
<dt><strong><code>searchdf</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe with new classifications</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>remergeDF</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all the data, merged back together</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remerge_data(classifieddf, searchdf):
    &#34;&#34;&#34;Function to merge newly-classified (or not) and previously classified data

    Parameters
    ----------
    classifieddf : pandas.DataFrame
        Dataframe that had already been classified previously
    searchdf : pandas.DataFrame
        Dataframe with new classifications

    Returns
    -------
    remergeDF : pandas.DataFrame
        Dataframe containing all the data, merged back together
    &#34;&#34;&#34;
    remergeDF = pd.concat([classifieddf,searchdf], join=&#39;inner&#39;).sort_index()
    return remergeDF</code></pre>
</details>
</dd>
<dt id="w4h.remove_bad_depth"><code class="name flex">
<span>def <span class="ident">remove_bad_depth</span></span>(<span>df, top_col='TOP', bottom_col='BOTTOM', depth_type='depth', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to remove all records in the dataframe with well interpretations where the depth information is bad (i.e., where the bottom of the record is neerer to the surface than the top)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing the well records and descriptions for each interval</dd>
<dt><strong><code>top_col</code></strong> :&ensp;<code>str</code>, default=<code>'TOP'</code></dt>
<dd>The name of the column containing the depth or elevation for the top of the interval, by default 'TOP'</dd>
<dt><strong><code>bottom_col</code></strong> :&ensp;<code>str</code>, default=<code>'BOTTOM'</code></dt>
<dd>The name of the column containing the depth or elevation for the bottom of each interval, by default 'BOTTOM'</dd>
<dt><strong><code>depth_type</code></strong> :&ensp;<code>str, {'depth', 'elevation'}</code></dt>
<dd>Whether the table is organized by depth or elevation. If depth, the top column will have smaller values than the bottom column. If elevation, the top column will have higher values than the bottom column, by default 'depth'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print results to the terminal, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Dataframe</code></dt>
<dd>Pandas dataframe with the records remvoed where the top is indicatd to be below the bottom.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_bad_depth(df, top_col=&#39;TOP&#39;, bottom_col=&#39;BOTTOM&#39;, depth_type=&#39;depth&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to remove all records in the dataframe with well interpretations where the depth information is bad (i.e., where the bottom of the record is neerer to the surface than the top)

    Parameters
    ----------
    df : pandas.DataFrame
        Pandas dataframe containing the well records and descriptions for each interval
    top_col : str, default=&#39;TOP&#39;
        The name of the column containing the depth or elevation for the top of the interval, by default &#39;TOP&#39;
    bottom_col : str, default=&#39;BOTTOM&#39;
        The name of the column containing the depth or elevation for the bottom of each interval, by default &#39;BOTTOM&#39;
    depth_type : str, {&#39;depth&#39;, &#39;elevation&#39;}
        Whether the table is organized by depth or elevation. If depth, the top column will have smaller values than the bottom column. If elevation, the top column will have higher values than the bottom column, by default &#39;depth&#39;
    verbose : bool, default = False
        Whether to print results to the terminal, by default False
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    pandas.Dataframe
        Pandas dataframe with the records remvoed where the top is indicatd to be below the bottom.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if depth_type.lower() ==&#39;depth&#39;:
        df[&#39;THICKNESS&#39;] = df[bottom_col] - df[top_col] #Calculate interval thickness
    elif depth_type.lower() ==&#39;elevation&#39; or depth_type==&#39;elev&#39;:
        df[&#39;THICKNESS&#39;] = df[top_col] - df[bottom_col] #Calculate interval thickness
    before = df.shape[0] #Calculate number of rows before dropping
    df = df[(df[&#39;THICKNESS&#39;] &gt;= 0)] #Only include rows where interval thickness is positive (bottom is deeper than top)
    df.reset_index(inplace=True, drop=True) #Reset index

    if verbose:
        print(&#34;Number of rows before dropping those with obviously bad depth information: &#34;+str(before))
        print(&#34;Number of rows after dropping those with obviously bad depth information: &#34;+str(df.shape[0]))
        print(&#39;Well records deleted: &#39;+str(before-df.shape[0]))
    return df</code></pre>
</details>
</dd>
<dt id="w4h.remove_no_depth"><code class="name flex">
<span>def <span class="ident">remove_no_depth</span></span>(<span>df, top_col='TOP', bottom_col='BOTTOM', no_data_val_table='', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to remove well intervals with no depth information</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing well descriptions</dd>
<dt><strong><code>top_col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of column containing information on the top of the well intervals, by default 'TOP'</dd>
<dt><strong><code>bottom_col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of column containing information on the bottom of the well intervals, by default 'BOTTOM'</dd>
<dt><strong><code>no_data_val_table</code></strong> :&ensp;<code>any</code>, optional</dt>
<dd>No data value in the input data, used by this function to indicate that depth data is not there, to be replaced by np.nan, by default ''</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to print results to console, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe with depths dropped</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_no_depth(df, top_col=&#39;TOP&#39;, bottom_col=&#39;BOTTOM&#39;, no_data_val_table=&#39;&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to remove well intervals with no depth information

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe containing well descriptions
    top_col : str, optional
        Name of column containing information on the top of the well intervals, by default &#39;TOP&#39;
    bottom_col : str, optional
        Name of column containing information on the bottom of the well intervals, by default &#39;BOTTOM&#39;
    no_data_val_table : any, optional
        No data value in the input data, used by this function to indicate that depth data is not there, to be replaced by np.nan, by default &#39;&#39;
    verbose : bool, optional
        Whether to print results to console, by default False
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    df : pandas.DataFrame
        Dataframe with depths dropped
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    #Replace empty cells in top and bottom columns with nan
    df[top_col] = df[top_col].replace(no_data_val_table, np.nan)
    df[bottom_col] = df[bottom_col].replace(no_data_val_table, np.nan)
    
    #Calculate number of rows before dropping
    before = df.shape[0]

    #Drop records without depth information
    df = df.dropna(subset=[top_col])
    df = df.dropna(subset=[bottom_col])
    df.reset_index(inplace=True, drop=True) #Reset index
  
    if verbose:
        print(&#34;Number of rows before dropping those without record depth information: &#34; + str(before))
        print(&#34;Number of rows after dropping those without record depth information: &#34; + str(df.shape[0]))
        print(&#39;Number of well records without formation information deleted: &#39; + str(before - df.shape[0]))
    
    return df</code></pre>
</details>
</dd>
<dt id="w4h.remove_no_formation"><code class="name flex">
<span>def <span class="ident">remove_no_formation</span></span>(<span>df, description_col='FORMATION', no_data_val_table='', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function that removes all records in the dataframe containing the well descriptions where no description is given.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing the well records with their individual descriptions</dd>
<dt><strong><code>description_col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of the column containing the geologic description of each interval, by default 'FORMATION'</dd>
<dt><strong><code>no_data_val_table</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The value expected if the column is empty or there is no data. These will be replaced by np.nan before being removed, by default ''</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to print the results of this step to the terminal, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with records with no description removed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_no_formation(df, description_col=&#39;FORMATION&#39;, no_data_val_table=&#39;&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function that removes all records in the dataframe containing the well descriptions where no description is given.

    Parameters
    ----------
    df : pandas.DataFrame
        Pandas dataframe containing the well records with their individual descriptions
    description_col : str, optional
        Name of the column containing the geologic description of each interval, by default &#39;FORMATION&#39;
    no_data_val_table : str, optional
        The value expected if the column is empty or there is no data. These will be replaced by np.nan before being removed, by default &#39;&#39;
    verbose : bool, optional
        Whether to print the results of this step to the terminal, by default False
    log : bool, default = False
        Whether to log results to log file, by default False
        
    Returns
    -------
    pandas.DataFrame
        Pandas dataframe with records with no description removed.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    #Replace empty cells in formation column with nans
    df[description_col] = df[description_col].replace(no_data_val_table, np.nan) 
    before = df.shape[0] #Calculate number of rows before dropping

    #Drop records without FORMATION information
    df = df.dropna(subset=[description_col])
    df.reset_index(inplace=True, drop=True) #Reset index

    if verbose:
        print(&#34;Number of rows before dropping those without FORMATION information: &#34;+str(before))
        print(&#34;Number of rows after dropping those without FORMATION information: &#34;+str(df.shape[0]))
        print(&#39;Well records deleted: &#39;+str(before-df.shape[0]))
        
    return df</code></pre>
</details>
</dd>
<dt id="w4h.remove_no_topo"><code class="name flex">
<span>def <span class="ident">remove_no_topo</span></span>(<span>df, zcol='ELEV_FT', no_data_val_table='', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to remove wells that do not have topography data (needed for layer selection later).</p>
<p>This function is intended to be run on the metadata table after elevations have attempted to been added.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing elevation information.</dd>
<dt><strong><code>zcol</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of elevation column</dd>
<dt><strong><code>no_data_val_table</code></strong> :&ensp;<code>any</code></dt>
<dd>Value in dataset that indicates no data is present (replaced with np.nan)</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to print outputs, by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with intervals with no topography removed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_no_topo(df, zcol=&#39;ELEV_FT&#39;, no_data_val_table=&#39;&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to remove wells that do not have topography data (needed for layer selection later).

    This function is intended to be run on the metadata table after elevations have attempted to been added.

    Parameters
    ----------
    df : pandas.DataFrame
        Pandas dataframe containing elevation information.
    zcol : str
        Name of elevation column
    no_data_val_table : any
        Value in dataset that indicates no data is present (replaced with np.nan)
    verbose : bool, optional
        Whether to print outputs, by default True
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    pandas.DataFrame
        Pandas dataframe with intervals with no topography removed.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    before = df.shape[0]
    
    df[zcol].replace(no_data_val_table, np.nan, inplace=True)
    df.dropna(subset=[zcol], inplace=True)
    
    if verbose:
        after = df.shape[0]
        print(&#39;Well records removed: &#39;+str(before-after))
        print(&#34;Number of rows before dropping those without surface elevation information: &#34;+str(before))
        print(&#34;Number of rows after dropping those without surface elevation information: &#34;+str(after))
    
    return df</code></pre>
</details>
</dd>
<dt id="w4h.remove_nonlocated"><code class="name flex">
<span>def <span class="ident">remove_nonlocated</span></span>(<span>df, metadata_df, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to remove wells and well intervals where there is no location information</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing well descriptions</dd>
<dt><strong><code>metadata_DF</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing metadata, including well locations (e.g., Latitude/Longitude)</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing only data with location information</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_nonlocated(df, metadata_df, verbose=False, log=False):
    &#34;&#34;&#34;Function to remove wells and well intervals where there is no location information

    Parameters
    ----------
    df : pandas.DataFrame
        Pandas dataframe containing well descriptions
    metadata_DF : pandas.DataFrame
        Pandas dataframe containing metadata, including well locations (e.g., Latitude/Longitude)
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    df : pandas.DataFrame
        Pandas dataframe containing only data with location information
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    before = df.shape[0] #Extract length of data before this process

    #Create Merged dataset only with data where wells exist in both databases (i.e., well has data and location info)
    df = pd.merge(df, metadata_df.set_index(&#39;API_NUMBER&#39;), on=&#39;API_NUMBER&#39;, how=&#39;left&#39;, indicator=&#39;Exist&#39;)
    df[&#39;Existbool&#39;] = np.where(df[&#39;Exist&#39;] == &#39;both&#39;, True, False)
    df = df[df[&#39;Existbool&#39;]==True].drop([&#39;Exist&#39;,&#39;Existbool&#39;], axis=1)
    
    #Create new downhole data table with only relevant records and columns
    keepCols=[&#39;API_NUMBER&#39;,&#39;TABLE_NAME&#39;,&#39;FORMATION&#39;,&#39;THICKNESS&#39;,&#39;TOP&#39;,&#39;BOTTOM&#39;]
    df = df[keepCols].copy()
    if verbose:
        after = df.shape[0]
        print(str(before-after)+&#39; records removed without location information.&#39;)
        print(str(df.shape[0])+&#39; wells remain from &#39;+str(df[&#39;API_NUMBER&#39;].unique().shape[0])+&#39; geolocated wells in study area.&#39;)
    return df</code></pre>
</details>
</dd>
<dt id="w4h.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>well_data, well_data_cols=None, metadata=None, well_metadata_cols=None, layers=9, description_col='FORMATION', top_col='TOP', bottom_col='BOTTOM', depth_type='depth', study_area=None, xcol='LONGITUDE', ycol='LATITUDE', zcol='ELEVATION', idcol='API_NUMBER', output_crs='EPSG:4269', surf_elev_file=None, bedrock_elev_file=None, model_grid=None, lith_dict=None, lith_dict_start=None, lith_dict_wildcard=None, target_dict=None, target_name='CoarseFine', export_dir=None, verbose=False, log=False, **keyword_parameters)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to run entire process with one line of code. </p>
<p>NOTE: verbose and log are boolean parameters used for most of the functions. verbose=True prints information to terminal, log=True logs to a file in the log_dir, which defaults to the export_dir</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>well_data</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path obj</code></dt>
<dd>Filepath to file or directory containing well data.</dd>
<dt><strong><code>well_data_cols</code></strong> :&ensp;<code>List</code> or <code>list-like</code></dt>
<dd>Columns to</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>Filepath to file or directory containing well metadata, such as location and elevation.</dd>
<dt><strong><code>well_metadata_cols</code></strong> :&ensp;<code>List</code> or <code>list-like</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>int</code>, default <code>= 9</code></dt>
<dd>The number of layers in the model grid</dd>
<dt><strong><code>description_col</code></strong> :&ensp;<code>str</code>, default <code>= 'FORMATION'</code></dt>
<dd>Name of column containing geologic descriptions of the well interval. This column should be in well_data.</dd>
<dt><strong><code>top_col</code></strong> :&ensp;<code>str</code>, default <code>= 'TOP'</code></dt>
<dd>Name of column containing depth/elevation at top of well interval. This column should be in well_data.</dd>
<dt><strong><code>bottom_col</code></strong> :&ensp;<code>str</code>, default <code>= 'BOTTOM'</code></dt>
<dd>Name of column containing depth/elevation at bottom of well interval. This column should be in well_data.</dd>
<dt><strong><code>depth_type</code></strong> :&ensp;<code>str</code>, default <code>= 'depth'</code></dt>
<dd>Whether values top_col or bottom_col refer to depth or elevation.</dd>
<dt><strong><code>study_area</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>geopandas.GeoDataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>xcol</code></strong> :&ensp;<code>str</code>, default <code>= 'LONGITUDE'</code></dt>
<dd>Name of column containing x coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default <code>= 'LATITUDE'</code></dt>
<dd>Name of column containing y coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.</dd>
<dt><strong><code>zcol</code></strong> :&ensp;<code>str</code>, default <code>= 'ELEVATION'</code></dt>
<dd>Name of column containing z coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.</dd>
<dt><strong><code>output_crs</code></strong> :&ensp;<code>crs definition accepted by pyproj</code>, default <code>= 'EPSG:4269'</code></dt>
<dd>CRS to output all of the data into</dd>
<dt><strong><code>surf_elev_file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>bedrock_elev_file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>model_grid</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>model grid parameters (see model_grid function)</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>lith_dict</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>pandas.DataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>lith_dict_start</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>pandas.DataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>lith_dict_wildcard</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>pandas.DataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>target_dict</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>pandas.DataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>target_name</code></strong> :&ensp;<code>str</code>, default <code>= 'CoarseFine'</code></dt>
<dd>Name of target of interest, to be used on exported files</dd>
<dt><strong><code>export_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, default <code>= None</code></dt>
<dd>Directory to export output files</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print updates/results</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to send parameters and outputs to log file, to be saved in export_dir, or the same directory as well_data if export_dir not defined.</dd>
<dt><strong><code>**keyword_parameters</code></strong></dt>
<dd>Keyword parameters used by any of the functions throughout the process. See list of functions above, and the API documentation for their possible parameters</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(well_data, well_data_cols=None, 
        metadata=None, well_metadata_cols=None, 
        layers = 9,
        description_col=&#39;FORMATION&#39;, top_col=&#39;TOP&#39;, bottom_col=&#39;BOTTOM&#39;, depth_type=&#39;depth&#39;,
        study_area=None, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, zcol=&#39;ELEVATION&#39;, idcol=&#39;API_NUMBER&#39;, output_crs=&#39;EPSG:4269&#39;,
        surf_elev_file=None, bedrock_elev_file=None, model_grid=None,
        lith_dict=None, lith_dict_start=None, lith_dict_wildcard=None,
        target_dict=None,
        target_name=&#39;CoarseFine&#39;,
        export_dir=None,
        verbose=False,
        log=False,
        **keyword_parameters):
    
    &#34;&#34;&#34;Function to run entire process with one line of code. 
    
    NOTE: verbose and log are boolean parameters used for most of the functions. verbose=True prints information to terminal, log=True logs to a file in the log_dir, which defaults to the export_dir

    Parameters
    ----------
    well_data : str or pathlib.Path obj
        Filepath to file or directory containing well data.
    well_data_cols : List or list-like
        Columns to 
    metadata : str or pathlib.Path object
        Filepath to file or directory containing well metadata, such as location and elevation.
    well_metadata_cols : List or list-like
        _description_
    layers : int, default = 9
        The number of layers in the model grid
    description_col : str, default = &#39;FORMATION&#39;
        Name of column containing geologic descriptions of the well interval. This column should be in well_data.
    top_col : str, default = &#39;TOP&#39;
        Name of column containing depth/elevation at top of well interval. This column should be in well_data.
    bottom_col : str, default = &#39;BOTTOM&#39;
        Name of column containing depth/elevation at bottom of well interval. This column should be in well_data.    
    depth_type : str, default = &#39;depth&#39;
        Whether values top_col or bottom_col refer to depth or elevation.
    study_area : str or pathlib.Path object, or geopandas.GeoDataFrame
        _description_
    xcol : str, default = &#39;LONGITUDE&#39; 
        Name of column containing x coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.
    ycol : str, default = &#39;LATITUDE&#39;
        Name of column containing y coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.
    zcol : str, default = &#39;ELEVATION&#39; 
        Name of column containing z coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.
    output_crs : crs definition accepted by pyproj, default = &#39;EPSG:4269&#39;
        CRS to output all of the data into
    surf_elev_file : str or pathlib.Path object
        _description_
    bedrock_elev_file : str or pathlib.Path object
        _description_
    model_grid : str or pathlib.Path object, or model grid parameters (see model_grid function)
        _description_
    lith_dict : str or pathlib.Path object, or pandas.DataFrame
        _description_
    lith_dict_start : str or pathlib.Path object, or pandas.DataFrame
        _description_
    lith_dict_wildcard : str or pathlib.Path object, or pandas.DataFrame
        _description_
    target_dict : str or pathlib.Path object, or pandas.DataFrame
        _description_
    target_name : str, default = &#39;CoarseFine&#39;
        Name of target of interest, to be used on exported files
    export_dir : str or pathlib.Path object, default = None
        Directory to export output files
    verbose : bool, default = False
        Whether to print updates/results
    log : bool, default = False
        Whether to send parameters and outputs to log file, to be saved in export_dir, or the same directory as well_data if export_dir not defined.
    **keyword_parameters
        Keyword parameters used by any of the functions throughout the process. See list of functions above, and the API documentation for their possible parameters
    &#34;&#34;&#34;

    #Get data (files or otherwise)
    file_setup_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.file_setup.__code__.co_varnames}
    
    #Check how well_data and metadata were defined
    if isinstance(well_data, pathlib.PurePath) or isinstance(well_data, str):
        #Convert well_data to pathlib.Path if not already
        if isinstance(well_data, str):
            well_data = pathlib.Path(well_data)

        if metadata is None:
            if well_data.is_dir():
                downholeDataPATH, headerDataPATH = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             
            elif well_data.exists():
                downholeDataPATH, _ = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             
            else:
                print(&#39;ERROR: well_data file does not exist:{}&#39;.format(well_data))
        elif isinstance(metadata, pathlib.PurePath) or isinstance(metadata, str):
            if isinstance(metadata, str):
                metadata = pathlib.Path(metadata)    
            downholeDataPATH, headerDataPATH = w4h.file_setup(well_data=well_data, metadata=metadata, **file_setup_kwargs)                
        else:
            if isinstance(metadata, pd.DataFrame):
                downholeDataPATH, _ = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             
                headerDataPATH = metadata
            elif metadata is None:
                downholeDataPATH, _ = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             

    elif isinstance(well_data, pd.DataFrame):
        if isinstance(metadata, pd.DataFrame):
            downholeDataPATH = well_data
            headerDataPATH = metadata
        elif isinstance(metadata, pathlib.PurePath) or isinstance(metadata, str):
            _, headerDataPATH = w4h.file_setup(well_data=metadata, metadata=metadata, verbose=verbose, log=log, **file_setup_kwargs)                
            downholeDataPATH = well_data
        else:
            print(&#39;ERROR: metadata must be a string filepath, a pathlib.Path object, or pandas.DataFrame&#39;)
    else:
        print(&#39;ERROR: well_data must be a string filepath, a pathlib.Path object, or pandas.DataFrame&#39;)

    #Get pandas dataframes from input
    read_raw_txt_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_raw_csv .__code__.co_varnames}
    downholeDataIN, headerDataIN = w4h.read_raw_csv (data_filepath=downholeDataPATH, metadata_filepath=headerDataPATH, verbose=verbose, log=log, **read_raw_txt_kwargs) 
    #Functions to read data into dataframes. Also excludes extraneous columns, and drops header data with no location information

    #Define data types (file will need to be udpated)
    downholeData = w4h.define_dtypes(undefined_df=downholeDataIN, datatypes=&#39;./resources/downholeDataTypes.txt&#39;, verbose=verbose, log=log)
    headerData = w4h.define_dtypes(undefined_df=headerDataIN, datatypes=&#39;./resources/headerDataTypes.txt&#39;, verbose=verbose, log=log)

    #Get Study area
    read_study_area_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_study_area.__code__.co_varnames}
    if study_area is None:
        studyAreaIN = None
        use_study_area = False
    else:
        studyAreaIN = w4h.read_study_area(study_area_path=study_area, log=log, **read_study_area_kwargs)
        use_study_area = True

    #Get surfaces and grid(s)
    read_grid_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_grid.__code__.co_varnames}

    modelGridPath = model_grid
    surfaceElevPath = surf_elev_file
    bedrockElevPath = bedrock_elev_file
    #UPDATE: allow other types of model grid read ***
    modelGrid = w4h.read_grid(grid_path=modelGridPath, grid_type=&#39;model&#39;, study_area=studyAreaIN, verbose=verbose, log=log, **read_grid_kwargs)
    surfaceElevGridIN = w4h.read_grid(grid_path=surfaceElevPath, grid_type=&#39;surface&#39;, study_area=studyAreaIN, verbose=verbose, log=log, **read_grid_kwargs)
    bedrockElevGridIN = w4h.read_grid(grid_path=bedrockElevPath, grid_type=&#39;bedrock&#39;, study_area=studyAreaIN, verbose=verbose, log=log, **read_grid_kwargs)

    #Add control points
    #UPDATE: Code here for adding in control points ***

    #UPDATE: MAKE SURE CRS&#39;s all align ***

    #Convert headerData to have geometry
    coords2geometry_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.coords2geometry.__code__.co_varnames}
    headerData = w4h.coords2geometry(df=headerData, xcol=xcol, ycol=ycol, zcol=zcol, log=log, **coords2geometry_kwargs)
    clip_gdf2study_area_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.clip_gdf2study_area.__code__.co_varnames}
    headerData = w4h.clip_gdf2study_area(study_area=studyAreaIN, gdf=headerData, log=log, **clip_gdf2study_area_kwargs)

    #Clean up data
    downholeData = w4h.remove_nonlocated(downholeData, headerData, log=log, verbose=verbose)
    headerData = w4h.remove_no_topo(df=headerData, zcol=zcol, verbose=verbose, log=log)

    remove_no_depth_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.remove_no_depth.__code__.co_varnames}
    donwholeData = w4h.remove_no_depth(downholeData, verbose=verbose, top_col=top_col, bottom_col=bottom_col, log=log, **remove_no_depth_kwargs) #Drop records with no depth information

    remove_bad_depth_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.remove_bad_depth.__code__.co_varnames}
    donwholeData = w4h.remove_bad_depth(downholeData, verbose=verbose, top_col=top_col, bottom_col=bottom_col, depth_type=depth_type, log=log, **remove_bad_depth_kwargs)#Drop records with bad depth information (i.e., top depth &gt; bottom depth) (Also calculates thickness of each record)

    remove_no_formation_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.remove_no_formation.__code__.co_varnames}
    downholeData = w4h.remove_no_formation(downholeData, description_col=description_col, verbose=verbose, log=log, **remove_no_formation_kwargs)

    #CLASSIFICATION
    #Read dictionary definitions and classify
    get_search_terms_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.get_search_terms.__code__.co_varnames}
    specTermsPATH, startTermsPATH, wildcardTermsPATH, = w4h.get_search_terms(spec_path=lith_dict, start_path=lith_dict_start, wildcard_path=lith_dict_wildcard, log=log, **get_search_terms_kwargs)
    read_dictionary_terms_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_dictionary_terms.__code__.co_varnames}
    specTerms = w4h.read_dictionary_terms(dict_file=specTermsPATH, log=log, **read_dictionary_terms_kwargs)
    startTerms = w4h.read_dictionary_terms(dict_file=startTermsPATH, log=log, **read_dictionary_terms_kwargs)
    wildcardTerms = w4h.read_dictionary_terms(dict_file=wildcardTermsPATH, log=log, **read_dictionary_terms_kwargs)

    #Clean up dictionary terms
    specTerms.drop_duplicates(subset=&#39;DESCRIPTION&#39;, inplace=True)
    specTerms.reset_index(inplace=True, drop=True)

    startTerms.drop_duplicates(subset=&#39;DESCRIPTION&#39;, inplace=True)
    startTerms.reset_index(inplace=True, drop=True)

    wildcardTerms.drop_duplicates(subset=&#39;DESCRIPTION&#39;, inplace=True)
    wildcardTerms.reset_index(inplace=True, drop=True)

    if verbose:
        print(&#39;Search terms to be used:&#39;)
        print(&#39;\t {} exact match term/definition pairs&#39;)
        print(&#39;\t {} starting match term/definition pairs&#39;)
        print(&#39;\t {} wildcard match term/definition pairs&#39;)

    #CLASSIFICATIONS
    #Exact match classifications
    downholeData = w4h.specific_define(downholeData, specTerms, description_col=description_col, verbose=verbose, log=log)
    
    #.startswith classifications
    classifedDF, searchDF = w4h.split_defined(downholeData, verbose=verbose, log=log)
    searchDF = w4h.start_define(df=searchDF, terms_df=startTerms, description_col=description_col, verbose=verbose, log=log)
    downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF) #UPDATE: Needed? ***    

    #wildcard/any substring match classifications    
    classifedDF, searchDF = w4h.split_defined(downholeData, verbose=verbose, log=log)
    searchDF = w4h.wildcard_define(df=searchDF, terms_df=wildcardTerms, description_col=description_col, verbose=verbose, log=log)
    downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF) #UPDATE: Needed? ***    

    #Depth classification
    classifedDF, searchDF = w4h.split_defined(downholeData, verbose=verbose, log=log)
    searchDF = w4h.depth_define(searchDF, thresh=550, verbose=verbose, log=log)
    downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF) #UPDATE: Needed? ***
    
    #Fill unclassified data
    downholeData = w4h.fill_unclassified(downholeData, classification_col=&#39;CLASS_FLAG&#39;)
    
    #Add target interpratations
    read_lithologies_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_lithologies.__code__.co_varnames}
    targetInterpDF = w4h.read_lithologies(lith_file=target_dict, log=log, **read_lithologies_kwargs)
    downholeData = w4h.merge_lithologies(df=downholeData, targinterps_df=targetInterpDF, target_col=&#39;TARGET&#39;, target_class=&#39;bool&#39;)
    
    #Sort dataframe to prepare for next steps
    #downholeData = w4h.sort_dataframe(df=downholeData, sort_cols=[&#39;API_NUMBER&#39;,&#39;TOP&#39;], remove_nans=True)
    downholeData = downholeData.sort_values(sort_cols=[idcol, top_col])
    downholeData.reset_index(inplace=True, drop=True)
    #UPDATE: Option to remove nans?
    downholeData = downholeData[pd.notna(downholeData[&#34;LITHOLOGY&#34;])]

    #Analyze Surface(s) and grid(s)
    bedrockGrid, surfaceGrid = w4h.align_rasters(grids_unaligned=[bedrockElevGridIN, surfaceElevGridIN], modelgrid=modelGrid, no_data_val=0, log=log)
    driftThickGrid, layerThickGrid = w4h.get_drift_thick(surface=surfaceGrid, bedrock=bedrockGrid, layers=layers, plot=verbose, log=log)
    #UPDATE: LAYER NAMES SO DON&#34;T INCLUDE FT
    headerData = w4h.sample_raster_points(raster=bedrockGrid, points_df=headerData, xcol=xcol, ycol=ycol, new_col=&#39;BEDROCK_ELEV_FT&#39;, verbose=verbose, log=log)
    headerData = w4h.sample_raster_points(raster=surfaceGrid, points_df=headerData, xcol=xcol, ycol=ycol, new_col=&#39;SURFACE_ELEV_FT&#39;, verbose=verbose, log=log)
    headerData = w4h.sample_raster_points(raster=driftThickGrid, points_df=headerData, xcol=xcol, ycol=ycol, new_col=&#39;BEDROCK_DEPTH_FT&#39;, verbose=verbose, log=log)
    headerData = w4h.sample_raster_points(raster=layerThickGrid, points_df=headerData, xcol=xcol, ycol=ycol, new_col=&#39;LAYER_THICK_FT&#39;, verbose=verbose, log=log)
    headerData = w4h.get_layer_depths(metadata=headerData, no_layers=layers, log=log)

    #UPDATE: Check if this actually works, I think they should be copies of each other if metadata is not specified and not found using the metadata_filename pattern in file_setup() ***
    #Merge header and data into one df, if applicable
    if downholeData.values.base is headerData.values.base:
        pass
    else:
        #downholeData = pd.merge(left = downholeData, right = headerData, on=idcol)
        downholeData = w4h.merge_tables(data_df=downholeData,  header_df=headerData, data_cols=None, header_cols=None, on=idcol, how=&#39;inner&#39;, auto_pick_cols=True, log=log)
    
    #UPDATE: START HERE AGAIN, double checck and get kwargs for all functions ***
    #downholeData = downholeData.copy()
    #UPDATE: Potentially need to remove duplicate columns here, I think I fixed that tho ***
    resdf = w4h.layer_target_thick(downholeData, layers=9, return_all=False, outfile_prefix=&#39;CoarseFine&#39;, export_dir=export_dir, depth_top_col=top_col, depth_bot_col=bottom_col, log=log)
    layers_data = w4h.layer_interp(points=resdf, layers=9, grid=modelGrid, method=&#39;lin&#39;, log=log)

    if export_dir is None:
        if well_data.is_dir():
            export_dir = well_data.joinpath(&#39;Output&#39;)
        else:
            export_dir = well_data.parent.joinpath()
        
        if not export_dir.exists():
            try:
                export_dir.mkdir()
            except:
                pass

    w4h.export_grids(grid_data=layers_data, out_path=export_dir, file_id=&#39;&#39;,filetype=&#39;tif&#39;, variable_sep=True, date_stamp=True, log=log)
    #UPDATE: export points?
    return resdf, layers_data</code></pre>
</details>
</dd>
<dt id="w4h.sample_raster_points"><code class="name flex">
<span>def <span class="ident">sample_raster_points</span></span>(<span>raster, points_df, xcol='LONGITUDE', ycol='LATITUDE', new_col='SAMPLED', verbose=True, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample raster values to points from geopandas geodataframe.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>raster</code></strong> :&ensp;<code>rioxarray data array</code></dt>
<dd>Raster containing values to be sampled.</dd>
<dt><strong><code>points_df</code></strong> :&ensp;<code>geopandas.geodataframe</code></dt>
<dd>Geopandas dataframe with geometry column containing point values to sample.</dd>
<dt><strong><code>xcol</code></strong> :&ensp;<code>str</code>, default=<code>'LONGITUDE'</code></dt>
<dd>Column containing name for x-column, by default 'LONGITUDE.'
This is used to output (potentially) reprojected point coordinates so as not to overwrite the original.</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default=<code>'LATITUDE'</code></dt>
<dd>Column containing name for y-column, by default 'LATITUDE.'
This is used to output (potentially) reprojected point coordinates so as not to overwrite the original.
new_col : str, optional</dd>
<dt><strong><code>new_col</code></strong> :&ensp;<code>str</code>, default=<code>'SAMPLED'</code></dt>
<dd>Name for name of new column containing points sampled from the raster, by default 'SAMPLED'.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to send to print() information about progress of function, by default True.</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>points_df</code></strong> :&ensp;<code>geopandas.geodataframe</code></dt>
<dd>Same as points_df, but with sampled values and potentially with reprojected coordinates.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_raster_points(raster, points_df, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, new_col=&#39;SAMPLED&#39;, verbose=True, log=False):  
    &#34;&#34;&#34;Sample raster values to points from geopandas geodataframe.

    Parameters
    ----------
    raster : rioxarray data array
        Raster containing values to be sampled.
    points_df : geopandas.geodataframe
        Geopandas dataframe with geometry column containing point values to sample.
    xcol : str, default=&#39;LONGITUDE&#39;
        Column containing name for x-column, by default &#39;LONGITUDE.&#39;
        This is used to output (potentially) reprojected point coordinates so as not to overwrite the original.
    ycol : str, default=&#39;LATITUDE&#39;
        Column containing name for y-column, by default &#39;LATITUDE.&#39;
        This is used to output (potentially) reprojected point coordinates so as not to overwrite the original.    new_col : str, optional
    new_col : str, default=&#39;SAMPLED&#39;
        Name for name of new column containing points sampled from the raster, by default &#39;SAMPLED&#39;.
    verbose : bool, default=True
        Whether to send to print() information about progress of function, by default True.
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    points_df : geopandas.geodataframe
        Same as points_df, but with sampled values and potentially with reprojected coordinates.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if verbose:
        nowTime = datetime.datetime.now()
        expectMin = (points_df.shape[0]/3054409) * 14
        endTime = nowTime+datetime.timedelta(minutes=expectMin)
        print(new_col+ &#34; sampling should be done by {:d}:{:02d}&#34;.format(endTime.hour, endTime.minute))

    #Project points to raster CRS
    rastercrsWKT=raster.spatial_ref.crs_wkt
    points_df = points_df.to_crs(rastercrsWKT)
    #if xcol==&#39;LONGITUDE&#39; and ycol==&#39;LATITUDE&#39;:
    xCOLOUT = xcol+&#39;_PROJ&#39;
    yCOLOUT = ycol+&#39;_PROJ&#39;
    points_df[xCOLOUT] = points_df[&#39;geometry&#39;].x
    points_df[yCOLOUT] = points_df[&#39;geometry&#39;].y
    xData = np.array(points_df[xCOLOUT].values)
    yData = np.array(points_df[yCOLOUT].values)
    sampleArr=raster.sel(x=xData, y=yData, method=&#39;nearest&#39;).values
    sampleArr = np.diag(sampleArr)
    sampleDF = pd.DataFrame(sampleArr, columns=[new_col])
    points_df[new_col] = sampleDF[new_col]
    return points_df</code></pre>
</details>
</dd>
<dt id="w4h.sort_dataframe"><code class="name flex">
<span>def <span class="ident">sort_dataframe</span></span>(<span>df, sort_cols=['API_NUMBER', 'TOP'], remove_nans=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to sort dataframe by one or more columns.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe to be sorted</dd>
<dt><strong><code>sort_cols</code></strong> :&ensp;<code>str</code> or <code>list</code> of <code>str</code>, default <code>= ['API_NUMBER','TOP']</code></dt>
<dd>Name(s) of columns by which to sort dataframe, by default ['API_NUMBER','TOP']</dd>
<dt><strong><code>remove_nans</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether or not to remove nans in the process, by default True</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df_sorted</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Sorted dataframe</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sort_dataframe(df, sort_cols=[&#39;API_NUMBER&#39;,&#39;TOP&#39;], remove_nans=True):
    &#34;&#34;&#34;Function to sort dataframe by one or more columns.

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe to be sorted
    sort_cols : str or list of str, default = [&#39;API_NUMBER&#39;,&#39;TOP&#39;]
        Name(s) of columns by which to sort dataframe, by default [&#39;API_NUMBER&#39;,&#39;TOP&#39;]
    remove_nans : bool, default = True
        Whether or not to remove nans in the process, by default True

    Returns
    -------
    df_sorted : pandas.DataFrame
        Sorted dataframe
    &#34;&#34;&#34;
    #Sort columns for better processing later
    df_sorted = df.sort_values(sort_cols)
    df_sorted.reset_index(inplace=True, drop=True)
    if remove_nans:
        df_sorted = df_sorted[pd.notna(df_sorted[&#34;LITHOLOGY&#34;])]
    return df_sorted</code></pre>
</details>
</dd>
<dt id="w4h.specific_define"><code class="name flex">
<span>def <span class="ident">specific_define</span></span>(<span>df, terms_df, description_col='FORMATION', terms_col='DESCRIPTION', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to classify terms that have been specifically defined in the terms_df.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Input dataframe with unclassified well descriptions.</dd>
<dt><strong><code>terms_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing the classifications</dd>
<dt><strong><code>description_col</code></strong> :&ensp;<code>str</code>, default=<code>'FORMATION'</code></dt>
<dd>Column name in df containing the well descriptions, by default 'FORMATION'.</dd>
<dt><strong><code>terms_col</code></strong> :&ensp;<code>str</code>, default=<code>'FORMATION'</code></dt>
<dd>Column name in terms_df containing the classified descriptions, by default 'FORMATION'.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to print up results, by default False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df_Interps</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing the well descriptions and their matched classifications.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def specific_define(df, terms_df, description_col=&#39;FORMATION&#39;, terms_col=&#39;DESCRIPTION&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to classify terms that have been specifically defined in the terms_df.

    Parameters
    ----------
    df : pandas.DataFrame
        Input dataframe with unclassified well descriptions.
    terms_df : pandas.DataFrame
        Dataframe containing the classifications
    description_col : str, default=&#39;FORMATION&#39;
        Column name in df containing the well descriptions, by default &#39;FORMATION&#39;.
    terms_col : str, default=&#39;FORMATION&#39;
        Column name in terms_df containing the classified descriptions, by default &#39;FORMATION&#39;.
    verbose : bool, default=False
        Whether to print up results, by default False.

    Returns
    -------
    df_Interps : pandas.DataFrame
        Dataframe containing the well descriptions and their matched classifications.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if description_col != terms_col:
        terms_df.rename(columns={terms_col:description_col}, inplace=True)
        terms_col = description_col

    df[description_col] = df[description_col].astype(str)
    terms_df[terms_col] = terms_df[terms_col].astype(str)

    df[description_col] = df[description_col].str.casefold()
    terms_df[terms_col] = terms_df[terms_col].str.casefold()
    #df[&#39;FORMATION&#39;] = df[&#39;FORMATION&#39;].str.strip([&#39;.,:?\t\s&#39;])
    #terms_df[&#39;FORMATION&#39;] = terms_df[&#39;FORMATION&#39;].str.strip([&#39;.,:?\t\s&#39;])

    terms_df.drop_duplicates(subset=terms_col, keep=&#39;last&#39;, inplace=True)
    terms_df.reset_index(drop=True, inplace=True)
    
    df_Interps = pd.merge(df, terms_df.set_index(terms_col), on=description_col, how=&#39;left&#39;)
    df_Interps.rename(columns={description_col:&#39;FORMATION&#39;}, inplace=True)
    df_Interps[&#39;BEDROCK_FLAG&#39;] = df_Interps[&#39;LITHOLOGY&#39;] == &#39;BEDROCK&#39;
    
    if verbose:
        print(&#34;Records Classified with full search term: &#34;+str(int(df_Interps[df_Interps[&#39;CLASS_FLAG&#39;]==1][&#39;CLASS_FLAG&#39;].sum())))
        print(&#34;Records Classified with full search term: &#34;+str(round((df_Interps[df_Interps[&#39;CLASS_FLAG&#39;]==1][&#39;CLASS_FLAG&#39;].sum()/df_Interps.shape[0])*100,2))+&#34;% of data&#34;)

    return df_Interps</code></pre>
</details>
</dd>
<dt id="w4h.split_defined"><code class="name flex">
<span>def <span class="ident">split_defined</span></span>(<span>df, classification_col='CLASS_FLAG', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to split dataframe with well descriptions into two dataframes based on whether a row has been classified.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all the well descriptions</dd>
<dt><strong><code>classification_col</code></strong> :&ensp;<code>str</code>, default <code>= 'CLASS_FLAG'</code></dt>
<dd>Name of column containing the classification flag, by default 'CLASS_FLAG'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print results, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Two-item tuple</code> of <code>pandas.Dataframe</code></dt>
<dd>tuple[0] is dataframe containing classified data, tuple[1] is dataframe containing unclassified data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split_defined(df, classification_col=&#39;CLASS_FLAG&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to split dataframe with well descriptions into two dataframes based on whether a row has been classified.

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe containing all the well descriptions
    classification_col : str, default = &#39;CLASS_FLAG&#39;
        Name of column containing the classification flag, by default &#39;CLASS_FLAG&#39;
    verbose : bool, default = False
        Whether to print results, by default False
    log : bool, default = False
        Whether to log results to log file

    Returns
    -------
    Two-item tuple of pandas.Dataframe
        tuple[0] is dataframe containing classified data, tuple[1] is dataframe containing unclassified data.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    classifedDF= df[df[classification_col].notna()] #Already-classifed data
    searchDF = df[df[classification_col].isna()] #Unclassified data
    
    if verbose:
        print(str(searchDF.shape[0])+&#39; records unclassified; isolated into searchDF.&#39;)
    
    return classifedDF, searchDF</code></pre>
</details>
</dd>
<dt id="w4h.start_define"><code class="name flex">
<span>def <span class="ident">start_define</span></span>(<span>df, terms_df, description_col='FORMATION', terms_col='DESCRIPTION', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to classify descriptions according to starting substring. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all the well descriptions</dd>
<dt><strong><code>terms_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all the startswith substrings to use for searching</dd>
<dt><strong><code>description_col</code></strong> :&ensp;<code>str</code>, default <code>= 'FORMATION'</code></dt>
<dd>Name of column in df containing descriptions, by default 'FORMATION'</dd>
<dt><strong><code>terms_col</code></strong> :&ensp;<code>str</code>, default <code>= 'FORMATION'</code></dt>
<dd>Name of column in terms_df containing startswith substring to match with description_col, by default 'FORMATION'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print out results, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log results to log file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing the original data and new classifications</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_define(df, terms_df, description_col=&#39;FORMATION&#39;, terms_col=&#39;DESCRIPTION&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to classify descriptions according to starting substring. 

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe containing all the well descriptions
    terms_df : pandas.DataFrame
        Dataframe containing all the startswith substrings to use for searching
    description_col : str, default = &#39;FORMATION&#39;
        Name of column in df containing descriptions, by default &#39;FORMATION&#39;
    terms_col : str, default = &#39;FORMATION&#39;
        Name of column in terms_df containing startswith substring to match with description_col, by default &#39;FORMATION&#39;
    verbose : bool, default = False
        Whether to print out results, by default False
    log : bool, default = True
        Whether to log results to log file

    Returns
    -------
    df : pandas.DataFrame
        Dataframe containing the original data and new classifications
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if verbose:
        #Estimate when it will end, based on test run
        estTime = df.shape[0]/3054409 * 6 #It took about 6 minutes to classify data with entire dataframe. This estimates the fraction of that it will take
        nowTime = datetime.datetime.now()
        endTime = nowTime+datetime.timedelta(minutes=estTime)
        print(&#34;Start Term process should be done by {:d}:{:02d}&#34;.format(endTime.hour, endTime.minute))

    #First, for each startterm, find all results in df that start with, add classification flag, and add interpretation.
    for i,s in enumerate(terms_df[terms_col]):
        df[&#39;CLASS_FLAG&#39;].where(~df[description_col].str.startswith(s,na=False),4,inplace=True)
        df[&#39;LITHOLOGY&#39;].where(~df[description_col].str.startswith(s,na=False),terms_df.loc[i,&#39;LITHOLOGY&#39;],inplace=True)
    df[&#39;BEDROCK_FLAG&#39;].loc[df[&#34;LITHOLOGY&#34;] == &#39;BEDROCK&#39;]
    
    if verbose:
        print(&#34;Records classified with start search term: &#34;+str(int(df[&#39;CLASS_FLAG&#39;].count())))
        print(&#34;Records classified with start search term: &#34;+str(round((df[&#39;CLASS_FLAG&#39;].count()/df.shape[0])*100,2))+&#34;% of remaining data&#34;)
        #print(&#34;Records classified with both search terms: &#34;+str(round(((df[&#39;CLASS_FLAG&#39;].count()+specDF[&#39;CLASS_FLAG&#39;].count())/downholeData_Interps.shape[0])*100,2))+&#34;% of all data&#34;)
        #This step usually takes about 5-6 minutes
    return df</code></pre>
</details>
</dd>
<dt id="w4h.wildcard_define"><code class="name flex">
<span>def <span class="ident">wildcard_define</span></span>(<span>df, terms_df, description_col='FORMATION', terms_col='DESCRIPTION', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to classify descriptions according to any substring. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all the well descriptions</dd>
<dt><strong><code>terms_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing all the startswith substrings to use for searching</dd>
<dt><strong><code>description_col</code></strong> :&ensp;<code>str</code>, default <code>= 'FORMATION'</code></dt>
<dd>Name of column in df containing descriptions, by default 'FORMATION'</dd>
<dt><strong><code>terms_col</code></strong> :&ensp;<code>str</code>, default <code>= 'FORMATION'</code></dt>
<dd>Name of column in terms_df containing startswith substring to match with description_col, by default 'FORMATION'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print out results, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log results to log file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing the original data and new classifications</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wildcard_define(df, terms_df, description_col=&#39;FORMATION&#39;, terms_col=&#39;DESCRIPTION&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Function to classify descriptions according to any substring. 

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe containing all the well descriptions
    terms_df : pandas.DataFrame
        Dataframe containing all the startswith substrings to use for searching
    description_col : str, default = &#39;FORMATION&#39;
        Name of column in df containing descriptions, by default &#39;FORMATION&#39;
    terms_col : str, default = &#39;FORMATION&#39;
        Name of column in terms_df containing startswith substring to match with description_col, by default &#39;FORMATION&#39;
    verbose : bool, default = False
        Whether to print out results, by default False
    log : bool, default = True
        Whether to log results to log file

    Returns
    -------
    df : pandas.DataFrame
        Dataframe containing the original data and new classifications
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if verbose:
        #Estimate when it will end, based on test run
        estTime = df.shape[0]/3054409 * 6 #It took about 6 minutes to classify data with entire dataframe. This estimates the fraction of that it will take
        nowTime = datetime.datetime.now()
        endTime = nowTime+datetime.timedelta(minutes=estTime)
        print(&#34;Wildcard Term process should be done by (?) {:d}:{:02d}&#34;.format(endTime.hour, endTime.minute))

    #First, for each startterm, find all results in df that start with, add classification flag, and add interpretation.
    for i,s in enumerate(terms_df[terms_col]):
        df[&#39;CLASS_FLAG&#39;].where(~df[description_col].str.contains(s, case=False, regex=False, na=False), 5, inplace=True)
        df[&#39;LITHOLOGY&#39;].where(~df[description_col].str.contains(s, case=False, regex=False, na=False),terms_df.loc[i,&#39;LITHOLOGY&#39;],inplace=True)
    df[&#39;BEDROCK_FLAG&#39;].loc[df[&#34;LITHOLOGY&#34;] == &#39;BEDROCK&#39;]
    
    if verbose:
        print(&#34;Records classified with wildcard search term: &#34;+str(int(df[&#39;CLASS_FLAG&#39;].count())))
        print(&#34;Records classified with wildcard search term: &#34;+str(round((df[&#39;CLASS_FLAG&#39;].count()/df.shape[0])*100,2))+&#34;% of remaining data&#34;)
        #print(&#34;Records classified with both search terms: &#34;+str(round(((df[&#39;CLASS_FLAG&#39;].count()+specDF[&#39;CLASS_FLAG&#39;].count())/downholeData_Interps.shape[0])*100,2))+&#34;% of all data&#34;)
        #This step usually takes about 5-6 minutes
    return df</code></pre>
</details>
</dd>
<dt id="w4h.xyz_metadata_merge"><code class="name flex">
<span>def <span class="ident">xyz_metadata_merge</span></span>(<span>xyz, metadata, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Add elevation to header data file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>xyz</code></strong> :&ensp;<code>pandas.Dataframe</code></dt>
<dd>Contains elevation for the points</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>pandas dataframe</code></dt>
<dd>Header data file</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log results to log file, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>headerXYZData</code></strong> :&ensp;<code>pandas.Dataframe</code></dt>
<dd>Header dataset merged to get elevation values</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def xyz_metadata_merge(xyz, metadata, log=False):
    &#34;&#34;&#34;Add elevation to header data file.

    Parameters
    ----------
    xyz : pandas.Dataframe
        Contains elevation for the points
    metadata : pandas dataframe
        Header data file
    log : bool, default = False
        Whether to log results to log file, by default False

    Returns
    -------
    headerXYZData : pandas.Dataframe
        Header dataset merged to get elevation values

    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    headerXYZData = metadata.merge(xyz, how=&#39;left&#39;, on=&#39;API_NUMBER&#39;)
    headerXYZData.drop([&#39;LATITUDE_x&#39;, &#39;LONGITUDE_x&#39;], axis=1, inplace=True)
    headerXYZData.rename({&#39;LATITUDE_y&#39;:&#39;LATITUDE&#39;, &#39;LONGITUDE_y&#39;:&#39;LONGITUDE&#39;}, axis=1, inplace=True)
    return headerXYZData</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="w4h.classify" href="classify.html">w4h.classify</a></code></li>
<li><code><a title="w4h.clean" href="clean.html">w4h.clean</a></code></li>
<li><code><a title="w4h.core" href="core.html">w4h.core</a></code></li>
<li><code><a title="w4h.export" href="export.html">w4h.export</a></code></li>
<li><code><a title="w4h.layers" href="layers.html">w4h.layers</a></code></li>
<li><code><a title="w4h.mapping" href="mapping.html">w4h.mapping</a></code></li>
<li><code><a title="w4h.read" href="read.html">w4h.read</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="w4h.align_rasters" href="#w4h.align_rasters">align_rasters</a></code></li>
<li><code><a title="w4h.clip_gdf2study_area" href="#w4h.clip_gdf2study_area">clip_gdf2study_area</a></code></li>
<li><code><a title="w4h.combine_dataset" href="#w4h.combine_dataset">combine_dataset</a></code></li>
<li><code><a title="w4h.coords2geometry" href="#w4h.coords2geometry">coords2geometry</a></code></li>
<li><code><a title="w4h.define_dtypes" href="#w4h.define_dtypes">define_dtypes</a></code></li>
<li><code><a title="w4h.depth_define" href="#w4h.depth_define">depth_define</a></code></li>
<li><code><a title="w4h.export_dataframe" href="#w4h.export_dataframe">export_dataframe</a></code></li>
<li><code><a title="w4h.export_grids" href="#w4h.export_grids">export_grids</a></code></li>
<li><code><a title="w4h.export_undefined" href="#w4h.export_undefined">export_undefined</a></code></li>
<li><code><a title="w4h.file_setup" href="#w4h.file_setup">file_setup</a></code></li>
<li><code><a title="w4h.fill_unclassified" href="#w4h.fill_unclassified">fill_unclassified</a></code></li>
<li><code><a title="w4h.get_current_date" href="#w4h.get_current_date">get_current_date</a></code></li>
<li><code><a title="w4h.get_drift_thick" href="#w4h.get_drift_thick">get_drift_thick</a></code></li>
<li><code><a title="w4h.get_layer_depths" href="#w4h.get_layer_depths">get_layer_depths</a></code></li>
<li><code><a title="w4h.get_most_recent" href="#w4h.get_most_recent">get_most_recent</a></code></li>
<li><code><a title="w4h.get_search_terms" href="#w4h.get_search_terms">get_search_terms</a></code></li>
<li><code><a title="w4h.get_unique_wells" href="#w4h.get_unique_wells">get_unique_wells</a></code></li>
<li><code><a title="w4h.grid2study_area" href="#w4h.grid2study_area">grid2study_area</a></code></li>
<li><code><a title="w4h.layer_interp" href="#w4h.layer_interp">layer_interp</a></code></li>
<li><code><a title="w4h.layer_target_thick" href="#w4h.layer_target_thick">layer_target_thick</a></code></li>
<li><code><a title="w4h.logger_function" href="#w4h.logger_function">logger_function</a></code></li>
<li><code><a title="w4h.merge_lithologies" href="#w4h.merge_lithologies">merge_lithologies</a></code></li>
<li><code><a title="w4h.merge_tables" href="#w4h.merge_tables">merge_tables</a></code></li>
<li><code><a title="w4h.read_dict" href="#w4h.read_dict">read_dict</a></code></li>
<li><code><a title="w4h.read_dictionary_terms" href="#w4h.read_dictionary_terms">read_dictionary_terms</a></code></li>
<li><code><a title="w4h.read_grid" href="#w4h.read_grid">read_grid</a></code></li>
<li><code><a title="w4h.read_lithologies" href="#w4h.read_lithologies">read_lithologies</a></code></li>
<li><code><a title="w4h.read_model_grid" href="#w4h.read_model_grid">read_model_grid</a></code></li>
<li><code><a title="w4h.read_raw_csv" href="#w4h.read_raw_csv">read_raw_csv</a></code></li>
<li><code><a title="w4h.read_study_area" href="#w4h.read_study_area">read_study_area</a></code></li>
<li><code><a title="w4h.read_wcs" href="#w4h.read_wcs">read_wcs</a></code></li>
<li><code><a title="w4h.read_wms" href="#w4h.read_wms">read_wms</a></code></li>
<li><code><a title="w4h.read_xyz" href="#w4h.read_xyz">read_xyz</a></code></li>
<li><code><a title="w4h.remerge_data" href="#w4h.remerge_data">remerge_data</a></code></li>
<li><code><a title="w4h.remove_bad_depth" href="#w4h.remove_bad_depth">remove_bad_depth</a></code></li>
<li><code><a title="w4h.remove_no_depth" href="#w4h.remove_no_depth">remove_no_depth</a></code></li>
<li><code><a title="w4h.remove_no_formation" href="#w4h.remove_no_formation">remove_no_formation</a></code></li>
<li><code><a title="w4h.remove_no_topo" href="#w4h.remove_no_topo">remove_no_topo</a></code></li>
<li><code><a title="w4h.remove_nonlocated" href="#w4h.remove_nonlocated">remove_nonlocated</a></code></li>
<li><code><a title="w4h.run" href="#w4h.run">run</a></code></li>
<li><code><a title="w4h.sample_raster_points" href="#w4h.sample_raster_points">sample_raster_points</a></code></li>
<li><code><a title="w4h.sort_dataframe" href="#w4h.sort_dataframe">sort_dataframe</a></code></li>
<li><code><a title="w4h.specific_define" href="#w4h.specific_define">specific_define</a></code></li>
<li><code><a title="w4h.split_defined" href="#w4h.split_defined">split_defined</a></code></li>
<li><code><a title="w4h.start_define" href="#w4h.start_define">start_define</a></code></li>
<li><code><a title="w4h.wildcard_define" href="#w4h.wildcard_define">wildcard_define</a></code></li>
<li><code><a title="w4h.xyz_metadata_merge" href="#w4h.xyz_metadata_merge">xyz_metadata_merge</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>