<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>w4h.core API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>w4h.core</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import datetime
import logging
import os
import pathlib

import pandas as pd

import w4h

def run(well_data, well_data_cols=None, 
        metadata=None, well_metadata_cols=None, 
        layers = 9,
        description_col=&#39;FORMATION&#39;, top_col=&#39;TOP&#39;, bottom_col=&#39;BOTTOM&#39;, depth_type=&#39;depth&#39;,
        study_area=None, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, zcol=&#39;ELEVATION&#39;, idcol=&#39;API_NUMBER&#39;, output_crs=&#39;EPSG:4269&#39;,
        surf_elev_file=None, bedrock_elev_file=None, model_grid=None,
        lith_dict=None, lith_dict_start=None, lith_dict_wildcard=None,
        target_dict=None,
        target_name=&#39;CoarseFine&#39;,
        export_dir=None,
        verbose=False,
        log=False,
        **keyword_parameters):
    
    &#34;&#34;&#34;Function to run entire process with one line of code. 
    
    NOTE: verbose and log are boolean parameters used for most of the functions. verbose=True prints information to terminal, log=True logs to a file in the log_dir, which defaults to the export_dir

    Parameters
    ----------
    well_data : str or pathlib.Path obj
        Filepath to file or directory containing well data.
    well_data_cols : List or list-like
        Columns to 
    metadata : str or pathlib.Path object
        Filepath to file or directory containing well metadata, such as location and elevation.
    well_metadata_cols : List or list-like
        _description_
    layers : int, default = 9
        The number of layers in the model grid
    description_col : str, default = &#39;FORMATION&#39;
        Name of column containing geologic descriptions of the well interval. This column should be in well_data.
    top_col : str, default = &#39;TOP&#39;
        Name of column containing depth/elevation at top of well interval. This column should be in well_data.
    bottom_col : str, default = &#39;BOTTOM&#39;
        Name of column containing depth/elevation at bottom of well interval. This column should be in well_data.    
    depth_type : str, default = &#39;depth&#39;
        Whether values top_col or bottom_col refer to depth or elevation.
    study_area : str or pathlib.Path object, or geopandas.GeoDataFrame
        _description_
    xcol : str, default = &#39;LONGITUDE&#39; 
        Name of column containing x coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.
    ycol : str, default = &#39;LATITUDE&#39;
        Name of column containing y coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.
    zcol : str, default = &#39;ELEVATION&#39; 
        Name of column containing z coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.
    output_crs : crs definition accepted by pyproj, default = &#39;EPSG:4269&#39;
        CRS to output all of the data into
    surf_elev_file : str or pathlib.Path object
        _description_
    bedrock_elev_file : str or pathlib.Path object
        _description_
    model_grid : str or pathlib.Path object, or model grid parameters (see model_grid function)
        _description_
    lith_dict : str or pathlib.Path object, or pandas.DataFrame
        _description_
    lith_dict_start : str or pathlib.Path object, or pandas.DataFrame
        _description_
    lith_dict_wildcard : str or pathlib.Path object, or pandas.DataFrame
        _description_
    target_dict : str or pathlib.Path object, or pandas.DataFrame
        _description_
    target_name : str, default = &#39;CoarseFine&#39;
        Name of target of interest, to be used on exported files
    export_dir : str or pathlib.Path object, default = None
        Directory to export output files
    verbose : bool, default = False
        Whether to print updates/results
    log : bool, default = False
        Whether to send parameters and outputs to log file, to be saved in export_dir, or the same directory as well_data if export_dir not defined.
    **keyword_parameters
        Keyword parameters used by any of the functions throughout the process. See list of functions above, and the API documentation for their possible parameters
    &#34;&#34;&#34;

    #Get data (files or otherwise)
    file_setup_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.file_setup.__code__.co_varnames}
    
    #Check how well_data and metadata were defined
    if isinstance(well_data, pathlib.PurePath) or isinstance(well_data, str):
        #Convert well_data to pathlib.Path if not already
        if isinstance(well_data, str):
            well_data = pathlib.Path(well_data)

        if metadata is None:
            if well_data.is_dir():
                downholeDataPATH, headerDataPATH = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             
            elif well_data.exists():
                downholeDataPATH, _ = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             
            else:
                print(&#39;ERROR: well_data file does not exist:{}&#39;.format(well_data))
        elif isinstance(metadata, pathlib.PurePath) or isinstance(metadata, str):
            if isinstance(metadata, str):
                metadata = pathlib.Path(metadata)    
            downholeDataPATH, headerDataPATH = w4h.file_setup(well_data=well_data, metadata=metadata, **file_setup_kwargs)                
        else:
            if isinstance(metadata, pd.DataFrame):
                downholeDataPATH, _ = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             
                headerDataPATH = metadata
            elif metadata is None:
                downholeDataPATH, _ = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             

    elif isinstance(well_data, pd.DataFrame):
        if isinstance(metadata, pd.DataFrame):
            downholeDataPATH = well_data
            headerDataPATH = metadata
        elif isinstance(metadata, pathlib.PurePath) or isinstance(metadata, str):
            _, headerDataPATH = w4h.file_setup(well_data=metadata, metadata=metadata, verbose=verbose, log=log, **file_setup_kwargs)                
            downholeDataPATH = well_data
        else:
            print(&#39;ERROR: metadata must be a string filepath, a pathlib.Path object, or pandas.DataFrame&#39;)
    else:
        print(&#39;ERROR: well_data must be a string filepath, a pathlib.Path object, or pandas.DataFrame&#39;)

    #Get pandas dataframes from input
    read_raw_txt_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_raw_csv .__code__.co_varnames}
    downholeDataIN, headerDataIN = w4h.read_raw_csv(data_filepath=downholeDataPATH, metadata_filepath=headerDataPATH, verbose=verbose, log=log, **read_raw_txt_kwargs) 
    #Functions to read data into dataframes. Also excludes extraneous columns, and drops header data with no location information

    #Define data types (file will need to be udpated)
    downholeData = w4h.define_dtypes(undefined_df=downholeDataIN, datatypes=&#39;./resources/downholeDataTypes.txt&#39;, verbose=verbose, log=log)
    headerData = w4h.define_dtypes(undefined_df=headerDataIN, datatypes=&#39;./resources/headerDataTypes.txt&#39;, verbose=verbose, log=log)

    #Get Study area
    read_study_area_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_study_area.__code__.co_varnames}
    if study_area is None:
        studyAreaIN = None
        use_study_area = False
    else:
        studyAreaIN = w4h.read_study_area(study_area_path=study_area, log=log, **read_study_area_kwargs)
        use_study_area = True

    #Get surfaces and grid(s)
    read_grid_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_grid.__code__.co_varnames}

    modelGridPath = model_grid
    surfaceElevPath = surf_elev_file
    bedrockElevPath = bedrock_elev_file
    #UPDATE: allow other types of model grid read ***
    modelGrid = w4h.read_grid(grid_path=modelGridPath, grid_type=&#39;model&#39;, study_area=studyAreaIN, verbose=verbose, log=log, **read_grid_kwargs)
    surfaceElevGridIN = w4h.read_grid(grid_path=surfaceElevPath, grid_type=&#39;surface&#39;, study_area=studyAreaIN, verbose=verbose, log=log, **read_grid_kwargs)
    bedrockElevGridIN = w4h.read_grid(grid_path=bedrockElevPath, grid_type=&#39;bedrock&#39;, study_area=studyAreaIN, verbose=verbose, log=log, **read_grid_kwargs)

    #Add control points
    #UPDATE: Code here for adding in control points ***

    #UPDATE: MAKE SURE CRS&#39;s all align ***

    #Convert headerData to have geometry
    coords2geometry_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.coords2geometry.__code__.co_varnames}
    headerData = w4h.coords2geometry(df=headerData, xcol=xcol, ycol=ycol, zcol=zcol, log=log, **coords2geometry_kwargs)
    clip_gdf2study_area_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.clip_gdf2study_area.__code__.co_varnames}
    headerData = w4h.clip_gdf2study_area(study_area=studyAreaIN, gdf=headerData, log=log, **clip_gdf2study_area_kwargs)

    #Clean up data
    downholeData = w4h.remove_nonlocated(downholeData, headerData, log=log, verbose=verbose)
    headerData = w4h.remove_no_topo(df=headerData, zcol=zcol, verbose=verbose, log=log)

    remove_no_depth_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.remove_no_depth.__code__.co_varnames}
    donwholeData = w4h.remove_no_depth(downholeData, verbose=verbose, top_col=top_col, bottom_col=bottom_col, log=log, **remove_no_depth_kwargs) #Drop records with no depth information

    remove_bad_depth_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.remove_bad_depth.__code__.co_varnames}
    donwholeData = w4h.remove_bad_depth(downholeData, verbose=verbose, top_col=top_col, bottom_col=bottom_col, depth_type=depth_type, log=log, **remove_bad_depth_kwargs)#Drop records with bad depth information (i.e., top depth &gt; bottom depth) (Also calculates thickness of each record)

    remove_no_formation_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.remove_no_formation.__code__.co_varnames}
    downholeData = w4h.remove_no_formation(downholeData, description_col=description_col, verbose=verbose, log=log, **remove_no_formation_kwargs)

    #CLASSIFICATION
    #Read dictionary definitions and classify
    get_search_terms_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.get_search_terms.__code__.co_varnames}
    specTermsPATH, startTermsPATH, wildcardTermsPATH, = w4h.get_search_terms(spec_path=lith_dict, start_path=lith_dict_start, wildcard_path=lith_dict_wildcard, log=log, **get_search_terms_kwargs)
    read_dictionary_terms_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_dictionary_terms.__code__.co_varnames}
    specTerms = w4h.read_dictionary_terms(dict_file=specTermsPATH, log=log, **read_dictionary_terms_kwargs)
    startTerms = w4h.read_dictionary_terms(dict_file=startTermsPATH, log=log, **read_dictionary_terms_kwargs)
    wildcardTerms = w4h.read_dictionary_terms(dict_file=wildcardTermsPATH, log=log, **read_dictionary_terms_kwargs)

    #Clean up dictionary terms
    specTerms.drop_duplicates(subset=&#39;DESCRIPTION&#39;, inplace=True)
    specTerms.reset_index(inplace=True, drop=True)

    startTerms.drop_duplicates(subset=&#39;DESCRIPTION&#39;, inplace=True)
    startTerms.reset_index(inplace=True, drop=True)

    wildcardTerms.drop_duplicates(subset=&#39;DESCRIPTION&#39;, inplace=True)
    wildcardTerms.reset_index(inplace=True, drop=True)

    if verbose:
        print(&#39;Search terms to be used:&#39;)
        print(&#39;\t {} exact match term/definition pairs&#39;)
        print(&#39;\t {} starting match term/definition pairs&#39;)
        print(&#39;\t {} wildcard match term/definition pairs&#39;)

    #CLASSIFICATIONS
    #Exact match classifications
    downholeData = w4h.specific_define(downholeData, specTerms, description_col=description_col, verbose=verbose, log=log)
    
    #.startswith classifications
    classifedDF, searchDF = w4h.split_defined(downholeData, verbose=verbose, log=log)
    searchDF = w4h.start_define(df=searchDF, terms_df=startTerms, description_col=description_col, verbose=verbose, log=log)
    downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF) #UPDATE: Needed? ***    

    #wildcard/any substring match classifications    
    classifedDF, searchDF = w4h.split_defined(downholeData, verbose=verbose, log=log)
    searchDF = w4h.wildcard_define(df=searchDF, terms_df=wildcardTerms, description_col=description_col, verbose=verbose, log=log)
    downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF) #UPDATE: Needed? ***    

    #Depth classification
    classifedDF, searchDF = w4h.split_defined(downholeData, verbose=verbose, log=log)
    searchDF = w4h.depth_define(searchDF, thresh=550, verbose=verbose, log=log)
    downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF) #UPDATE: Needed? ***
    
    #Fill unclassified data
    downholeData = w4h.fill_unclassified(downholeData, classification_col=&#39;CLASS_FLAG&#39;)
    
    #Add target interpratations
    read_lithologies_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_lithologies.__code__.co_varnames}
    targetInterpDF = w4h.read_lithologies(lith_file=target_dict, log=log, **read_lithologies_kwargs)
    downholeData = w4h.merge_lithologies(df=downholeData, targinterps_df=targetInterpDF, target_col=&#39;TARGET&#39;, target_class=&#39;bool&#39;)
    
    #Sort dataframe to prepare for next steps
    #downholeData = w4h.sort_dataframe(df=downholeData, sort_cols=[&#39;API_NUMBER&#39;,&#39;TOP&#39;], remove_nans=True)
    downholeData = downholeData.sort_values(sort_cols=[idcol, top_col])
    downholeData.reset_index(inplace=True, drop=True)
    #UPDATE: Option to remove nans?
    downholeData = downholeData[pd.notna(downholeData[&#34;LITHOLOGY&#34;])]

    #Analyze Surface(s) and grid(s)
    bedrockGrid, surfaceGrid = w4h.align_rasters(grids_unaligned=[bedrockElevGridIN, surfaceElevGridIN], modelgrid=modelGrid, no_data_val=0, log=log)
    driftThickGrid, layerThickGrid = w4h.get_drift_thick(surface=surfaceGrid, bedrock=bedrockGrid, layers=layers, plot=verbose, log=log)
    #UPDATE: LAYER NAMES SO DON&#34;T INCLUDE FT
    headerData = w4h.sample_raster_points(raster=bedrockGrid, points_df=headerData, xcol=xcol, ycol=ycol, new_col=&#39;BEDROCK_ELEV_FT&#39;, verbose=verbose, log=log)
    headerData = w4h.sample_raster_points(raster=surfaceGrid, points_df=headerData, xcol=xcol, ycol=ycol, new_col=&#39;SURFACE_ELEV_FT&#39;, verbose=verbose, log=log)
    headerData = w4h.sample_raster_points(raster=driftThickGrid, points_df=headerData, xcol=xcol, ycol=ycol, new_col=&#39;BEDROCK_DEPTH_FT&#39;, verbose=verbose, log=log)
    headerData = w4h.sample_raster_points(raster=layerThickGrid, points_df=headerData, xcol=xcol, ycol=ycol, new_col=&#39;LAYER_THICK_FT&#39;, verbose=verbose, log=log)
    headerData = w4h.get_layer_depths(metadata=headerData, no_layers=layers, log=log)

    #UPDATE: Check if this actually works, I think they should be copies of each other if metadata is not specified and not found using the metadata_filename pattern in file_setup() ***
    #Merge header and data into one df, if applicable
    if downholeData.values.base is headerData.values.base:
        pass
    else:
        #downholeData = pd.merge(left = downholeData, right = headerData, on=idcol)
        downholeData = w4h.merge_tables(data_df=downholeData,  header_df=headerData, data_cols=None, header_cols=None, on=idcol, how=&#39;inner&#39;, auto_pick_cols=True, log=log)
    
    #UPDATE: START HERE AGAIN, double checck and get kwargs for all functions ***
    #downholeData = downholeData.copy()
    #UPDATE: Potentially need to remove duplicate columns here, I think I fixed that tho ***
    resdf = w4h.layer_target_thick(downholeData, layers=9, return_all=False, outfile_prefix=&#39;CoarseFine&#39;, export_dir=export_dir, depth_top_col=top_col, depth_bot_col=bottom_col, log=log)
    layers_data = w4h.layer_interp(points=resdf, layers=9, grid=modelGrid, method=&#39;lin&#39;, log=log)

    if export_dir is None:
        if well_data.is_dir():
            export_dir = well_data.joinpath(&#39;Output&#39;)
        else:
            export_dir = well_data.parent.joinpath()
        
        if not export_dir.exists():
            try:
                export_dir.mkdir()
            except:
                pass

    w4h.export_grids(grid_data=layers_data, out_path=export_dir, file_id=&#39;&#39;,filetype=&#39;tif&#39;, variable_sep=True, date_stamp=True, log=log)
    #UPDATE: export points?
    return resdf, layers_data


log_filename=None #Set up so exists but is None
def logger_function(logtocommence, parameters, func_name):
    &#34;&#34;&#34;Function to log other functions, to be called from within other functions

    Parameters
    ----------
    logtocommence : bool
        Whether to perform logging steps
    parameters : dict
        Dictionary containing parameters and their values, from function
    func_name : str
        Name of function within which this is called
    &#34;&#34;&#34;
    if logtocommence:
        global log_filename
        #log parameter should be false by default on all. If true, will show up in kwargs
        
        #Get the log parameter value
        if &#39;log&#39; in parameters.keys():
            log_file = parameters.pop(&#39;log&#39;, None)
        else:
            #If it wasn&#39;t set, default to None
            log_file = None
        
        #Get currenet time and setup format for log messages
        curr_time = datetime.datetime.now()
        FORMAT = &#39;%(asctime)s  %(message)s&#39;

        #Check if we are starting a new logfile (only does this during run of file_setup() or (currently non-existent) new_logfile() functions)
        if log_file == True and (func_name == &#39;file_setup&#39; or func_name == &#39;new_logfile&#39;):

            #Get the log_dir variable set as a file_setup() parameter, or default to None if not specified
            out_dir = parameters.pop(&#39;log_dir&#39;, None)
            if out_dir is None:
                #If output directory not specified, default to the input directory
                out_dir = parameters[&#39;well_data&#39;]
            
            #Get the timestamp for the filename (this won&#39;t change, so represents the start of logging)
            timestamp = curr_time.strftime(&#39;%Y-%m-%d_%H-%M-%S&#39;)
            log_filename = pathlib.Path(out_dir).joinpath(f&#34;log_{timestamp}.txt&#34;)
            if &#39;verbose&#39; in parameters.keys():
                print(&#39;Logging data to&#39;, log_filename)

            #Set up logging stream using logging module
            logging.basicConfig(filename=log_filename, level=logging.INFO, format=FORMAT, filemode=&#39;w&#39;)

            #Log 
            logging.info(f&#34;{func_name} CALLED WITH PARAMETERS:\n\t {parameters}&#34;)
        elif log_file == True:
            #Run this for functions that aren&#39;t setting up logging file
            if log_filename:
                #Get the log stream and log this function&#39;s call with parameters
                logging.basicConfig(filename=log_filename, level=logging.INFO, format=FORMAT)
                logging.info(f&#34;{func_name} CALLED WITH PARAMETERS: \n\t{parameters}&#34;)
            else:
                #If log file has not already been set up, set it up
                timestamp = curr_time.strftime(&#39;%Y-%m-%d_%H-%M-%S&#39;)
                log_filename = f&#34;log_{timestamp}.txt&#34;

                #Now, get the log stream and log this function&#39;s call with parameters
                logging.basicConfig(filename=log_filename, level=logging.INFO, format=FORMAT)
                logging.info(f&#34;{func_name} CALLED WITH PARAMETERS: \n\t{parameters}&#34;)
        else:
            #Don&#39;t log if log=False
            pass
    return

def __check_parameter_names(verbose=True):
    #Check parameters are unique
    import inspect
    import w4h
    import pandas as pd
    function_list = [w4h.file_setup,
                 w4h.read_raw_csv,
                 w4h.define_dtypes,
                 w4h.read_study_area,
                 w4h.read_grid,
                 w4h.add_control_points,
                 w4h.coords2geometry,
                 w4h.clip_gdf2study_area,
                 w4h.remove_nonlocated,
                 w4h.remove_no_topo,
                 w4h.remove_no_depth,
                 w4h.remove_bad_depth,
                 w4h.remove_no_formation,
                 w4h.get_search_terms,
                 w4h.read_dictionary_terms,
                 w4h.specific_define,
                 w4h.start_define,
                 w4h.wildcard_define,
                 w4h.depth_define,
                 w4h.fill_unclassified,
                 w4h.read_lithologies,
                 w4h.merge_lithologies,
                 w4h.align_rasters,
                 w4h.get_drift_thick,
                 w4h.sample_raster_points,
                 w4h.get_layer_depths,
                 w4h.layer_target_thick,
                 w4h.layer_interp,
                 w4h.export_grids]
    
    paramDF = pd.DataFrame()
    for f in function_list:
        currParamList = inspect.getfullargspec(f)[0]
        fList = []
        for p in currParamList:
            fList.append(f.__name__)
        currParamDF = pd.DataFrame({&#39;Function&#39;:fList, &#39;Parameter&#39;:currParamList})
        paramDF = pd.concat([paramDF, currParamDF])

    uniqueDF = paramDF.drop_duplicates(subset=&#39;Parameter&#39;).copy()

    for up in uniqueDF[&#39;Parameter&#39;]:
        if up != &#39;verbose&#39; and up!=&#39;log&#39;:
            matchDF = paramDF[paramDF[&#39;Parameter&#39;]==up].copy()
            if verbose:
                if matchDF.shape[0] &gt; 1:
                    print(matchDF)
    
    return paramDF</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="w4h.core.logger_function"><code class="name flex">
<span>def <span class="ident">logger_function</span></span>(<span>logtocommence, parameters, func_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to log other functions, to be called from within other functions</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>logtocommence</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to perform logging steps</dd>
<dt><strong><code>parameters</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary containing parameters and their values, from function</dd>
<dt><strong><code>func_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of function within which this is called</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def logger_function(logtocommence, parameters, func_name):
    &#34;&#34;&#34;Function to log other functions, to be called from within other functions

    Parameters
    ----------
    logtocommence : bool
        Whether to perform logging steps
    parameters : dict
        Dictionary containing parameters and their values, from function
    func_name : str
        Name of function within which this is called
    &#34;&#34;&#34;
    if logtocommence:
        global log_filename
        #log parameter should be false by default on all. If true, will show up in kwargs
        
        #Get the log parameter value
        if &#39;log&#39; in parameters.keys():
            log_file = parameters.pop(&#39;log&#39;, None)
        else:
            #If it wasn&#39;t set, default to None
            log_file = None
        
        #Get currenet time and setup format for log messages
        curr_time = datetime.datetime.now()
        FORMAT = &#39;%(asctime)s  %(message)s&#39;

        #Check if we are starting a new logfile (only does this during run of file_setup() or (currently non-existent) new_logfile() functions)
        if log_file == True and (func_name == &#39;file_setup&#39; or func_name == &#39;new_logfile&#39;):

            #Get the log_dir variable set as a file_setup() parameter, or default to None if not specified
            out_dir = parameters.pop(&#39;log_dir&#39;, None)
            if out_dir is None:
                #If output directory not specified, default to the input directory
                out_dir = parameters[&#39;well_data&#39;]
            
            #Get the timestamp for the filename (this won&#39;t change, so represents the start of logging)
            timestamp = curr_time.strftime(&#39;%Y-%m-%d_%H-%M-%S&#39;)
            log_filename = pathlib.Path(out_dir).joinpath(f&#34;log_{timestamp}.txt&#34;)
            if &#39;verbose&#39; in parameters.keys():
                print(&#39;Logging data to&#39;, log_filename)

            #Set up logging stream using logging module
            logging.basicConfig(filename=log_filename, level=logging.INFO, format=FORMAT, filemode=&#39;w&#39;)

            #Log 
            logging.info(f&#34;{func_name} CALLED WITH PARAMETERS:\n\t {parameters}&#34;)
        elif log_file == True:
            #Run this for functions that aren&#39;t setting up logging file
            if log_filename:
                #Get the log stream and log this function&#39;s call with parameters
                logging.basicConfig(filename=log_filename, level=logging.INFO, format=FORMAT)
                logging.info(f&#34;{func_name} CALLED WITH PARAMETERS: \n\t{parameters}&#34;)
            else:
                #If log file has not already been set up, set it up
                timestamp = curr_time.strftime(&#39;%Y-%m-%d_%H-%M-%S&#39;)
                log_filename = f&#34;log_{timestamp}.txt&#34;

                #Now, get the log stream and log this function&#39;s call with parameters
                logging.basicConfig(filename=log_filename, level=logging.INFO, format=FORMAT)
                logging.info(f&#34;{func_name} CALLED WITH PARAMETERS: \n\t{parameters}&#34;)
        else:
            #Don&#39;t log if log=False
            pass
    return</code></pre>
</details>
</dd>
<dt id="w4h.core.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>well_data, well_data_cols=None, metadata=None, well_metadata_cols=None, layers=9, description_col='FORMATION', top_col='TOP', bottom_col='BOTTOM', depth_type='depth', study_area=None, xcol='LONGITUDE', ycol='LATITUDE', zcol='ELEVATION', idcol='API_NUMBER', output_crs='EPSG:4269', surf_elev_file=None, bedrock_elev_file=None, model_grid=None, lith_dict=None, lith_dict_start=None, lith_dict_wildcard=None, target_dict=None, target_name='CoarseFine', export_dir=None, verbose=False, log=False, **keyword_parameters)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to run entire process with one line of code. </p>
<p>NOTE: verbose and log are boolean parameters used for most of the functions. verbose=True prints information to terminal, log=True logs to a file in the log_dir, which defaults to the export_dir</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>well_data</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path obj</code></dt>
<dd>Filepath to file or directory containing well data.</dd>
<dt><strong><code>well_data_cols</code></strong> :&ensp;<code>List</code> or <code>list-like</code></dt>
<dd>Columns to</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>Filepath to file or directory containing well metadata, such as location and elevation.</dd>
<dt><strong><code>well_metadata_cols</code></strong> :&ensp;<code>List</code> or <code>list-like</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>int</code>, default <code>= 9</code></dt>
<dd>The number of layers in the model grid</dd>
<dt><strong><code>description_col</code></strong> :&ensp;<code>str</code>, default <code>= 'FORMATION'</code></dt>
<dd>Name of column containing geologic descriptions of the well interval. This column should be in well_data.</dd>
<dt><strong><code>top_col</code></strong> :&ensp;<code>str</code>, default <code>= 'TOP'</code></dt>
<dd>Name of column containing depth/elevation at top of well interval. This column should be in well_data.</dd>
<dt><strong><code>bottom_col</code></strong> :&ensp;<code>str</code>, default <code>= 'BOTTOM'</code></dt>
<dd>Name of column containing depth/elevation at bottom of well interval. This column should be in well_data.</dd>
<dt><strong><code>depth_type</code></strong> :&ensp;<code>str</code>, default <code>= 'depth'</code></dt>
<dd>Whether values top_col or bottom_col refer to depth or elevation.</dd>
<dt><strong><code>study_area</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>geopandas.GeoDataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>xcol</code></strong> :&ensp;<code>str</code>, default <code>= 'LONGITUDE'</code></dt>
<dd>Name of column containing x coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default <code>= 'LATITUDE'</code></dt>
<dd>Name of column containing y coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.</dd>
<dt><strong><code>zcol</code></strong> :&ensp;<code>str</code>, default <code>= 'ELEVATION'</code></dt>
<dd>Name of column containing z coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.</dd>
<dt><strong><code>output_crs</code></strong> :&ensp;<code>crs definition accepted by pyproj</code>, default <code>= 'EPSG:4269'</code></dt>
<dd>CRS to output all of the data into</dd>
<dt><strong><code>surf_elev_file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>bedrock_elev_file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>model_grid</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>model grid parameters (see model_grid function)</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>lith_dict</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>pandas.DataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>lith_dict_start</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>pandas.DataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>lith_dict_wildcard</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>pandas.DataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>target_dict</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>pandas.DataFrame</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>target_name</code></strong> :&ensp;<code>str</code>, default <code>= 'CoarseFine'</code></dt>
<dd>Name of target of interest, to be used on exported files</dd>
<dt><strong><code>export_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, default <code>= None</code></dt>
<dd>Directory to export output files</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print updates/results</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to send parameters and outputs to log file, to be saved in export_dir, or the same directory as well_data if export_dir not defined.</dd>
<dt><strong><code>**keyword_parameters</code></strong></dt>
<dd>Keyword parameters used by any of the functions throughout the process. See list of functions above, and the API documentation for their possible parameters</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(well_data, well_data_cols=None, 
        metadata=None, well_metadata_cols=None, 
        layers = 9,
        description_col=&#39;FORMATION&#39;, top_col=&#39;TOP&#39;, bottom_col=&#39;BOTTOM&#39;, depth_type=&#39;depth&#39;,
        study_area=None, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, zcol=&#39;ELEVATION&#39;, idcol=&#39;API_NUMBER&#39;, output_crs=&#39;EPSG:4269&#39;,
        surf_elev_file=None, bedrock_elev_file=None, model_grid=None,
        lith_dict=None, lith_dict_start=None, lith_dict_wildcard=None,
        target_dict=None,
        target_name=&#39;CoarseFine&#39;,
        export_dir=None,
        verbose=False,
        log=False,
        **keyword_parameters):
    
    &#34;&#34;&#34;Function to run entire process with one line of code. 
    
    NOTE: verbose and log are boolean parameters used for most of the functions. verbose=True prints information to terminal, log=True logs to a file in the log_dir, which defaults to the export_dir

    Parameters
    ----------
    well_data : str or pathlib.Path obj
        Filepath to file or directory containing well data.
    well_data_cols : List or list-like
        Columns to 
    metadata : str or pathlib.Path object
        Filepath to file or directory containing well metadata, such as location and elevation.
    well_metadata_cols : List or list-like
        _description_
    layers : int, default = 9
        The number of layers in the model grid
    description_col : str, default = &#39;FORMATION&#39;
        Name of column containing geologic descriptions of the well interval. This column should be in well_data.
    top_col : str, default = &#39;TOP&#39;
        Name of column containing depth/elevation at top of well interval. This column should be in well_data.
    bottom_col : str, default = &#39;BOTTOM&#39;
        Name of column containing depth/elevation at bottom of well interval. This column should be in well_data.    
    depth_type : str, default = &#39;depth&#39;
        Whether values top_col or bottom_col refer to depth or elevation.
    study_area : str or pathlib.Path object, or geopandas.GeoDataFrame
        _description_
    xcol : str, default = &#39;LONGITUDE&#39; 
        Name of column containing x coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.
    ycol : str, default = &#39;LATITUDE&#39;
        Name of column containing y coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.
    zcol : str, default = &#39;ELEVATION&#39; 
        Name of column containing z coordinates. This column should be in metadata unless metadata is not read, then it should be in well_data.
    output_crs : crs definition accepted by pyproj, default = &#39;EPSG:4269&#39;
        CRS to output all of the data into
    surf_elev_file : str or pathlib.Path object
        _description_
    bedrock_elev_file : str or pathlib.Path object
        _description_
    model_grid : str or pathlib.Path object, or model grid parameters (see model_grid function)
        _description_
    lith_dict : str or pathlib.Path object, or pandas.DataFrame
        _description_
    lith_dict_start : str or pathlib.Path object, or pandas.DataFrame
        _description_
    lith_dict_wildcard : str or pathlib.Path object, or pandas.DataFrame
        _description_
    target_dict : str or pathlib.Path object, or pandas.DataFrame
        _description_
    target_name : str, default = &#39;CoarseFine&#39;
        Name of target of interest, to be used on exported files
    export_dir : str or pathlib.Path object, default = None
        Directory to export output files
    verbose : bool, default = False
        Whether to print updates/results
    log : bool, default = False
        Whether to send parameters and outputs to log file, to be saved in export_dir, or the same directory as well_data if export_dir not defined.
    **keyword_parameters
        Keyword parameters used by any of the functions throughout the process. See list of functions above, and the API documentation for their possible parameters
    &#34;&#34;&#34;

    #Get data (files or otherwise)
    file_setup_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.file_setup.__code__.co_varnames}
    
    #Check how well_data and metadata were defined
    if isinstance(well_data, pathlib.PurePath) or isinstance(well_data, str):
        #Convert well_data to pathlib.Path if not already
        if isinstance(well_data, str):
            well_data = pathlib.Path(well_data)

        if metadata is None:
            if well_data.is_dir():
                downholeDataPATH, headerDataPATH = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             
            elif well_data.exists():
                downholeDataPATH, _ = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             
            else:
                print(&#39;ERROR: well_data file does not exist:{}&#39;.format(well_data))
        elif isinstance(metadata, pathlib.PurePath) or isinstance(metadata, str):
            if isinstance(metadata, str):
                metadata = pathlib.Path(metadata)    
            downholeDataPATH, headerDataPATH = w4h.file_setup(well_data=well_data, metadata=metadata, **file_setup_kwargs)                
        else:
            if isinstance(metadata, pd.DataFrame):
                downholeDataPATH, _ = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             
                headerDataPATH = metadata
            elif metadata is None:
                downholeDataPATH, _ = w4h.file_setup(well_data=well_data, verbose=verbose, log=log, **file_setup_kwargs)             

    elif isinstance(well_data, pd.DataFrame):
        if isinstance(metadata, pd.DataFrame):
            downholeDataPATH = well_data
            headerDataPATH = metadata
        elif isinstance(metadata, pathlib.PurePath) or isinstance(metadata, str):
            _, headerDataPATH = w4h.file_setup(well_data=metadata, metadata=metadata, verbose=verbose, log=log, **file_setup_kwargs)                
            downholeDataPATH = well_data
        else:
            print(&#39;ERROR: metadata must be a string filepath, a pathlib.Path object, or pandas.DataFrame&#39;)
    else:
        print(&#39;ERROR: well_data must be a string filepath, a pathlib.Path object, or pandas.DataFrame&#39;)

    #Get pandas dataframes from input
    read_raw_txt_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_raw_csv .__code__.co_varnames}
    downholeDataIN, headerDataIN = w4h.read_raw_csv(data_filepath=downholeDataPATH, metadata_filepath=headerDataPATH, verbose=verbose, log=log, **read_raw_txt_kwargs) 
    #Functions to read data into dataframes. Also excludes extraneous columns, and drops header data with no location information

    #Define data types (file will need to be udpated)
    downholeData = w4h.define_dtypes(undefined_df=downholeDataIN, datatypes=&#39;./resources/downholeDataTypes.txt&#39;, verbose=verbose, log=log)
    headerData = w4h.define_dtypes(undefined_df=headerDataIN, datatypes=&#39;./resources/headerDataTypes.txt&#39;, verbose=verbose, log=log)

    #Get Study area
    read_study_area_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_study_area.__code__.co_varnames}
    if study_area is None:
        studyAreaIN = None
        use_study_area = False
    else:
        studyAreaIN = w4h.read_study_area(study_area_path=study_area, log=log, **read_study_area_kwargs)
        use_study_area = True

    #Get surfaces and grid(s)
    read_grid_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_grid.__code__.co_varnames}

    modelGridPath = model_grid
    surfaceElevPath = surf_elev_file
    bedrockElevPath = bedrock_elev_file
    #UPDATE: allow other types of model grid read ***
    modelGrid = w4h.read_grid(grid_path=modelGridPath, grid_type=&#39;model&#39;, study_area=studyAreaIN, verbose=verbose, log=log, **read_grid_kwargs)
    surfaceElevGridIN = w4h.read_grid(grid_path=surfaceElevPath, grid_type=&#39;surface&#39;, study_area=studyAreaIN, verbose=verbose, log=log, **read_grid_kwargs)
    bedrockElevGridIN = w4h.read_grid(grid_path=bedrockElevPath, grid_type=&#39;bedrock&#39;, study_area=studyAreaIN, verbose=verbose, log=log, **read_grid_kwargs)

    #Add control points
    #UPDATE: Code here for adding in control points ***

    #UPDATE: MAKE SURE CRS&#39;s all align ***

    #Convert headerData to have geometry
    coords2geometry_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.coords2geometry.__code__.co_varnames}
    headerData = w4h.coords2geometry(df=headerData, xcol=xcol, ycol=ycol, zcol=zcol, log=log, **coords2geometry_kwargs)
    clip_gdf2study_area_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.clip_gdf2study_area.__code__.co_varnames}
    headerData = w4h.clip_gdf2study_area(study_area=studyAreaIN, gdf=headerData, log=log, **clip_gdf2study_area_kwargs)

    #Clean up data
    downholeData = w4h.remove_nonlocated(downholeData, headerData, log=log, verbose=verbose)
    headerData = w4h.remove_no_topo(df=headerData, zcol=zcol, verbose=verbose, log=log)

    remove_no_depth_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.remove_no_depth.__code__.co_varnames}
    donwholeData = w4h.remove_no_depth(downholeData, verbose=verbose, top_col=top_col, bottom_col=bottom_col, log=log, **remove_no_depth_kwargs) #Drop records with no depth information

    remove_bad_depth_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.remove_bad_depth.__code__.co_varnames}
    donwholeData = w4h.remove_bad_depth(downholeData, verbose=verbose, top_col=top_col, bottom_col=bottom_col, depth_type=depth_type, log=log, **remove_bad_depth_kwargs)#Drop records with bad depth information (i.e., top depth &gt; bottom depth) (Also calculates thickness of each record)

    remove_no_formation_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.remove_no_formation.__code__.co_varnames}
    downholeData = w4h.remove_no_formation(downholeData, description_col=description_col, verbose=verbose, log=log, **remove_no_formation_kwargs)

    #CLASSIFICATION
    #Read dictionary definitions and classify
    get_search_terms_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.get_search_terms.__code__.co_varnames}
    specTermsPATH, startTermsPATH, wildcardTermsPATH, = w4h.get_search_terms(spec_path=lith_dict, start_path=lith_dict_start, wildcard_path=lith_dict_wildcard, log=log, **get_search_terms_kwargs)
    read_dictionary_terms_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_dictionary_terms.__code__.co_varnames}
    specTerms = w4h.read_dictionary_terms(dict_file=specTermsPATH, log=log, **read_dictionary_terms_kwargs)
    startTerms = w4h.read_dictionary_terms(dict_file=startTermsPATH, log=log, **read_dictionary_terms_kwargs)
    wildcardTerms = w4h.read_dictionary_terms(dict_file=wildcardTermsPATH, log=log, **read_dictionary_terms_kwargs)

    #Clean up dictionary terms
    specTerms.drop_duplicates(subset=&#39;DESCRIPTION&#39;, inplace=True)
    specTerms.reset_index(inplace=True, drop=True)

    startTerms.drop_duplicates(subset=&#39;DESCRIPTION&#39;, inplace=True)
    startTerms.reset_index(inplace=True, drop=True)

    wildcardTerms.drop_duplicates(subset=&#39;DESCRIPTION&#39;, inplace=True)
    wildcardTerms.reset_index(inplace=True, drop=True)

    if verbose:
        print(&#39;Search terms to be used:&#39;)
        print(&#39;\t {} exact match term/definition pairs&#39;)
        print(&#39;\t {} starting match term/definition pairs&#39;)
        print(&#39;\t {} wildcard match term/definition pairs&#39;)

    #CLASSIFICATIONS
    #Exact match classifications
    downholeData = w4h.specific_define(downholeData, specTerms, description_col=description_col, verbose=verbose, log=log)
    
    #.startswith classifications
    classifedDF, searchDF = w4h.split_defined(downholeData, verbose=verbose, log=log)
    searchDF = w4h.start_define(df=searchDF, terms_df=startTerms, description_col=description_col, verbose=verbose, log=log)
    downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF) #UPDATE: Needed? ***    

    #wildcard/any substring match classifications    
    classifedDF, searchDF = w4h.split_defined(downholeData, verbose=verbose, log=log)
    searchDF = w4h.wildcard_define(df=searchDF, terms_df=wildcardTerms, description_col=description_col, verbose=verbose, log=log)
    downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF) #UPDATE: Needed? ***    

    #Depth classification
    classifedDF, searchDF = w4h.split_defined(downholeData, verbose=verbose, log=log)
    searchDF = w4h.depth_define(searchDF, thresh=550, verbose=verbose, log=log)
    downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF) #UPDATE: Needed? ***
    
    #Fill unclassified data
    downholeData = w4h.fill_unclassified(downholeData, classification_col=&#39;CLASS_FLAG&#39;)
    
    #Add target interpratations
    read_lithologies_kwargs = {k: v for k, v in locals()[&#39;keyword_parameters&#39;].items() if k in w4h.read_lithologies.__code__.co_varnames}
    targetInterpDF = w4h.read_lithologies(lith_file=target_dict, log=log, **read_lithologies_kwargs)
    downholeData = w4h.merge_lithologies(df=downholeData, targinterps_df=targetInterpDF, target_col=&#39;TARGET&#39;, target_class=&#39;bool&#39;)
    
    #Sort dataframe to prepare for next steps
    #downholeData = w4h.sort_dataframe(df=downholeData, sort_cols=[&#39;API_NUMBER&#39;,&#39;TOP&#39;], remove_nans=True)
    downholeData = downholeData.sort_values(sort_cols=[idcol, top_col])
    downholeData.reset_index(inplace=True, drop=True)
    #UPDATE: Option to remove nans?
    downholeData = downholeData[pd.notna(downholeData[&#34;LITHOLOGY&#34;])]

    #Analyze Surface(s) and grid(s)
    bedrockGrid, surfaceGrid = w4h.align_rasters(grids_unaligned=[bedrockElevGridIN, surfaceElevGridIN], modelgrid=modelGrid, no_data_val=0, log=log)
    driftThickGrid, layerThickGrid = w4h.get_drift_thick(surface=surfaceGrid, bedrock=bedrockGrid, layers=layers, plot=verbose, log=log)
    #UPDATE: LAYER NAMES SO DON&#34;T INCLUDE FT
    headerData = w4h.sample_raster_points(raster=bedrockGrid, points_df=headerData, xcol=xcol, ycol=ycol, new_col=&#39;BEDROCK_ELEV_FT&#39;, verbose=verbose, log=log)
    headerData = w4h.sample_raster_points(raster=surfaceGrid, points_df=headerData, xcol=xcol, ycol=ycol, new_col=&#39;SURFACE_ELEV_FT&#39;, verbose=verbose, log=log)
    headerData = w4h.sample_raster_points(raster=driftThickGrid, points_df=headerData, xcol=xcol, ycol=ycol, new_col=&#39;BEDROCK_DEPTH_FT&#39;, verbose=verbose, log=log)
    headerData = w4h.sample_raster_points(raster=layerThickGrid, points_df=headerData, xcol=xcol, ycol=ycol, new_col=&#39;LAYER_THICK_FT&#39;, verbose=verbose, log=log)
    headerData = w4h.get_layer_depths(metadata=headerData, no_layers=layers, log=log)

    #UPDATE: Check if this actually works, I think they should be copies of each other if metadata is not specified and not found using the metadata_filename pattern in file_setup() ***
    #Merge header and data into one df, if applicable
    if downholeData.values.base is headerData.values.base:
        pass
    else:
        #downholeData = pd.merge(left = downholeData, right = headerData, on=idcol)
        downholeData = w4h.merge_tables(data_df=downholeData,  header_df=headerData, data_cols=None, header_cols=None, on=idcol, how=&#39;inner&#39;, auto_pick_cols=True, log=log)
    
    #UPDATE: START HERE AGAIN, double checck and get kwargs for all functions ***
    #downholeData = downholeData.copy()
    #UPDATE: Potentially need to remove duplicate columns here, I think I fixed that tho ***
    resdf = w4h.layer_target_thick(downholeData, layers=9, return_all=False, outfile_prefix=&#39;CoarseFine&#39;, export_dir=export_dir, depth_top_col=top_col, depth_bot_col=bottom_col, log=log)
    layers_data = w4h.layer_interp(points=resdf, layers=9, grid=modelGrid, method=&#39;lin&#39;, log=log)

    if export_dir is None:
        if well_data.is_dir():
            export_dir = well_data.joinpath(&#39;Output&#39;)
        else:
            export_dir = well_data.parent.joinpath()
        
        if not export_dir.exists():
            try:
                export_dir.mkdir()
            except:
                pass

    w4h.export_grids(grid_data=layers_data, out_path=export_dir, file_id=&#39;&#39;,filetype=&#39;tif&#39;, variable_sep=True, date_stamp=True, log=log)
    #UPDATE: export points?
    return resdf, layers_data</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="w4h" href="index.html">w4h</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="w4h.core.logger_function" href="#w4h.core.logger_function">logger_function</a></code></li>
<li><code><a title="w4h.core.run" href="#w4h.core.run">run</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>