<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>w4h.layers API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>w4h.layers</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import datetime
import inspect
import os
import pathlib

import rioxarray as rxr
import xarray as xr
import geopandas as gpd
import pandas as pd
import numpy as np
from shapely.geometry import Point
from scipy import interpolate

import w4h
from w4h import logger_function

#Function to Merge tables
def merge_tables(data_df, header_df, data_cols=None, header_cols=None, auto_pick_cols=False, drop_duplicate_cols=True, log=False, **kwargs):
    &#34;&#34;&#34;Function to merge tables, intended for merging metadata table with data table

    Parameters
    ----------
    data_df : pandas.DataFrame
        &#34;Left&#34; dataframe, intended for this purpose to be dataframe with main data, but can be anything
    header_df : pandas.DataFrame
        &#34;Right&#34; dataframe, intended for this purpose to be dataframe with metadata, but can be anything
    data_cols : list, optional
        List of strings of column names, for columns to be included after join from &#34;left&#34; table (data table). If None, all columns are kept, by default None
    header_cols : list, optional
        List of strings of columns names, for columns to be included in merged table after merge from &#34;right&#34; table (metadata). If None, all columns are kept, by default None
    auto_pick_cols : bool, default = False
        Whether to autopick the columns from the metadata table. If True, the following column names are kept:[&#39;API_NUMBER&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;BEDROCK_ELEV_FT&#39;, &#39;SURFACE_ELEV_FT&#39;, &#39;BEDROCK_DEPTH_FT&#39;, &#39;LAYER_THICK_FT&#39;], by default False
    drop_duplicate_cols : bool, optional
        If True, drops duplicate columns from the tables so that columns do not get renamed upon merge, by default True
    log : bool, default = False
        Whether to log inputs and outputs to log file.
    **kwargs
        kwargs that are passed directly to pd.merge(). By default, the &#39;on&#39; and &#39;how&#39; parameters are defined as on=&#39;API_NUMBER&#39; and how=&#39;inner&#39;

    Returns
    -------
    mergedTable : pandas.DataFrame
        Merged dataframe
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if auto_pick_cols:
        header_cols = [&#39;API_NUMBER&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;BEDROCK_ELEV_FT&#39;, &#39;SURFACE_ELEV_FT&#39;, &#39;BEDROCK_DEPTH_FT&#39;, &#39;LAYER_THICK_FT&#39;]
        for c in header_df.columns:
            if c.startswith(&#39;ELEV_&#39;) or c.startswith(&#39;DEPTH&#39;):
                header_cols.append(c)
            if &#39;_PROJ&#39; in c:
                header_cols.append(c)
        header_cols.append(&#39;geometry&#39;)
    elif header_cols is None:
        header_cols = header_df.columns
    else:
        header_cols = header_cols

    #If not specified, get all the cols
    if data_cols is None:
        data_cols = data_df.columns

    #Drop duplicate columns
    if drop_duplicate_cols:
        header_colCopy= header_cols.copy()
        remCount = 0
        for i, c in enumerate(header_colCopy):
            if c in data_cols and c != on:
                print(&#39;REMOVING&#39;, header_cols[i-remCount])
                header_cols.pop(i - remCount)
                remCount += 1

    leftTable_join = data_df[data_cols]
    rightTable_join = header_df[header_cols]

    #Defults for on and how
    if &#39;on&#39; not in kwargs.keys():
        kwargs[&#39;on&#39;]=&#39;API_NUMBER&#39;

    if &#39;how&#39; not in kwargs.keys():
        kwargs[&#39;how&#39;]=&#39;inner&#39;

    mergedTable = pd.merge(left=leftTable_join, right=rightTable_join, **kwargs)
    return mergedTable

#Get layer depths of each layer, based on precalculated layer thickness
def get_layer_depths(well_metadata, no_layers=9, log=False):
    &#34;&#34;&#34;Function to calculate depths and elevations of each model layer at each well based on surface elevation, bedrock elevation, and number of layers/layer thickness

    Parameters
    ----------
    well_metadata : pandas.DataFrame
        Dataframe containing well metdata
    no_layers : int, default=9
        Number of layers. This should correlate with get_drift_thick() input parameter, if drift thickness was calculated using that function, by default 9.
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    pandas.Dataframe
        Dataframe containing new columns for depth to layers and elevation of layers.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    for layer in range(0, no_layers): #For each layer
        #Make column names
        depthColName  = &#39;DEPTH_FT_LAYER&#39;+str(layer+1)
        #depthMcolName = &#39;Depth_M_LAYER&#39;+str(layer) 

        #Calculate depth to each layer at each well, in feet and meters
        well_metadata[depthColName]  = well_metadata[&#39;LAYER_THICK_FT&#39;] * layer
        #headerData[depthMcolName] = headerData[depthColName] * 0.3048

    for layer in range(0, no_layers): #For each layer
        elevColName = &#39;ELEV_FT_LAYER&#39;+str(layer+1)
        #elevMColName = &#39;ELEV_M_LAYER&#39;+str(layer)
            
        well_metadata[elevColName]  = well_metadata[&#39;SURFACE_ELEV_FT&#39;] - well_metadata[&#39;LAYER_THICK_FT&#39;] * layer
        #headerData[elevMColName]  = headerData[&#39;SURFACE_ELEV_M&#39;] - headerData[&#39;LAYER_THICK_M&#39;] * layer
    return well_metadata

#Function to export the result of thickness of target sediments in each layer
def layer_target_thick(df, layers=9, return_all=False, export_dir=None, outfile_prefix=&#39;&#39;, depth_top_col=&#39;TOP&#39;, depth_bot_col=&#39;BOTTOM&#39;, log=False):
    &#34;&#34;&#34;Function to calculate thickness of target material in each layer at each well point

    Parameters
    ----------
    df : geopandas.geodataframe
        Geodataframe containing classified data, surface elevation, bedrock elevation, layer depths, geometry.
    layers : int, default=9
        Number of layers in model, by default 9
    return_all : bool, default=False
        If True, return list of original geodataframes with extra column added for target thick for each layer.
        If False, return list of geopandas.geodataframes with only essential information for each layer.
    export_dir : str or pathlib.Path, default=None
        If str or pathlib.Path, should be directory to which to export dataframes built in function.
    outfile_prefix : str, default=&#39;&#39;
        Only used if export_dir is set. Will be used at the start of the exported filenames
    depth_top_col : str, default=&#39;TOP&#39;
        Name of column containing data for depth to top of described well intervals
    depth_bot_col : str, default=&#39;BOTTOM&#39;
        Name of column containing data for depth to bottom of described well intervals
    log : bool, default = True
        Whether to log inputs and outputs to log file.
    
    Returns
    -------
    res_df or res : geopandas.geodataframe
        Geopandas geodataframe containing only important information needed for next stage of analysis.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    df[&#39;TOP_ELEV_FT&#39;] = df[&#39;SURFACE_ELEV_FT&#39;] - df[depth_top_col]
    df[&#39;BOT_ELEV_FT&#39;] = df[&#39;SURFACE_ELEV_FT&#39;] - df[depth_bot_col]

    layerList = range(1,layers+1)
    res_list = []
    resdf_list = []
    #Generate Column names based on (looped) integers
    for layer in layerList:
        zStr = &#39;ELEV&#39;
        zColT = &#39;TOP_ELEV_FT&#39;
        zColB = &#39;BOT_ELEV_FT&#39;
        topCol = zStr+&#39;_FT_LAYER&#39;+str(layer)
        if layer != 9: #For all layers except the bottom layer....
            botCol = zStr+&#39;_FT_LAYER&#39;+str(layer+1) #use the layer below it to 
        else: #Otherwise, ...
            botCol = &#34;BEDROCK_&#34;+zStr+&#34;_FT&#34; #Use the (corrected) bedrock depth

        #Divide records into 4 separate categories for ease of calculation, to be joined back together later  
            #Category 1: Well interval starts above layer top, ends within model layer
            #Category 2: Well interval is entirely contained withing model layer
            #Category 3: Well interval starts within model layer, continues through bottom of model layer
            #Category 4: well interval begins and ends on either side of model layer (model layer is contained within well layer)

        #records1 = intervals that go through the top of the layer and bottom is within layer
        records1 = df.loc[(df[zColT] &gt;= df[topCol]) &amp; #Top of the well is above or equal to the top of the layer
                        (df[zColB] &lt;= df[topCol]) &amp; # &amp; #Bottom is below the top of the layer
                        (df[zColB] &lt;= df[zColT])].copy() #Bottom is deeper than top (should already be the case)
        records1[&#39;TARG_THICK_FT&#39;] = pd.DataFrame(np.round((records1.loc[:,topCol]-records1.loc[: , zColB]) * records1[&#39;TARGET&#39;],3)).copy() #Multiply &#34;target&#34; (1 or 0) by length within layer            
        
        #records2 = entire interval is within layer
        records2 = df.loc[(df[zColT] &lt;= df[topCol]) &amp; #Top of the well is lower than top of the layer 
                    (df[zColB] &gt;= df[botCol]) &amp; #Bottom of the well is above bottom of the layer 
                    (df[zColB] &lt;= df[zColT])].copy() #Bottom ofthe well is deeper than or equal to top (should already be the case)
        records2[&#39;TARG_THICK_FT&#39;] = pd.DataFrame(np.round((records2.loc[: , zColT] - records2.loc[: , zColB]) * records2[&#39;TARGET&#39;],3)).copy()

        #records3 = intervals with top within layer and bottom of interval going through bottom of layer
        records3 = df.loc[(df[zColT] &gt; df[botCol]) &amp; #Top of the well is above bottom of layer
                    (df[zColB] &lt; df[botCol]) &amp; #Bottom of the well is below bottom of layer
                    (df[zColT] &lt;= df[topCol]) &amp; #Top of well is below top of layer
                    (df[zColB] &lt;= df[zColT])].copy() #Bottom is deeper than top (should already be the case)
        records3[&#39;TARG_THICK_FT&#39;] = pd.DataFrame(np.round((records3.loc[: , zColT] - (records3.loc[:,botCol]))*records3[&#39;TARGET&#39;],3)).copy()

        #records4 = interval goes through entire layer
        records4 = df.loc[(df[zColT] &gt;= df[topCol]) &amp; #Top of well is above top of layer
                    (df[zColB] &lt; df[botCol]) &amp; #Bottom of well is below bottom of layer
                    (df[zColB] &lt;= df[zColT])].copy() #Bottom of well is below top of well
        records4[&#39;TARG_THICK_FT&#39;] = pd.DataFrame(np.round((records4.loc[: , topCol]-records4.loc[: , botCol]) * records4[&#39;TARGET&#39;],3)).copy()
        
        #Put the four calculated record categories back together into single dataframe
        res = pd.concat([records1, records2, records3, records4])

        #The sign may be reversed if using depth rather than elevation
        if (res[&#39;TARG_THICK_FT&#39;] &lt; 0).all():
            res[&#39;TARG_THICK_FT&#39;] = res[&#39;TARG_THICK_FT&#39;] * -1
        
        #Cannot have negative thicknesses
        res[&#39;TARG_THICK_FT&#39;].clip(lower=0, inplace=True)
        res[&#39;LAYER_THICK_FT&#39;].clip(lower=0, inplace=True)
        
        #Get geometrys for each unique API/well
        res_df = res.groupby(by=[&#39;API_NUMBER&#39;,&#39;LATITUDE&#39;,&#39;LONGITUDE&#39;], as_index=False).sum(numeric_only=True)#Calculate thickness for each well interval in the layer indicated (e.g., if there are two well intervals from same well in one model layer)
        uniqInd = pd.DataFrame([v.values[0] for k, v in res.groupby(&#39;API_NUMBER&#39;).groups.items()]).loc[:,0]
        geomCol = res.loc[uniqInd, &#39;geometry&#39;]
        geomCol = pd.DataFrame(geomCol[~geomCol.index.duplicated(keep=&#39;first&#39;)]).reset_index()
        

        res_df[&#39;TARG_THICK_PER&#39;] =  pd.DataFrame(np.round(res_df[&#39;TARG_THICK_FT&#39;]/res_df[&#39;LAYER_THICK_FT&#39;],3)) #Calculate thickness as percent of total layer thickness
        res_df[&#39;TARG_THICK_PER&#39;] = res_df[&#39;TARG_THICK_PER&#39;].where(res_df[&#39;TARG_THICK_PER&#39;]!=np.inf, other=0) 

        res_df[&#34;LAYER&#34;] = layer #Just to have as part of the output file, include the present layer in the file itself as a separate column
        res_df = res_df[[&#39;API_NUMBER&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;LATITUDE_PROJ&#39;, &#39;LONGITUDE_PROJ&#39;,&#39;TOP&#39;, &#39;BOTTOM&#39;, &#39;TOP_ELEV_FT&#39;, &#39;BOT_ELEV_FT&#39;, &#39;SURFACE_ELEV_FT&#39;, topCol, botCol,&#39;LAYER_THICK_FT&#39;,&#39;TARG_THICK_FT&#39;, &#39;TARG_THICK_PER&#39;, &#39;LAYER&#39;]].copy() #Format dataframe for output
        res_df = gpd.GeoDataFrame(res_df, geometry=geomCol.loc[:,&#39;geometry&#39;])
        resdf_list.append(res_df)
        res_list.append(res)

        if isinstance(export_dir, pathlib.PurePath) or type(export_dir) is str:
            export_dir = pathlib.Path(export_dir)
            if export_dir.is_dir():
                pass
            else:
                try:
                    os.mkdir(export_dir)
                except:
                    print(&#39;Specified export directory does not exist and cannot be created. Function will continue run, but data will not be exported.&#39;)

            #Format and build export filepath
            export_dir = str(export_dir).replace(&#39;\\&#39;, &#39;/&#39;).replace(&#39;\\&#39;[0], &#39;/&#39;)
            zFillDigs = len(str(len(layerList)))
            if outfile_prefix[-1]==&#39;_&#39;:
                outfile_prefix = outfile_prefix[:-1]
            if export_dir[-1] ==&#39;/&#39;:
                export_dir = export_dir[:-1]
            nowStr = str(datetime.datetime.today().date())+&#39;_&#39;+str(datetime.datetime.today().hour)+&#39;-&#39;+str(datetime.datetime.today().minute)+&#39;-&#39;+str(datetime.datetime.today().second)
            outPath = export_dir+&#39;/&#39;+outfile_prefix+&#39;_Lyr&#39;+str(layer).zfill(zFillDigs)+&#39;_&#39;+nowStr+&#39;.csv&#39;

            if return_all:
                res.to_csv(outPath, sep=&#39;,&#39;, na_rep=np.nan, index_label=&#39;ID&#39;)
                res_df.to_csv(outPath, sep=&#39;,&#39;, na_rep=np.nan, index_label=&#39;ID&#39;)
            else:
                res_df.to_csv(outPath, sep=&#39;,&#39;, na_rep=np.nan, index_label=&#39;ID&#39;)

    if return_all:
        return res_list, resdf_list
    else:
        return resdf_list


#Interpolate layers to model grid
def layer_interp(points, grid, layers=None, method=&#39;nearest&#39;, return_type=&#39;dataarray&#39;, export_dir=None, targetcol=&#39;TARG_THICK_PER&#39;, lyrcol=&#39;LAYER&#39;, xcol=None, ycol=None, xcoord=&#39;x&#39;, ycoord=&#39;y&#39;, log=False, **kwargs):
    &#34;&#34;&#34;Function to interpolate results, going from points to grid data. Uses scipy.interpolate module.

    Parameters
    ----------
    points : list
        List containing pandas dataframes or geopandas geoadataframes containing the point data. Should be resDF_list output from layer_target_thick().
    grid : xr.DataArray or xr.Dataset
        Xarray dataarray with the coordinates/spatial reference of the output grid to interpolate to
    layers : int, default=None
        Number of layers for interpolation. If None, uses the length ofthe points list to determine number of layers. By default None.
    method : str, {&#39;nearest&#39;, &#39;interp2d&#39;,&#39;linear&#39;, &#39;cloughtocher&#39;, &#39;radial basis function&#39;}
        Type of interpolation to use. See scipy.interpolate N-D scattered. Values can be any of the following (also shown in &#34;kind&#34; column of N-D scattered section of table here: https://docs.scipy.org/doc/scipy/tutorial/interpolate.html). By default &#39;nearest&#39;
    return_type : str, {&#39;dataarray&#39;, &#39;dataset&#39;}
        Type of xarray object to return, either xr.DataArray or xr.Dataset, by default &#39;dataarray.&#39;
    export_dir : str or pathlib.Path, default=None
        Export directory for interpolated grids, using w4h.export_grids(). If None, does not export, by default None.
    targetcol : str, default = &#39;TARG_THICK_PER&#39;
        Name of column in points containing data to be interpolated, by default &#39;TARG_THICK_PER&#39;.
    lyrcol : str, default = &#39;Layer&#39;
        Name of column containing layer number. Not currently used, by default &#39;LAYER&#39;
    xcol : str, default = &#39;None&#39;
        Name of column containing x coordinates. If None, will look for &#39;geometry&#39; column, as in a geopandas.GeoDataframe. By default None
    ycol : str, default = &#39;None&#39;
        Name of column containing y coordinates. If None, will look for &#39;geometry&#39; column, as in a geopandas.GeoDataframe. By default None
    xcoord : str, default=&#39;x&#39;
        Name of x coordinate in grid, used to extract x values of grid, by default &#39;x&#39;
    ycoord : str, default=&#39;y&#39;
        Name of y coordinate in grid, used to extract x values of grid, by default &#39;y&#39;
    log : bool, default = True
        Whether to log inputs and outputs to log file.
    **kwargs
        Keyword arguments to be read directly into whichever scipy.interpolate function is designated by the method parameter.

    Returns
    -------
    interp_data : xr.DataArray or xr.Dataset, depending on return_type
        By default, returns an xr.DataArray object with the layers added as a new dimension called Layer. Can also specify return_type=&#39;dataset&#39; to return an xr.Dataset with each layer as a separate variable.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    nnList = [&#39;nearest&#39;, &#39;nearest neighbor&#39;, &#39;nearestneighbor&#39;,&#39;neighbor&#39;,  &#39;nn&#39;,&#39;n&#39;]
    splineList = [&#39;interp2d&#39;, &#39;interp2&#39;, &#39;interp&#39;, &#39;spline&#39;, &#39;spl&#39;, &#39;sp&#39;, &#39;s&#39;]
    linList = [&#39;linear&#39;, &#39;lin&#39;, &#39;l&#39;]
    ctList = [&#39;clough tocher&#39;, &#39;clough&#39;, &#39;cloughtocher&#39;, &#39;ct&#39;, &#39;c&#39;]
    rbfList = [&#39;rbf&#39;, &#39;radial basis&#39;, &#39;radial basis function&#39;, &#39;r&#39;, &#39;radial&#39;]
    #k-nearest neighbors from scikit-learn?
    #kriging? (from pykrige or maybe also from scikit-learn)
    
        
    X = np.round(grid[xcoord].values, 3)# #Extract xcoords from grid
    Y = np.round(grid[ycoord].values, 3)# #Extract ycoords from grid
    
    if layers is None and (type(points) is list or type(points) is dict):
        layers = len(points)

    if len(points) != layers:
        print(&#39;You have specified a different number of layers than what is iterable in the points argument. This may not work properly.&#39;)

    daDict = {}
    for lyr in range(1, layers+1):
        if type(points) is list or type(points) is dict:
            pts = points[lyr-1]
            dataX = pts
        else:
            pts = points

        if xcol is None:
            if &#39;geometry&#39; in pts.columns:
                dataX = pts[&#39;geometry&#39;].x
            else:
                print(&#39;xcol not specified and geometry column not detected (points is not/does not contain geopandas.GeoDataFrame)&#39; )
                return
        else:
            dataX = pts[xcol]
        
        if ycol is None:
            if &#39;geometry&#39; in pts.columns:
                dataY = pts[&#39;geometry&#39;].y
            else:
                print(&#39;ycol not specified and geometry column not detected (points is not/does not contain geopandas.GeoDataFrame)&#39; )
                return
        else:
            dataY = pts[ycol]

        #layer = pts[lyrcol]        
        interpVal = pts[targetcol]
        if method.lower() in nnList:
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            dataPoints = np.array(list(zip(dataX, dataY)))
            interp = interpolate.NearestNDInterpolator(dataPoints, interpVal, **kwargs)
            Z = interp(X, Y)
        elif method.lower() in linList:
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            interp = interpolate.LinearNDInterpolator(list(zip(dataX, dataY)), interpVal, **kwargs)
            Z = interp(X, Y)
        elif method.lower() in ctList:
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            if &#39;tol&#39; not in kwargs:
                kwargs[&#39;tol&#39;] = 1e10
            interp = interpolate.CloughTocher2DInterpolator(list(zip(dataX, dataY)), interpVal, **kwargs)
            Z = interp(X, Y) 
        elif method.lower() in rbfList:
            dataXY=  np.column_stack((dataX, dataY))
            interp = interpolate.RBFInterpolator(dataXY, interpVal, **kwargs)
            print(&#34;Radial Basis Function does not work well with many well-based datasets. Consider instead specifying &#39;nearest&#39;, &#39;linear&#39;, &#39;spline&#39;, or &#39;clough tocher&#39; for interpolation method.&#34;)
            Z = interp(np.column_stack((X.ravel(), Y.ravel()))).reshape(X.shape)
        elif method.lower() in splineList:
            Z = interpolate.bisplrep(dataX, dataY, interpVal, **kwargs)
                #interp = interpolate.interp2d(dataX, dataY, interpVal, kind=lin_kind, **kwargs)
                #Z = interp(X, Y)
        else:
            print(&#39;Specified interpolation method not recognized, using nearest neighbor.&#39;)
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            interp = interpolate.NearestNDInterpolator(list(zip(dataX, dataY)), interpVal, **kwargs)
            Z = interp(X, Y)

        #global ZTest
        #ZTest = Z

        interp_grid = xr.DataArray( #Create new datarray with new data values, but everything else the same
                    data=Z,
                    dims=grid.dims,
                    coords=grid.coords)
        
        if &#39;band&#39; in interp_grid.coords:
            interp_grid = interp_grid.drop_vars(&#39;band&#39;)
        interp_grid = interp_grid.clip(min=0, max=1, keep_attrs=True)

        interp_grid = interp_grid.expand_dims(dim=&#39;Layer&#39;)
        interp_grid = interp_grid.assign_coords(Layer=[lyr])

        del Z
        del dataX
        del dataY
        del interpVal
        del interp

        #interp_grid=interp_grid.interpolate_na(dim=x)
        zFillDigs = len(str(layers))
        daDict[&#39;Layer&#39;+str(lyr).zfill(zFillDigs)] = interp_grid
        del interp_grid
        print(&#39;Completed interpolation for Layer &#39;+str(lyr).zfill(zFillDigs))

    dataAList = [&#39;dataarray&#39;, &#39;da&#39;, &#39;a&#39;, &#39;array&#39;]
    dataSList = [&#39;dataset&#39;, &#39;ds&#39;, &#39;set&#39;]
    if return_type.lower() in dataAList:
        interp_data = xr.concat(daDict.values(), dim=&#39;Layer&#39;)
        interp_data = interp_data.assign_coords(Layer=np.arange(1,10))
    elif return_type.lower() in dataSList:
        interp_data = xr.Dataset(daDict)
        print(&#39;Done with interpolation, getting global attrs&#39;)
        common_attrs = {}
        for i, (var_name, data_array) in enumerate(interp_data.data_vars.items()):
            if i == 0:
                common_attrs = data_array.attrs
            else:
                common_attrs = {k: v for k, v in common_attrs.items() if k in data_array.attrs and data_array.attrs[k] == v}
        interp_data.attrs.update(common_attrs)
    else:
        print(&#34;{} is not a valid input for return_type. Please set return_type to either &#39;dataarray&#39; or &#39;dataset&#39;&#34;.format(return_type))
        return

    if export_dir is None:
        pass
    else:
        w4h.export_grids(grid_data=interp_data, out_path=export_dir, file_id=&#39;&#39;,filetype=&#39;tif&#39;, variable_sep=True, date_stamp=True)
        print(&#39;Exported to {}&#39;.format(export_dir))

    return interp_data

#Optional, combine dataset
def combine_dataset(layer_dataset, surface_elev, bedrock_elev, layer_thick, log=False):
    &#34;&#34;&#34;Function to combine xarray datasets or datarrays into a single xr.Dataset. Useful to add surface, bedrock, layer thick, and layer datasets all into one variable, for pickling, for example.

    Parameters
    ----------
    layer_dataset : xr.DataArray 
        DataArray contining all the interpolated layer information.
    surface_elev : xr.DataArray
        DataArray containing surface elevation data
    bedrock_elev : xr.DataArray
        DataArray containing bedrock elevation data
    layer_thick : xr.DataArray
        DataArray containing the layer thickness at each point in the model grid
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    xr.Dataset
        Dataset with all input arrays set to different variables within the dataset.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    
    daDict = {}
    daDict[&#39;Layers&#39;] = layer_dataset
    daDict[&#39;Surface_Elev&#39;] = surface_elev
    daDict[&#39;Bedrock_Elev&#39;] = bedrock_elev
    daDict[&#39;Layer_Thickness&#39;] = layer_thick

    combined_dataset = xr.Dataset(daDict)

    return combined_dataset</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="w4h.layers.combine_dataset"><code class="name flex">
<span>def <span class="ident">combine_dataset</span></span>(<span>layer_dataset, surface_elev, bedrock_elev, layer_thick, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to combine xarray datasets or datarrays into a single xr.Dataset. Useful to add surface, bedrock, layer thick, and layer datasets all into one variable, for pickling, for example.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>layer_dataset</code></strong> :&ensp;<code>xr.DataArray </code></dt>
<dd>DataArray contining all the interpolated layer information.</dd>
<dt><strong><code>surface_elev</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>DataArray containing surface elevation data</dd>
<dt><strong><code>bedrock_elev</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>DataArray containing bedrock elevation data</dd>
<dt><strong><code>layer_thick</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>DataArray containing the layer thickness at each point in the model grid</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>xr.Dataset</code></dt>
<dd>Dataset with all input arrays set to different variables within the dataset.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_dataset(layer_dataset, surface_elev, bedrock_elev, layer_thick, log=False):
    &#34;&#34;&#34;Function to combine xarray datasets or datarrays into a single xr.Dataset. Useful to add surface, bedrock, layer thick, and layer datasets all into one variable, for pickling, for example.

    Parameters
    ----------
    layer_dataset : xr.DataArray 
        DataArray contining all the interpolated layer information.
    surface_elev : xr.DataArray
        DataArray containing surface elevation data
    bedrock_elev : xr.DataArray
        DataArray containing bedrock elevation data
    layer_thick : xr.DataArray
        DataArray containing the layer thickness at each point in the model grid
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    xr.Dataset
        Dataset with all input arrays set to different variables within the dataset.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    
    daDict = {}
    daDict[&#39;Layers&#39;] = layer_dataset
    daDict[&#39;Surface_Elev&#39;] = surface_elev
    daDict[&#39;Bedrock_Elev&#39;] = bedrock_elev
    daDict[&#39;Layer_Thickness&#39;] = layer_thick

    combined_dataset = xr.Dataset(daDict)

    return combined_dataset</code></pre>
</details>
</dd>
<dt id="w4h.layers.get_layer_depths"><code class="name flex">
<span>def <span class="ident">get_layer_depths</span></span>(<span>well_metadata, no_layers=9, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to calculate depths and elevations of each model layer at each well based on surface elevation, bedrock elevation, and number of layers/layer thickness</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>well_metadata</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Dataframe containing well metdata</dd>
<dt><strong><code>no_layers</code></strong> :&ensp;<code>int</code>, default=<code>9</code></dt>
<dd>Number of layers. This should correlate with get_drift_thick() input parameter, if drift thickness was calculated using that function, by default 9.</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.Dataframe</code></dt>
<dd>Dataframe containing new columns for depth to layers and elevation of layers.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_layer_depths(well_metadata, no_layers=9, log=False):
    &#34;&#34;&#34;Function to calculate depths and elevations of each model layer at each well based on surface elevation, bedrock elevation, and number of layers/layer thickness

    Parameters
    ----------
    well_metadata : pandas.DataFrame
        Dataframe containing well metdata
    no_layers : int, default=9
        Number of layers. This should correlate with get_drift_thick() input parameter, if drift thickness was calculated using that function, by default 9.
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    pandas.Dataframe
        Dataframe containing new columns for depth to layers and elevation of layers.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    for layer in range(0, no_layers): #For each layer
        #Make column names
        depthColName  = &#39;DEPTH_FT_LAYER&#39;+str(layer+1)
        #depthMcolName = &#39;Depth_M_LAYER&#39;+str(layer) 

        #Calculate depth to each layer at each well, in feet and meters
        well_metadata[depthColName]  = well_metadata[&#39;LAYER_THICK_FT&#39;] * layer
        #headerData[depthMcolName] = headerData[depthColName] * 0.3048

    for layer in range(0, no_layers): #For each layer
        elevColName = &#39;ELEV_FT_LAYER&#39;+str(layer+1)
        #elevMColName = &#39;ELEV_M_LAYER&#39;+str(layer)
            
        well_metadata[elevColName]  = well_metadata[&#39;SURFACE_ELEV_FT&#39;] - well_metadata[&#39;LAYER_THICK_FT&#39;] * layer
        #headerData[elevMColName]  = headerData[&#39;SURFACE_ELEV_M&#39;] - headerData[&#39;LAYER_THICK_M&#39;] * layer
    return well_metadata</code></pre>
</details>
</dd>
<dt id="w4h.layers.layer_interp"><code class="name flex">
<span>def <span class="ident">layer_interp</span></span>(<span>points, grid, layers=None, method='nearest', return_type='dataarray', export_dir=None, targetcol='TARG_THICK_PER', lyrcol='LAYER', xcol=None, ycol=None, xcoord='x', ycoord='y', log=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to interpolate results, going from points to grid data. Uses scipy.interpolate module.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>points</code></strong> :&ensp;<code>list</code></dt>
<dd>List containing pandas dataframes or geopandas geoadataframes containing the point data. Should be resDF_list output from layer_target_thick().</dd>
<dt><strong><code>grid</code></strong> :&ensp;<code>xr.DataArray</code> or <code>xr.Dataset</code></dt>
<dd>Xarray dataarray with the coordinates/spatial reference of the output grid to interpolate to</dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>int</code>, default=<code>None</code></dt>
<dd>Number of layers for interpolation. If None, uses the length ofthe points list to determine number of layers. By default None.</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str, {'nearest', 'interp2d','linear', 'cloughtocher', 'radial basis function'}</code></dt>
<dd>Type of interpolation to use. See scipy.interpolate N-D scattered. Values can be any of the following (also shown in "kind" column of N-D scattered section of table here: <a href="https://docs.scipy.org/doc/scipy/tutorial/interpolate.html">https://docs.scipy.org/doc/scipy/tutorial/interpolate.html</a>). By default 'nearest'</dd>
<dt><strong><code>return_type</code></strong> :&ensp;<code>str, {'dataarray', 'dataset'}</code></dt>
<dd>Type of xarray object to return, either xr.DataArray or xr.Dataset, by default 'dataarray.'</dd>
<dt><strong><code>export_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, default=<code>None</code></dt>
<dd>Export directory for interpolated grids, using w4h.export_grids(). If None, does not export, by default None.</dd>
<dt><strong><code>targetcol</code></strong> :&ensp;<code>str</code>, default <code>= 'TARG_THICK_PER'</code></dt>
<dd>Name of column in points containing data to be interpolated, by default 'TARG_THICK_PER'.</dd>
<dt><strong><code>lyrcol</code></strong> :&ensp;<code>str</code>, default <code>= 'Layer'</code></dt>
<dd>Name of column containing layer number. Not currently used, by default 'LAYER'</dd>
<dt><strong><code>xcol</code></strong> :&ensp;<code>str</code>, default <code>= 'None'</code></dt>
<dd>Name of column containing x coordinates. If None, will look for 'geometry' column, as in a geopandas.GeoDataframe. By default None</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default <code>= 'None'</code></dt>
<dd>Name of column containing y coordinates. If None, will look for 'geometry' column, as in a geopandas.GeoDataframe. By default None</dd>
<dt><strong><code>xcoord</code></strong> :&ensp;<code>str</code>, default=<code>'x'</code></dt>
<dd>Name of x coordinate in grid, used to extract x values of grid, by default 'x'</dd>
<dt><strong><code>ycoord</code></strong> :&ensp;<code>str</code>, default=<code>'y'</code></dt>
<dd>Name of y coordinate in grid, used to extract x values of grid, by default 'y'</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments to be read directly into whichever scipy.interpolate function is designated by the method parameter.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>interp_data</code></strong> :&ensp;<code>xr.DataArray</code> or <code>xr.Dataset, depending on return_type</code></dt>
<dd>By default, returns an xr.DataArray object with the layers added as a new dimension called Layer. Can also specify return_type='dataset' to return an xr.Dataset with each layer as a separate variable.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def layer_interp(points, grid, layers=None, method=&#39;nearest&#39;, return_type=&#39;dataarray&#39;, export_dir=None, targetcol=&#39;TARG_THICK_PER&#39;, lyrcol=&#39;LAYER&#39;, xcol=None, ycol=None, xcoord=&#39;x&#39;, ycoord=&#39;y&#39;, log=False, **kwargs):
    &#34;&#34;&#34;Function to interpolate results, going from points to grid data. Uses scipy.interpolate module.

    Parameters
    ----------
    points : list
        List containing pandas dataframes or geopandas geoadataframes containing the point data. Should be resDF_list output from layer_target_thick().
    grid : xr.DataArray or xr.Dataset
        Xarray dataarray with the coordinates/spatial reference of the output grid to interpolate to
    layers : int, default=None
        Number of layers for interpolation. If None, uses the length ofthe points list to determine number of layers. By default None.
    method : str, {&#39;nearest&#39;, &#39;interp2d&#39;,&#39;linear&#39;, &#39;cloughtocher&#39;, &#39;radial basis function&#39;}
        Type of interpolation to use. See scipy.interpolate N-D scattered. Values can be any of the following (also shown in &#34;kind&#34; column of N-D scattered section of table here: https://docs.scipy.org/doc/scipy/tutorial/interpolate.html). By default &#39;nearest&#39;
    return_type : str, {&#39;dataarray&#39;, &#39;dataset&#39;}
        Type of xarray object to return, either xr.DataArray or xr.Dataset, by default &#39;dataarray.&#39;
    export_dir : str or pathlib.Path, default=None
        Export directory for interpolated grids, using w4h.export_grids(). If None, does not export, by default None.
    targetcol : str, default = &#39;TARG_THICK_PER&#39;
        Name of column in points containing data to be interpolated, by default &#39;TARG_THICK_PER&#39;.
    lyrcol : str, default = &#39;Layer&#39;
        Name of column containing layer number. Not currently used, by default &#39;LAYER&#39;
    xcol : str, default = &#39;None&#39;
        Name of column containing x coordinates. If None, will look for &#39;geometry&#39; column, as in a geopandas.GeoDataframe. By default None
    ycol : str, default = &#39;None&#39;
        Name of column containing y coordinates. If None, will look for &#39;geometry&#39; column, as in a geopandas.GeoDataframe. By default None
    xcoord : str, default=&#39;x&#39;
        Name of x coordinate in grid, used to extract x values of grid, by default &#39;x&#39;
    ycoord : str, default=&#39;y&#39;
        Name of y coordinate in grid, used to extract x values of grid, by default &#39;y&#39;
    log : bool, default = True
        Whether to log inputs and outputs to log file.
    **kwargs
        Keyword arguments to be read directly into whichever scipy.interpolate function is designated by the method parameter.

    Returns
    -------
    interp_data : xr.DataArray or xr.Dataset, depending on return_type
        By default, returns an xr.DataArray object with the layers added as a new dimension called Layer. Can also specify return_type=&#39;dataset&#39; to return an xr.Dataset with each layer as a separate variable.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    nnList = [&#39;nearest&#39;, &#39;nearest neighbor&#39;, &#39;nearestneighbor&#39;,&#39;neighbor&#39;,  &#39;nn&#39;,&#39;n&#39;]
    splineList = [&#39;interp2d&#39;, &#39;interp2&#39;, &#39;interp&#39;, &#39;spline&#39;, &#39;spl&#39;, &#39;sp&#39;, &#39;s&#39;]
    linList = [&#39;linear&#39;, &#39;lin&#39;, &#39;l&#39;]
    ctList = [&#39;clough tocher&#39;, &#39;clough&#39;, &#39;cloughtocher&#39;, &#39;ct&#39;, &#39;c&#39;]
    rbfList = [&#39;rbf&#39;, &#39;radial basis&#39;, &#39;radial basis function&#39;, &#39;r&#39;, &#39;radial&#39;]
    #k-nearest neighbors from scikit-learn?
    #kriging? (from pykrige or maybe also from scikit-learn)
    
        
    X = np.round(grid[xcoord].values, 3)# #Extract xcoords from grid
    Y = np.round(grid[ycoord].values, 3)# #Extract ycoords from grid
    
    if layers is None and (type(points) is list or type(points) is dict):
        layers = len(points)

    if len(points) != layers:
        print(&#39;You have specified a different number of layers than what is iterable in the points argument. This may not work properly.&#39;)

    daDict = {}
    for lyr in range(1, layers+1):
        if type(points) is list or type(points) is dict:
            pts = points[lyr-1]
            dataX = pts
        else:
            pts = points

        if xcol is None:
            if &#39;geometry&#39; in pts.columns:
                dataX = pts[&#39;geometry&#39;].x
            else:
                print(&#39;xcol not specified and geometry column not detected (points is not/does not contain geopandas.GeoDataFrame)&#39; )
                return
        else:
            dataX = pts[xcol]
        
        if ycol is None:
            if &#39;geometry&#39; in pts.columns:
                dataY = pts[&#39;geometry&#39;].y
            else:
                print(&#39;ycol not specified and geometry column not detected (points is not/does not contain geopandas.GeoDataFrame)&#39; )
                return
        else:
            dataY = pts[ycol]

        #layer = pts[lyrcol]        
        interpVal = pts[targetcol]
        if method.lower() in nnList:
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            dataPoints = np.array(list(zip(dataX, dataY)))
            interp = interpolate.NearestNDInterpolator(dataPoints, interpVal, **kwargs)
            Z = interp(X, Y)
        elif method.lower() in linList:
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            interp = interpolate.LinearNDInterpolator(list(zip(dataX, dataY)), interpVal, **kwargs)
            Z = interp(X, Y)
        elif method.lower() in ctList:
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            if &#39;tol&#39; not in kwargs:
                kwargs[&#39;tol&#39;] = 1e10
            interp = interpolate.CloughTocher2DInterpolator(list(zip(dataX, dataY)), interpVal, **kwargs)
            Z = interp(X, Y) 
        elif method.lower() in rbfList:
            dataXY=  np.column_stack((dataX, dataY))
            interp = interpolate.RBFInterpolator(dataXY, interpVal, **kwargs)
            print(&#34;Radial Basis Function does not work well with many well-based datasets. Consider instead specifying &#39;nearest&#39;, &#39;linear&#39;, &#39;spline&#39;, or &#39;clough tocher&#39; for interpolation method.&#34;)
            Z = interp(np.column_stack((X.ravel(), Y.ravel()))).reshape(X.shape)
        elif method.lower() in splineList:
            Z = interpolate.bisplrep(dataX, dataY, interpVal, **kwargs)
                #interp = interpolate.interp2d(dataX, dataY, interpVal, kind=lin_kind, **kwargs)
                #Z = interp(X, Y)
        else:
            print(&#39;Specified interpolation method not recognized, using nearest neighbor.&#39;)
            X, Y = np.meshgrid(X, Y, sparse=True) #2D Grid for interpolation
            interp = interpolate.NearestNDInterpolator(list(zip(dataX, dataY)), interpVal, **kwargs)
            Z = interp(X, Y)

        #global ZTest
        #ZTest = Z

        interp_grid = xr.DataArray( #Create new datarray with new data values, but everything else the same
                    data=Z,
                    dims=grid.dims,
                    coords=grid.coords)
        
        if &#39;band&#39; in interp_grid.coords:
            interp_grid = interp_grid.drop_vars(&#39;band&#39;)
        interp_grid = interp_grid.clip(min=0, max=1, keep_attrs=True)

        interp_grid = interp_grid.expand_dims(dim=&#39;Layer&#39;)
        interp_grid = interp_grid.assign_coords(Layer=[lyr])

        del Z
        del dataX
        del dataY
        del interpVal
        del interp

        #interp_grid=interp_grid.interpolate_na(dim=x)
        zFillDigs = len(str(layers))
        daDict[&#39;Layer&#39;+str(lyr).zfill(zFillDigs)] = interp_grid
        del interp_grid
        print(&#39;Completed interpolation for Layer &#39;+str(lyr).zfill(zFillDigs))

    dataAList = [&#39;dataarray&#39;, &#39;da&#39;, &#39;a&#39;, &#39;array&#39;]
    dataSList = [&#39;dataset&#39;, &#39;ds&#39;, &#39;set&#39;]
    if return_type.lower() in dataAList:
        interp_data = xr.concat(daDict.values(), dim=&#39;Layer&#39;)
        interp_data = interp_data.assign_coords(Layer=np.arange(1,10))
    elif return_type.lower() in dataSList:
        interp_data = xr.Dataset(daDict)
        print(&#39;Done with interpolation, getting global attrs&#39;)
        common_attrs = {}
        for i, (var_name, data_array) in enumerate(interp_data.data_vars.items()):
            if i == 0:
                common_attrs = data_array.attrs
            else:
                common_attrs = {k: v for k, v in common_attrs.items() if k in data_array.attrs and data_array.attrs[k] == v}
        interp_data.attrs.update(common_attrs)
    else:
        print(&#34;{} is not a valid input for return_type. Please set return_type to either &#39;dataarray&#39; or &#39;dataset&#39;&#34;.format(return_type))
        return

    if export_dir is None:
        pass
    else:
        w4h.export_grids(grid_data=interp_data, out_path=export_dir, file_id=&#39;&#39;,filetype=&#39;tif&#39;, variable_sep=True, date_stamp=True)
        print(&#39;Exported to {}&#39;.format(export_dir))

    return interp_data</code></pre>
</details>
</dd>
<dt id="w4h.layers.layer_target_thick"><code class="name flex">
<span>def <span class="ident">layer_target_thick</span></span>(<span>df, layers=9, return_all=False, export_dir=None, outfile_prefix='', depth_top_col='TOP', depth_bot_col='BOTTOM', log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to calculate thickness of target material in each layer at each well point</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>geopandas.geodataframe</code></dt>
<dd>Geodataframe containing classified data, surface elevation, bedrock elevation, layer depths, geometry.</dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>int</code>, default=<code>9</code></dt>
<dd>Number of layers in model, by default 9</dd>
<dt><strong><code>return_all</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>If True, return list of original geodataframes with extra column added for target thick for each layer.
If False, return list of geopandas.geodataframes with only essential information for each layer.</dd>
<dt><strong><code>export_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, default=<code>None</code></dt>
<dd>If str or pathlib.Path, should be directory to which to export dataframes built in function.</dd>
<dt><strong><code>outfile_prefix</code></strong> :&ensp;<code>str</code>, default=<code>''</code></dt>
<dd>Only used if export_dir is set. Will be used at the start of the exported filenames</dd>
<dt><strong><code>depth_top_col</code></strong> :&ensp;<code>str</code>, default=<code>'TOP'</code></dt>
<dd>Name of column containing data for depth to top of described well intervals</dd>
<dt><strong><code>depth_bot_col</code></strong> :&ensp;<code>str</code>, default=<code>'BOTTOM'</code></dt>
<dd>Name of column containing data for depth to bottom of described well intervals</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>res_df</code> or <code>res : geopandas.geodataframe</code></dt>
<dd>Geopandas geodataframe containing only important information needed for next stage of analysis.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def layer_target_thick(df, layers=9, return_all=False, export_dir=None, outfile_prefix=&#39;&#39;, depth_top_col=&#39;TOP&#39;, depth_bot_col=&#39;BOTTOM&#39;, log=False):
    &#34;&#34;&#34;Function to calculate thickness of target material in each layer at each well point

    Parameters
    ----------
    df : geopandas.geodataframe
        Geodataframe containing classified data, surface elevation, bedrock elevation, layer depths, geometry.
    layers : int, default=9
        Number of layers in model, by default 9
    return_all : bool, default=False
        If True, return list of original geodataframes with extra column added for target thick for each layer.
        If False, return list of geopandas.geodataframes with only essential information for each layer.
    export_dir : str or pathlib.Path, default=None
        If str or pathlib.Path, should be directory to which to export dataframes built in function.
    outfile_prefix : str, default=&#39;&#39;
        Only used if export_dir is set. Will be used at the start of the exported filenames
    depth_top_col : str, default=&#39;TOP&#39;
        Name of column containing data for depth to top of described well intervals
    depth_bot_col : str, default=&#39;BOTTOM&#39;
        Name of column containing data for depth to bottom of described well intervals
    log : bool, default = True
        Whether to log inputs and outputs to log file.
    
    Returns
    -------
    res_df or res : geopandas.geodataframe
        Geopandas geodataframe containing only important information needed for next stage of analysis.
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    df[&#39;TOP_ELEV_FT&#39;] = df[&#39;SURFACE_ELEV_FT&#39;] - df[depth_top_col]
    df[&#39;BOT_ELEV_FT&#39;] = df[&#39;SURFACE_ELEV_FT&#39;] - df[depth_bot_col]

    layerList = range(1,layers+1)
    res_list = []
    resdf_list = []
    #Generate Column names based on (looped) integers
    for layer in layerList:
        zStr = &#39;ELEV&#39;
        zColT = &#39;TOP_ELEV_FT&#39;
        zColB = &#39;BOT_ELEV_FT&#39;
        topCol = zStr+&#39;_FT_LAYER&#39;+str(layer)
        if layer != 9: #For all layers except the bottom layer....
            botCol = zStr+&#39;_FT_LAYER&#39;+str(layer+1) #use the layer below it to 
        else: #Otherwise, ...
            botCol = &#34;BEDROCK_&#34;+zStr+&#34;_FT&#34; #Use the (corrected) bedrock depth

        #Divide records into 4 separate categories for ease of calculation, to be joined back together later  
            #Category 1: Well interval starts above layer top, ends within model layer
            #Category 2: Well interval is entirely contained withing model layer
            #Category 3: Well interval starts within model layer, continues through bottom of model layer
            #Category 4: well interval begins and ends on either side of model layer (model layer is contained within well layer)

        #records1 = intervals that go through the top of the layer and bottom is within layer
        records1 = df.loc[(df[zColT] &gt;= df[topCol]) &amp; #Top of the well is above or equal to the top of the layer
                        (df[zColB] &lt;= df[topCol]) &amp; # &amp; #Bottom is below the top of the layer
                        (df[zColB] &lt;= df[zColT])].copy() #Bottom is deeper than top (should already be the case)
        records1[&#39;TARG_THICK_FT&#39;] = pd.DataFrame(np.round((records1.loc[:,topCol]-records1.loc[: , zColB]) * records1[&#39;TARGET&#39;],3)).copy() #Multiply &#34;target&#34; (1 or 0) by length within layer            
        
        #records2 = entire interval is within layer
        records2 = df.loc[(df[zColT] &lt;= df[topCol]) &amp; #Top of the well is lower than top of the layer 
                    (df[zColB] &gt;= df[botCol]) &amp; #Bottom of the well is above bottom of the layer 
                    (df[zColB] &lt;= df[zColT])].copy() #Bottom ofthe well is deeper than or equal to top (should already be the case)
        records2[&#39;TARG_THICK_FT&#39;] = pd.DataFrame(np.round((records2.loc[: , zColT] - records2.loc[: , zColB]) * records2[&#39;TARGET&#39;],3)).copy()

        #records3 = intervals with top within layer and bottom of interval going through bottom of layer
        records3 = df.loc[(df[zColT] &gt; df[botCol]) &amp; #Top of the well is above bottom of layer
                    (df[zColB] &lt; df[botCol]) &amp; #Bottom of the well is below bottom of layer
                    (df[zColT] &lt;= df[topCol]) &amp; #Top of well is below top of layer
                    (df[zColB] &lt;= df[zColT])].copy() #Bottom is deeper than top (should already be the case)
        records3[&#39;TARG_THICK_FT&#39;] = pd.DataFrame(np.round((records3.loc[: , zColT] - (records3.loc[:,botCol]))*records3[&#39;TARGET&#39;],3)).copy()

        #records4 = interval goes through entire layer
        records4 = df.loc[(df[zColT] &gt;= df[topCol]) &amp; #Top of well is above top of layer
                    (df[zColB] &lt; df[botCol]) &amp; #Bottom of well is below bottom of layer
                    (df[zColB] &lt;= df[zColT])].copy() #Bottom of well is below top of well
        records4[&#39;TARG_THICK_FT&#39;] = pd.DataFrame(np.round((records4.loc[: , topCol]-records4.loc[: , botCol]) * records4[&#39;TARGET&#39;],3)).copy()
        
        #Put the four calculated record categories back together into single dataframe
        res = pd.concat([records1, records2, records3, records4])

        #The sign may be reversed if using depth rather than elevation
        if (res[&#39;TARG_THICK_FT&#39;] &lt; 0).all():
            res[&#39;TARG_THICK_FT&#39;] = res[&#39;TARG_THICK_FT&#39;] * -1
        
        #Cannot have negative thicknesses
        res[&#39;TARG_THICK_FT&#39;].clip(lower=0, inplace=True)
        res[&#39;LAYER_THICK_FT&#39;].clip(lower=0, inplace=True)
        
        #Get geometrys for each unique API/well
        res_df = res.groupby(by=[&#39;API_NUMBER&#39;,&#39;LATITUDE&#39;,&#39;LONGITUDE&#39;], as_index=False).sum(numeric_only=True)#Calculate thickness for each well interval in the layer indicated (e.g., if there are two well intervals from same well in one model layer)
        uniqInd = pd.DataFrame([v.values[0] for k, v in res.groupby(&#39;API_NUMBER&#39;).groups.items()]).loc[:,0]
        geomCol = res.loc[uniqInd, &#39;geometry&#39;]
        geomCol = pd.DataFrame(geomCol[~geomCol.index.duplicated(keep=&#39;first&#39;)]).reset_index()
        

        res_df[&#39;TARG_THICK_PER&#39;] =  pd.DataFrame(np.round(res_df[&#39;TARG_THICK_FT&#39;]/res_df[&#39;LAYER_THICK_FT&#39;],3)) #Calculate thickness as percent of total layer thickness
        res_df[&#39;TARG_THICK_PER&#39;] = res_df[&#39;TARG_THICK_PER&#39;].where(res_df[&#39;TARG_THICK_PER&#39;]!=np.inf, other=0) 

        res_df[&#34;LAYER&#34;] = layer #Just to have as part of the output file, include the present layer in the file itself as a separate column
        res_df = res_df[[&#39;API_NUMBER&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;LATITUDE_PROJ&#39;, &#39;LONGITUDE_PROJ&#39;,&#39;TOP&#39;, &#39;BOTTOM&#39;, &#39;TOP_ELEV_FT&#39;, &#39;BOT_ELEV_FT&#39;, &#39;SURFACE_ELEV_FT&#39;, topCol, botCol,&#39;LAYER_THICK_FT&#39;,&#39;TARG_THICK_FT&#39;, &#39;TARG_THICK_PER&#39;, &#39;LAYER&#39;]].copy() #Format dataframe for output
        res_df = gpd.GeoDataFrame(res_df, geometry=geomCol.loc[:,&#39;geometry&#39;])
        resdf_list.append(res_df)
        res_list.append(res)

        if isinstance(export_dir, pathlib.PurePath) or type(export_dir) is str:
            export_dir = pathlib.Path(export_dir)
            if export_dir.is_dir():
                pass
            else:
                try:
                    os.mkdir(export_dir)
                except:
                    print(&#39;Specified export directory does not exist and cannot be created. Function will continue run, but data will not be exported.&#39;)

            #Format and build export filepath
            export_dir = str(export_dir).replace(&#39;\\&#39;, &#39;/&#39;).replace(&#39;\\&#39;[0], &#39;/&#39;)
            zFillDigs = len(str(len(layerList)))
            if outfile_prefix[-1]==&#39;_&#39;:
                outfile_prefix = outfile_prefix[:-1]
            if export_dir[-1] ==&#39;/&#39;:
                export_dir = export_dir[:-1]
            nowStr = str(datetime.datetime.today().date())+&#39;_&#39;+str(datetime.datetime.today().hour)+&#39;-&#39;+str(datetime.datetime.today().minute)+&#39;-&#39;+str(datetime.datetime.today().second)
            outPath = export_dir+&#39;/&#39;+outfile_prefix+&#39;_Lyr&#39;+str(layer).zfill(zFillDigs)+&#39;_&#39;+nowStr+&#39;.csv&#39;

            if return_all:
                res.to_csv(outPath, sep=&#39;,&#39;, na_rep=np.nan, index_label=&#39;ID&#39;)
                res_df.to_csv(outPath, sep=&#39;,&#39;, na_rep=np.nan, index_label=&#39;ID&#39;)
            else:
                res_df.to_csv(outPath, sep=&#39;,&#39;, na_rep=np.nan, index_label=&#39;ID&#39;)

    if return_all:
        return res_list, resdf_list
    else:
        return resdf_list</code></pre>
</details>
</dd>
<dt id="w4h.layers.merge_tables"><code class="name flex">
<span>def <span class="ident">merge_tables</span></span>(<span>data_df, header_df, data_cols=None, header_cols=None, auto_pick_cols=False, drop_duplicate_cols=True, log=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to merge tables, intended for merging metadata table with data table</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>"Left" dataframe, intended for this purpose to be dataframe with main data, but can be anything</dd>
<dt><strong><code>header_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>"Right" dataframe, intended for this purpose to be dataframe with metadata, but can be anything</dd>
<dt><strong><code>data_cols</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>List of strings of column names, for columns to be included after join from "left" table (data table). If None, all columns are kept, by default None</dd>
<dt><strong><code>header_cols</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>List of strings of columns names, for columns to be included in merged table after merge from "right" table (metadata). If None, all columns are kept, by default None</dd>
<dt><strong><code>auto_pick_cols</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to autopick the columns from the metadata table. If True, the following column names are kept:['API_NUMBER', 'LATITUDE', 'LONGITUDE', 'BEDROCK_ELEV_FT', 'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT'], by default False</dd>
<dt><strong><code>drop_duplicate_cols</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, drops duplicate columns from the tables so that columns do not get renamed upon merge, by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>kwargs that are passed directly to pd.merge(). By default, the 'on' and 'how' parameters are defined as on='API_NUMBER' and how='inner'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mergedTable</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Merged dataframe</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_tables(data_df, header_df, data_cols=None, header_cols=None, auto_pick_cols=False, drop_duplicate_cols=True, log=False, **kwargs):
    &#34;&#34;&#34;Function to merge tables, intended for merging metadata table with data table

    Parameters
    ----------
    data_df : pandas.DataFrame
        &#34;Left&#34; dataframe, intended for this purpose to be dataframe with main data, but can be anything
    header_df : pandas.DataFrame
        &#34;Right&#34; dataframe, intended for this purpose to be dataframe with metadata, but can be anything
    data_cols : list, optional
        List of strings of column names, for columns to be included after join from &#34;left&#34; table (data table). If None, all columns are kept, by default None
    header_cols : list, optional
        List of strings of columns names, for columns to be included in merged table after merge from &#34;right&#34; table (metadata). If None, all columns are kept, by default None
    auto_pick_cols : bool, default = False
        Whether to autopick the columns from the metadata table. If True, the following column names are kept:[&#39;API_NUMBER&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;BEDROCK_ELEV_FT&#39;, &#39;SURFACE_ELEV_FT&#39;, &#39;BEDROCK_DEPTH_FT&#39;, &#39;LAYER_THICK_FT&#39;], by default False
    drop_duplicate_cols : bool, optional
        If True, drops duplicate columns from the tables so that columns do not get renamed upon merge, by default True
    log : bool, default = False
        Whether to log inputs and outputs to log file.
    **kwargs
        kwargs that are passed directly to pd.merge(). By default, the &#39;on&#39; and &#39;how&#39; parameters are defined as on=&#39;API_NUMBER&#39; and how=&#39;inner&#39;

    Returns
    -------
    mergedTable : pandas.DataFrame
        Merged dataframe
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if auto_pick_cols:
        header_cols = [&#39;API_NUMBER&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;BEDROCK_ELEV_FT&#39;, &#39;SURFACE_ELEV_FT&#39;, &#39;BEDROCK_DEPTH_FT&#39;, &#39;LAYER_THICK_FT&#39;]
        for c in header_df.columns:
            if c.startswith(&#39;ELEV_&#39;) or c.startswith(&#39;DEPTH&#39;):
                header_cols.append(c)
            if &#39;_PROJ&#39; in c:
                header_cols.append(c)
        header_cols.append(&#39;geometry&#39;)
    elif header_cols is None:
        header_cols = header_df.columns
    else:
        header_cols = header_cols

    #If not specified, get all the cols
    if data_cols is None:
        data_cols = data_df.columns

    #Drop duplicate columns
    if drop_duplicate_cols:
        header_colCopy= header_cols.copy()
        remCount = 0
        for i, c in enumerate(header_colCopy):
            if c in data_cols and c != on:
                print(&#39;REMOVING&#39;, header_cols[i-remCount])
                header_cols.pop(i - remCount)
                remCount += 1

    leftTable_join = data_df[data_cols]
    rightTable_join = header_df[header_cols]

    #Defults for on and how
    if &#39;on&#39; not in kwargs.keys():
        kwargs[&#39;on&#39;]=&#39;API_NUMBER&#39;

    if &#39;how&#39; not in kwargs.keys():
        kwargs[&#39;how&#39;]=&#39;inner&#39;

    mergedTable = pd.merge(left=leftTable_join, right=rightTable_join, **kwargs)
    return mergedTable</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="w4h" href="index.html">w4h</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="w4h.layers.combine_dataset" href="#w4h.layers.combine_dataset">combine_dataset</a></code></li>
<li><code><a title="w4h.layers.get_layer_depths" href="#w4h.layers.get_layer_depths">get_layer_depths</a></code></li>
<li><code><a title="w4h.layers.layer_interp" href="#w4h.layers.layer_interp">layer_interp</a></code></li>
<li><code><a title="w4h.layers.layer_target_thick" href="#w4h.layers.layer_target_thick">layer_target_thick</a></code></li>
<li><code><a title="w4h.layers.merge_tables" href="#w4h.layers.merge_tables">merge_tables</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>