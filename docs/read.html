<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>w4h.read API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>w4h.read</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pandas as pd
import numpy as np
import pathlib
import datetime
import os
import json
repoDir = pathlib.Path(os.getcwd())

# Gets the current date for use with in code
def get_current_date():
    &#34;&#34;&#34; Gets the current date to help with finding the most recent file
        ---------------------
        Parameters:
            None

        ---------------------
        Returns:
            todayDate   : datetime object with today&#39;s date
            dateSuffix  : str to use for naming output files
    &#34;&#34;&#34;
    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)
    dateSuffix = &#39;_&#39;+todayDateStr
    return todayDate, dateSuffix

#Function to get most recent file 
def get_most_recent(dir=str(repoDir)+&#39;/resources&#39;, glob_pattern=&#39;*&#39;, verbose=True):
    &#34;&#34;&#34;Function to find the most recent file with the indicated pattern, using pathlib.glob function.

    Parameters
    ----------
    dir : str or pathlib.Path object, optional
        Directory in which to find the most recent file, by default str(repoDir)+&#39;/resources&#39;
    glob_pattern : str, optional
        String used by the pathlib.glob() function/method for searching, by default &#39;*&#39;

    Returns
    -------
    pathlib.Path object
        Pathlib Path object of the most recent file fitting the glob pattern indicated in the glob_pattern parameter.
    &#34;&#34;&#34;
    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)

    files = pathlib.Path(dir).rglob(glob_pattern) #Get all the files that fit the pattern

    fileDates = []
    for f in files: #Get the file dates from their file modification times
        fileDates.append(np.datetime64(datetime.datetime.fromtimestamp(os.path.getmtime(f))))
    globInd = np.argmin(np.datetime64(todayDateStr) - np.array(fileDates)) #Find the index of the most recent file

    #Iterate through glob/files again (need to recreate glob)
    files = pathlib.Path(dir).rglob(glob_pattern)
    for j, f in enumerate(files):
        if j == globInd:
            mostRecentFile=f
            break
    
    if verbose:
        print(&#39;Most Recent version of this file is : &#39;+mostRecentFile.name)

    return mostRecentFile

#Function to setup files of interest
def file_setup(db_dir, metadata_dir=None, xyz_dir=None, data_pattern=&#39;*ISGS_DOWNHOLE_DATA*.txt&#39;, metadata_pattern=&#39;*ISGS_HEADER*.txt&#39;, xyz_pattern= &#39;*xyzData*&#39;, verbose=False):
    &#34;&#34;&#34;Function to setup files, assuming data, metadata, and elevation/location are in separate files (there should be one &#34;key&#34;/identifying column consistent across all files to join/merge them later)

    This function may not be useful if files are organized differently than this structure. If that is the case, it is recommended to use the get_most_recent() function for each individual file needed.

    Parameters
    ----------
    db_dir : str or pathlib.Path object, optional
        Str or pathlib.Path to directory containing input files, by default str(repoDir)+&#39;/resources&#39;
    metadata_dir : str or pathlib.Path object, optional
        Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+&#39;/resources&#39;
    xyz_dir : str or pathlib.Path object, optional
        Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+&#39;/resources&#39;
    data_pattern : str, optional
        Pattern used by pathlib.glob() to get the most recent data file, by default &#39;*ISGS_DOWNHOLE_DATA*.txt&#39;
    metadata_pattern : str, optional
        Pattern used by pathlib.glob() to get the most recent metadata file, by default &#39;*ISGS_HEADER*.txt&#39;
    xyz_pattern : str, optional
        Pattern used by pathlib.glob() to get the most recent elevation/location file, by default &#39;*xyzData*&#39;
    verbose : bool, default = False
        Whether to print name of files to terminal, by default True

    Returns
    -------
    _type_
        _description_
    &#34;&#34;&#34;
    #Define  filepath variables to be used later for reading/writing files
    raw_directory = pathlib.Path(db_dir)
    if metadata_dir is None:
        metadata_dir=raw_directory
    else:
        metadata_dir=pathlib.Path(metadata_dir)

    if xyz_dir is None:
        xyz_dir=raw_directory
    else:
        xyz_dir=pathlib.Path(xyz_dir)

    downholeDataFILE = get_most_recent(raw_directory, data_pattern)
    headerDataFILE = get_most_recent(metadata_dir, metadata_pattern)
    xyzInFILE = get_most_recent(xyz_dir,xyz_pattern)

    downholeDataPATH = pathlib.Path(downholeDataFILE)
    headerDataPATH = pathlib.Path(headerDataFILE)
    xyzInPATH = pathlib.Path(xyzInFILE)

    if verbose:
        print(&#39;Using the following files:\n&#39;)
        print(&#39;\t&#39;, downholeDataFILE)
        print(&#39;\t&#39;, headerDataFILE)
        print(&#39;\t&#39;, xyzInFILE)

    #Define datatypes, to use later
    #downholeDataDTYPES = {&#39;ID&#39;:np.uint32, &#34;API_NUMBER&#34;:np.uint64,&#34;TABLE_NAME&#34;:str,&#34;WHO&#34;:str,&#34;INTERPRET_DATE&#34;:str,&#34;FORMATION&#34;:str,&#34;THICKNESS&#34;:np.float64,&#34;TOP&#34;:np.float64,&#34;BOTTOM&#34;:np.float64}
    #headerDataDTYPES = {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#34;TDFORMATION&#34;:str,&#34;PRODFORM&#34;:str,&#34;TOTAL_DEPTH&#34;:np.float64,&#34;SECTION&#34;:np.float64,&#34;TWP&#34;:np.float64,&#34;TDIR&#34;:str,&#34;RNG&#34;:np.float64,&#34;RDIR&#34;:str,&#34;MERIDIAN&#34;:np.float64,&#34;FARM_NAME&#34;:str,&#34;NSFOOT&#34;:np.float64,&#34;NSDIR&#34;:str,&#34;EWFOOT&#34;:np.float64,&#34;EWDIR&#34;:str,&#34;QUARTERS&#34;:str,&#34;ELEVATION&#34;:np.float64,&#34;ELEVREF&#34;:str,&#34;COMP_DATE&#34;:str,&#34;STATUS&#34;:str,&#34;FARM_NUM&#34;:str,&#34;COUNTY_CODE&#34;:np.float64,&#34;PERMIT_NUMBER&#34;:str,&#34;COMPANY_NAME&#34;:str,&#34;COMPANY_CODE&#34;:str,&#34;PERMIT_DATE&#34;:str,&#34;CORNER&#34;:str,&#34;LATITUDE&#34;:np.float64,&#34;LONGITUDE&#34;:np.float64,&#34;ENTERED_BY&#34;:str,&#34;UPDDATE&#34;:str,&#34;ELEVSOURCE&#34;:str, &#34;ELEV_FT&#34;:np.float64}
    return downholeDataPATH, headerDataPATH, xyzInPATH

#Read raw data by text
def read_raw_txt(raw_dir, data_filename, metadata_filename, data_cols=None, metadata_cols=None, x_col=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, id_col=&#39;API_NUMBER&#39;, encoding=&#39;latin-1&#39;, verbose=False):
    &#34;&#34;&#34;Easy function to read raw .txt files output from (for example), an Access database

    Parameters
    ----------
    raw_dir : str or pathlib.Path object
        String or pathlib.Path to directory containing the files.
    data_filename : str
        Filename of the file containing data, including the extension.
    metadata_filename : str
        Filename of the file containing metadata, including the extension.
    data_cols : list, default = None
        List with strings with names of columns from txt file to keep after reading. If None, [&#34;API_NUMBER&#34;,&#34;TABLE_NAME&#34;,&#34;FORMATION&#34;,&#34;THICKNESS&#34;,&#34;TOP&#34;,&#34;BOTTOM&#34;], by default None.
    metadata_cols : list, default = None
        List with strings with names of columns from txt file to keep after reading. If None, [&#39;API_NUMBER&#39;,&#34;TOTAL_DEPTH&#34;,&#34;SECTION&#34;,&#34;TWP&#34;,&#34;TDIR&#34;,&#34;RNG&#34;,&#34;RDIR&#34;,&#34;MERIDIAN&#34;,&#34;QUARTERS&#34;,&#34;ELEVATION&#34;,&#34;ELEVREF&#34;,&#34;COUNTY_CODE&#34;,&#34;LATITUDE&#34;,&#34;LONGITUDE&#34;,&#34;ELEVSOURCE&#34;], by default None
    x_col : str, default = &#39;LONGITUDE&#39;
        Name of column in metadata file indicating the x-location of the well, by default &#39;LONGITUDE&#39;
    ycol : str, default = &#39;LATITUDE&#39;
        Name of the column in metadata file indicating the y-location of the well, by default &#39;LATITUDE&#39;
    id_col : str, default = &#39;API_NUMBER&#39;
        Name of the column with the key/identifier that will be used to merge data later, by default &#39;API_NUMBER&#39;
    encoding : str, default = &#39;latin-1&#39;
        Encoding of the data in the input files, by default &#39;latin-1&#39;
    verbose : bool, default = False
        Whether to print the number of rows in the input columns, by default False

    Returns
    -------
    (pandas.DataFrame, pandas.DataFrame)
        Tuple/list with two pandas dataframes: (data, metadata)
    &#34;&#34;&#34;

    if data_cols is None:
        data_useCols = [&#34;API_NUMBER&#34;,&#34;TABLE_NAME&#34;,&#34;FORMATION&#34;,&#34;THICKNESS&#34;,&#34;TOP&#34;,&#34;BOTTOM&#34;]
    if metadata_cols is None:
        metadata_useCols = [&#39;API_NUMBER&#39;,&#34;TOTAL_DEPTH&#34;,&#34;SECTION&#34;,&#34;TWP&#34;,&#34;TDIR&#34;,&#34;RNG&#34;,&#34;RDIR&#34;,&#34;MERIDIAN&#34;,&#34;QUARTERS&#34;,&#34;ELEVATION&#34;,&#34;ELEVREF&#34;,&#34;COUNTY_CODE&#34;,&#34;LATITUDE&#34;,&#34;LONGITUDE&#34;,&#34;ELEVSOURCE&#34;]
   
    raw_dir = pathlib.path(raw_dir)
    raw_dir = raw_dir.as_posix()
    if raw_dir[-1] != &#39;/&#39;:
        raw_dir = raw_dir + &#39;/&#39;

    downholeDataIN = pd.read_csv(raw_dir+str(data_filename), sep=&#39;,&#39;, header=&#39;infer&#39;, encoding=encoding, usecols=data_useCols)
    headerDataIN = pd.read_csv(raw_dir+str(metadata_filename), sep=&#39;,&#39;, header=&#39;infer&#39;, encoding=encoding, usecols=metadata_useCols)

    #Drop data with no API        
    downholeDataIN = downholeDataIN.dropna(subset=[id_col]) #Drop data with no API
    headerDataIN = headerDataIN.dropna(subset=[id_col]) #Drop metadata with no API

    #Drop data with no or missing location information
    headerDataIN = headerDataIN.dropna(subset=[ycol]) 
    headerDataIN = headerDataIN.dropna(subset=[x_col])
    
    #Reset index so index goes from 0 in numerical/integer order
    headerDataIN.reset_index(inplace=True, drop=True)
    downholeDataIN.reset_index(inplace=True, drop=True)
    
    #Print outputs to terminal, if designated
    if verbose:
        print(&#39;Data file has &#39; + str(downholeDataIN.shape[0])+&#39; valid well records.&#39;)
        print(&#34;Metadata file has &#34;+str(headerDataIN.shape[0])+&#34; unique wells with valid location information.&#34;)
    
    return downholeDataIN, headerDataIN

#Read file with xyz data
def read_xyz(rawdir, xyzfile, dtypes=None, verbose=False):
    &#34;&#34;&#34;Function to read file containing xyz data (elevation/location)

    Parameters
    ----------
    rawdir : str or pathlib.Path object
        String to the directory in which the xyz file is contained
    xyzfile : str
        String with the filename of the xyz file, including extension.
    dtypes : dict, default = None
        Dictionary containing the datatypes for the columns int he xyz file. If None, {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#39;LATITUDE&#39;:np.float64,&#39;LONGITUDE&#39;:np.float64,&#39;ELEV_FT&#39;:np.float64}, by default None
    verbose : bool, default = False
        Whether to print the number of xyz records to the terminal, by default False

    Returns
    -------
    pandas.DataFrame
        Pandas dataframe containing the elevation and location data
    &#34;&#34;&#34;

    if dtypes is None:
        xyzDTypes = {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#39;LATITUDE&#39;:np.float64,&#39;LONGITUDE&#39;:np.float64,&#39;ELEV_FT&#39;:np.float64}

    xyzDataIN = pd.read_csv(rawdir+str(xyzfile), sep=&#39;,&#39;, header=&#39;infer&#39;, dtype=xyzDTypes, index_col=&#39;ID&#39;)
    
    if verbose:
        print(&#39;XYZ file has &#39; + str(xyzDataIN.shape[0])+&#39; records with elevation and location.&#39;)
    return xyzDataIN

#Get filepath of resource in resource folder
def __get_resource_path(res):
    &#34;&#34;&#34;Function to get the path to the resource folder (used in other functions)

    Parameters
    ----------
    res : _type_
        _description_

    Returns
    -------
    _type_
        _description_
    &#34;&#34;&#34;
    repoPath = pathlib.Path(__file__).parent.parent
    repoPathStr = str(repoPath).replace(&#39;\\&#39;, &#39;/&#39;).replace(&#39;\\&#39;[0], &#39;/&#39;)
    resource = repoPathStr+&#39;/resources/&#39;+res
    return resource

#Read dictionary file into dictionary variable
def read_dict(file, keytype=&#39;np&#39;):
    &#34;&#34;&#34;Function to read a text file with a dictionary in it into a python dictionary

    Parameters
    ----------
    file : str or pathlib.Path object
        Filepath to the file of interest containing the dictionary text
    keytype : str, optional
        String indicating the datatypes used in the text, currently only &#39;np&#39; is implemented, by default &#39;np&#39;

    Returns
    -------
    dict
        Dictionary translated from text file.
    &#34;&#34;&#34;

    with open(file, &#39;r&#39;) as f:
        data= f.read()

    jsDict = json.loads(data)
    if keytype==&#39;np&#39;:
        for k in jsDict.keys():
            jsDict[k] = getattr(np, jsDict[k])
    
    return jsDict

#Define the datatypes for a dataframe
def define_dtypes(df, dtypes=None, dtype_file=None, dtype_dir=str(repoDir)+&#39;/resources/&#39;):
    &#34;&#34;&#34;Function to define datatypes of a dataframe, especially with file-indicated dyptes

    Parameters
    ----------
    df : pandas.DataFrame
        Pandas dataframe with columns whose datatypes need to be (re)defined
    dtypes : dict or None, default = None
        Dictionary containing datatypes, to be used in pandas.DataFrame.astype() function. If None, will read from file indicated by dtype_file (which must be defined, along with dtype_dir), by default None
    dtype_file : str or None, default = None
        Filename of file containing datatypes (txt file in dictionary format). If None, dtypes must be defined, by default None.
    dtype_dir : str or pathlib.Path obejct, default = str(repoDir)+&#39;/resources/&#39;
        Directory containing dtype_file, by default 

    Returns
    -------
    dfout : pandas.DataFrame
        Pandas dataframe containing redefined columns
    &#34;&#34;&#34;

    dfout = df.copy()
    
    if dtypes is None:
        if isinstance(dtype_dir, pathlib.PurePath):
            dtype_dir = dtype_dir.as_posix()
        if dtype_dir[-1] != &#39;/&#39;:
            dtype_dir = dtype_dir + &#39;/&#39;

        if dtype_file is None:
            print(&#39;ERROR: Either dtype_file (and dtype_dir) or dtypes must be defined&#39;)
            return 
        
        dtype_file = pathlib.Path(dtype_dir+dtype_file)
        dtypes = read_dict(file=dtype_file)
    
    for i in range(0, np.shape(dfout)[1]):
        dfout.iloc[:,i] = df.iloc[:,i].astype(dtypes[df.iloc[:,i].name])

    return dfout

#Define the search term filepaths
def get_search_terms(spec_dir=str(repoDir)+&#39;/resources/&#39;, specStartPattern=&#39;*SearchTerms-Specific*&#39;, start_dir=None, startGlobPattern = &#39;*SearchTerms-Start*&#39;):
    &#34;&#34;&#34;Read in dictionary files for downhole data

    Parameters
    ----------
    spec_dir : str or pathlib.Path, optional
        Directory where the file containing the specific search terms is located, by default str(repoDir)+&#39;/resources/&#39;
    specStartPattern : str, optional
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Specific*&#39;
    start_dir : str or None, optional
        Directory where the file containing the start search terms is located, by default None
    startGlobPattern : str, optional
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Start*&#39;

    Returns
    -------
    (specTermsPath, startTermsPath) : tuple
        Tuple containing the pandas dataframe with specific search terms and one with start search terms
    &#34;&#34;&#34;
    
    #specTermsFile = &#34;SearchTerms-Specific_BedrockOrNo_2022-09.csv&#34; #Specific matches
    #startTermsFile = &#34;SearchTerms-Start_BedrockOrNo.csv&#34; #Wildcard matches for the start of the description
    
    if start_dir is None:
        start_dir = spec_dir

    specTermsPath = get_most_recent(spec_dir, specStartPattern)
    startTermsPath = get_most_recent(start_dir, startGlobPattern)
    
    return specTermsPath, startTermsPath

#Read files into pandas dataframes
def read_dictionary_terms(dict_file, cols=None, col_types=None, dictionary_type=None, class_flag=1, rem_extra_cols=True):
    &#34;&#34;&#34;Function to read dictionary terms from file into pandas dataframe

    Parameters
    ----------
    dict_file : str or pathlib.Path object, or list of these
        File or list of files to be read
    cols : dict or None, default = None
        Dictionary containing columns to be renamed. If None, see source code for renaming actions, by default None
    col_types : dict or None, default = None
        Dictionary containing column types to be set. If None, see source code for renaming actions, by default None, by default None
    dictionary_type : str or None, {None, &#39;exact&#39;, &#39;start&#39;}
        Indicator of which kind of dictionary terms to be read in: None, &#39;exact&#39; or &#39;start&#39;, by default None.
            - If None, uses name of file to try to determine. If it cannot, it will default to using the classification flag from class_flag
            - If &#39;exact&#39;, will be used to search for exact matches to geologic descriptions
            - If &#39;start&#39;, will be used as with the .startswith() string method to find inexact matches to geologic descriptions
    class_flag : int, default = 1
        Classification flag to be used if dictionary_type is None and cannot be otherwise determined, by default 1
    rem_extra_cols : bool, default = True
        Whether to remove the extra columns from the input file after it is read in as a pandas dataframe, by default True

    Returns
    -------
    dict_terms : pandas.DataFrame
        Pandas dataframe with formatting ready to be used in the classification steps of this package
    &#34;&#34;&#34;
    #Read files into pandas dataframes
    dict_terms = []
    if type(dict_file) is list:
        for f in dict_file:
            dict_terms.append(pd.read_csv(f))
            if &#39;ID&#39; in dict_terms.columns:
                dict_terms.set_index(&#39;ID&#39;, drop=True, inplace=True)
    else:
        dict_terms.append(pd.read_csv(dict_file))
        if &#39;ID&#39; in dict_terms[-1].columns:
            dict_terms[-1].set_index(&#39;ID&#39;, drop=True, inplace=True)
        dict_file = [dict_file]

    #Rename important columns
    searchTermList = [&#39;searchterm&#39;, &#39;search&#39;, &#39;exact&#39;]
    startTermList = [&#39;startterm&#39;, &#39;start&#39;, &#39;startswith&#39;]
                
    #Recast all columns to datatypes of headerData to defined types
    dict_termDtypes = {&#39;FORMATION&#39;:str,&#39;INTERPRETATION&#39;:str, &#39;CLASS_FLAG&#39;:np.uint8}

    if dictionary_type is None:
        dictionary_type=&#39;&#39;

    for i, d in enumerate(dict_terms):
        if dictionary_type.lower() in searchTermList or (dictionary_type==&#39;&#39; and &#39;spec&#39; in str(dict_file[i]).lower()):
            d[&#39;CLASS_FLAG&#39;] = 1
        elif dictionary_type.lower() in startTermList or (dictionary_type==&#39;&#39; and &#39;start&#39; in str(dict_file[i]).lower()):
            d[&#39;CLASS_FLAG&#39;] = 4 #Start term classification flag
        else:
            d[&#39;CLASS_FLAG&#39;] = class_flag #Custom classification flag, defined as argument
            #1=specific match,2 (not defined), 3: bedrock classification for obvious bedrock, 4: Wildcard/start term

        if cols is None:
            if &#39;SearchTerm&#39; in d.columns:
                d.rename(columns={&#39;SearchTerm&#39;:&#39;FORMATION&#39;}, inplace=True)
            if &#39;StartTerm&#39; in d.columns:
                d.rename(columns={&#39;StartTerm&#39;:&#39;FORMATION&#39;}, inplace=True)        
            if &#39;InterpUpdate&#39; in d.columns:
                d.rename(columns={&#39;InterpUpdate&#39;:&#39;INTERPRETATION&#39;}, inplace=True)
        else:
            for k in list(cols.keys()):
                d.rename(columns={k:cols[k]}, inplace=True)
        if col_types is None:
            pass
        else:
            for k in list(col_types.keys()):
                d.rename(columns={k:col_types[k]}, inplace=True)
    
        for i in range(0, np.shape(d)[1]):
            if d.iloc[:,i].name in list(dict_termDtypes.keys()):
                d.iloc[:,i] = d.iloc[:,i].astype(dict_termDtypes[d.iloc[:,i].name])
    
        #Delete duplicate definitions
        d.drop_duplicates(subset=&#39;FORMATION&#39;,inplace=True) #Apparently, there are some duplicate definitions, which need to be deleted first
        d.reset_index(inplace=True, drop=True)

    #If only one file designated, convert it so it&#39;s no longer a list, but just a dataframe
    if len(dict_terms) == 1:
        dict_terms = dict_terms[0]
    
    #Whether to remove extra columns that aren&#39;t needed from dataframe
    if rem_extra_cols:
        dict_terms = dict_terms[[&#39;FORMATION&#39;, &#39;INTERPRETATION&#39;, &#39;CLASS_FLAG&#39;]]

    return dict_terms

#Function to read lithology file into pandas dataframe
def read_lithologies(litho_dir=None, lith_file=None, interp_col=&#39;LITHOLOGY&#39;, target_col=&#39;CODE&#39;, use_cols=None):
    &#34;&#34;&#34;Function to read lithology file into pandas dataframe

    Parameters
    ----------
    litho_dir : str or pathlib.Path object, default = None
        Directory where lithology file is located. If None, default is in source coude, by default None
    lith_file : str, default = None
        Filename of lithology file. If None, default is in source coude, by default None
    interp_col : str, default = &#39;LITHOLOGY&#39;
        Column to used to match interpretations
    target_col : str, default = &#39;CODE&#39;
        Column to be used as target code
    use_cols : list, default = None
        Which columns to use when reading in dataframe. If None, defaults to [&#39;LITHOLOGY&#39;, &#39;CODE&#39;].
    Returns
    -------
    pandas.DataFrame
        Pandas dataframe with lithology information
    &#34;&#34;&#34;
    #dictDir = &#34;\\\\isgs-sinkhole\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SupportingDocs\\&#34;
    if litho_dir is None:
        litho_dir=str(repoDir)+&#39;/resources/&#39;
    elif isinstance(litho_dir, pathlib.PurePath):
        litho_dir = litho_dir.as_posix()
    
    litho_dir.replace(&#39;\\&#39;, &#39;/&#39;)
    litho_dir.replace(&#39;\\&#39;[-1], &#39;/&#39;)
    if litho_dir[-1] != &#39;/&#39;:
        litho_dir = litho_dir+&#39;/&#39;

    if lith_file is None:
        lith_file=&#39;Lithology_Interp_FineCoarse.csv&#39;
    
    if use_cols is None:
        use_cols = [&#39;LITHOLOGY&#39;, &#39;CODE&#39;]

    lithFPath = pathlib.Path(litho_dir+lith_file)
    lithoDF = pd.read_csv(lithFPath, usecols=use_cols)

    lithoDF.rename(columns={interp_col:&#39;INTERPRETATION&#39;, target_col:&#39;TARGET&#39;}, inplace=True)

    return lithoDF</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="w4h.read.define_dtypes"><code class="name flex">
<span>def <span class="ident">define_dtypes</span></span>(<span>df, dtypes=None, dtype_file=None, dtype_dir='C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology/resources/')</span>
</code></dt>
<dd>
<div class="desc"><p>Function to define datatypes of a dataframe, especially with file-indicated dyptes</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with columns whose datatypes need to be (re)defined</dd>
<dt><strong><code>dtypes</code></strong> :&ensp;<code>dict</code> or <code>None</code>, default <code>= None</code></dt>
<dd>Dictionary containing datatypes, to be used in pandas.DataFrame.astype() function. If None, will read from file indicated by dtype_file (which must be defined, along with dtype_dir), by default None</dd>
<dt><strong><code>dtype_file</code></strong> :&ensp;<code>str</code> or <code>None</code>, default <code>= None</code></dt>
<dd>Filename of file containing datatypes (txt file in dictionary format). If None, dtypes must be defined, by default None.</dd>
<dt><strong><code>dtype_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path obejct</code>, default <code>= str(repoDir)+'/resources/'</code></dt>
<dd>Directory containing dtype_file, by default</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dfout</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing redefined columns</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def define_dtypes(df, dtypes=None, dtype_file=None, dtype_dir=str(repoDir)+&#39;/resources/&#39;):
    &#34;&#34;&#34;Function to define datatypes of a dataframe, especially with file-indicated dyptes

    Parameters
    ----------
    df : pandas.DataFrame
        Pandas dataframe with columns whose datatypes need to be (re)defined
    dtypes : dict or None, default = None
        Dictionary containing datatypes, to be used in pandas.DataFrame.astype() function. If None, will read from file indicated by dtype_file (which must be defined, along with dtype_dir), by default None
    dtype_file : str or None, default = None
        Filename of file containing datatypes (txt file in dictionary format). If None, dtypes must be defined, by default None.
    dtype_dir : str or pathlib.Path obejct, default = str(repoDir)+&#39;/resources/&#39;
        Directory containing dtype_file, by default 

    Returns
    -------
    dfout : pandas.DataFrame
        Pandas dataframe containing redefined columns
    &#34;&#34;&#34;

    dfout = df.copy()
    
    if dtypes is None:
        if isinstance(dtype_dir, pathlib.PurePath):
            dtype_dir = dtype_dir.as_posix()
        if dtype_dir[-1] != &#39;/&#39;:
            dtype_dir = dtype_dir + &#39;/&#39;

        if dtype_file is None:
            print(&#39;ERROR: Either dtype_file (and dtype_dir) or dtypes must be defined&#39;)
            return 
        
        dtype_file = pathlib.Path(dtype_dir+dtype_file)
        dtypes = read_dict(file=dtype_file)
    
    for i in range(0, np.shape(dfout)[1]):
        dfout.iloc[:,i] = df.iloc[:,i].astype(dtypes[df.iloc[:,i].name])

    return dfout</code></pre>
</details>
</dd>
<dt id="w4h.read.file_setup"><code class="name flex">
<span>def <span class="ident">file_setup</span></span>(<span>db_dir, metadata_dir=None, xyz_dir=None, data_pattern='*ISGS_DOWNHOLE_DATA*.txt', metadata_pattern='*ISGS_HEADER*.txt', xyz_pattern='*xyzData*', verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to setup files, assuming data, metadata, and elevation/location are in separate files (there should be one "key"/identifying column consistent across all files to join/merge them later)</p>
<p>This function may not be useful if files are organized differently than this structure. If that is the case, it is recommended to use the get_most_recent() function for each individual file needed.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>db_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, optional</dt>
<dd>Str or pathlib.Path to directory containing input files, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>metadata_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, optional</dt>
<dd>Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>xyz_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, optional</dt>
<dd>Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>data_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Pattern used by pathlib.glob() to get the most recent data file, by default '<em>ISGS_DOWNHOLE_DATA</em>.txt'</dd>
<dt><strong><code>metadata_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Pattern used by pathlib.glob() to get the most recent metadata file, by default '<em>ISGS_HEADER</em>.txt'</dd>
<dt><strong><code>xyz_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Pattern used by pathlib.glob() to get the most recent elevation/location file, by default '<em>xyzData</em>'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print name of files to terminal, by default True</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_type_</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def file_setup(db_dir, metadata_dir=None, xyz_dir=None, data_pattern=&#39;*ISGS_DOWNHOLE_DATA*.txt&#39;, metadata_pattern=&#39;*ISGS_HEADER*.txt&#39;, xyz_pattern= &#39;*xyzData*&#39;, verbose=False):
    &#34;&#34;&#34;Function to setup files, assuming data, metadata, and elevation/location are in separate files (there should be one &#34;key&#34;/identifying column consistent across all files to join/merge them later)

    This function may not be useful if files are organized differently than this structure. If that is the case, it is recommended to use the get_most_recent() function for each individual file needed.

    Parameters
    ----------
    db_dir : str or pathlib.Path object, optional
        Str or pathlib.Path to directory containing input files, by default str(repoDir)+&#39;/resources&#39;
    metadata_dir : str or pathlib.Path object, optional
        Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+&#39;/resources&#39;
    xyz_dir : str or pathlib.Path object, optional
        Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+&#39;/resources&#39;
    data_pattern : str, optional
        Pattern used by pathlib.glob() to get the most recent data file, by default &#39;*ISGS_DOWNHOLE_DATA*.txt&#39;
    metadata_pattern : str, optional
        Pattern used by pathlib.glob() to get the most recent metadata file, by default &#39;*ISGS_HEADER*.txt&#39;
    xyz_pattern : str, optional
        Pattern used by pathlib.glob() to get the most recent elevation/location file, by default &#39;*xyzData*&#39;
    verbose : bool, default = False
        Whether to print name of files to terminal, by default True

    Returns
    -------
    _type_
        _description_
    &#34;&#34;&#34;
    #Define  filepath variables to be used later for reading/writing files
    raw_directory = pathlib.Path(db_dir)
    if metadata_dir is None:
        metadata_dir=raw_directory
    else:
        metadata_dir=pathlib.Path(metadata_dir)

    if xyz_dir is None:
        xyz_dir=raw_directory
    else:
        xyz_dir=pathlib.Path(xyz_dir)

    downholeDataFILE = get_most_recent(raw_directory, data_pattern)
    headerDataFILE = get_most_recent(metadata_dir, metadata_pattern)
    xyzInFILE = get_most_recent(xyz_dir,xyz_pattern)

    downholeDataPATH = pathlib.Path(downholeDataFILE)
    headerDataPATH = pathlib.Path(headerDataFILE)
    xyzInPATH = pathlib.Path(xyzInFILE)

    if verbose:
        print(&#39;Using the following files:\n&#39;)
        print(&#39;\t&#39;, downholeDataFILE)
        print(&#39;\t&#39;, headerDataFILE)
        print(&#39;\t&#39;, xyzInFILE)

    #Define datatypes, to use later
    #downholeDataDTYPES = {&#39;ID&#39;:np.uint32, &#34;API_NUMBER&#34;:np.uint64,&#34;TABLE_NAME&#34;:str,&#34;WHO&#34;:str,&#34;INTERPRET_DATE&#34;:str,&#34;FORMATION&#34;:str,&#34;THICKNESS&#34;:np.float64,&#34;TOP&#34;:np.float64,&#34;BOTTOM&#34;:np.float64}
    #headerDataDTYPES = {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#34;TDFORMATION&#34;:str,&#34;PRODFORM&#34;:str,&#34;TOTAL_DEPTH&#34;:np.float64,&#34;SECTION&#34;:np.float64,&#34;TWP&#34;:np.float64,&#34;TDIR&#34;:str,&#34;RNG&#34;:np.float64,&#34;RDIR&#34;:str,&#34;MERIDIAN&#34;:np.float64,&#34;FARM_NAME&#34;:str,&#34;NSFOOT&#34;:np.float64,&#34;NSDIR&#34;:str,&#34;EWFOOT&#34;:np.float64,&#34;EWDIR&#34;:str,&#34;QUARTERS&#34;:str,&#34;ELEVATION&#34;:np.float64,&#34;ELEVREF&#34;:str,&#34;COMP_DATE&#34;:str,&#34;STATUS&#34;:str,&#34;FARM_NUM&#34;:str,&#34;COUNTY_CODE&#34;:np.float64,&#34;PERMIT_NUMBER&#34;:str,&#34;COMPANY_NAME&#34;:str,&#34;COMPANY_CODE&#34;:str,&#34;PERMIT_DATE&#34;:str,&#34;CORNER&#34;:str,&#34;LATITUDE&#34;:np.float64,&#34;LONGITUDE&#34;:np.float64,&#34;ENTERED_BY&#34;:str,&#34;UPDDATE&#34;:str,&#34;ELEVSOURCE&#34;:str, &#34;ELEV_FT&#34;:np.float64}
    return downholeDataPATH, headerDataPATH, xyzInPATH</code></pre>
</details>
</dd>
<dt id="w4h.read.get_current_date"><code class="name flex">
<span>def <span class="ident">get_current_date</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="gets-the-current-date-to-help-with-finding-the-most-recent-file">Gets The Current Date To Help With Finding The Most Recent File</h2>
<h2 id="parameters">Parameters</h2>
<p>None</p>
<hr>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>todayDate
</code></dt>
<dd>datetime object with today's date</dd>
<dt><code>dateSuffix
</code></dt>
<dd>str to use for naming output files</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_current_date():
    &#34;&#34;&#34; Gets the current date to help with finding the most recent file
        ---------------------
        Parameters:
            None

        ---------------------
        Returns:
            todayDate   : datetime object with today&#39;s date
            dateSuffix  : str to use for naming output files
    &#34;&#34;&#34;
    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)
    dateSuffix = &#39;_&#39;+todayDateStr
    return todayDate, dateSuffix</code></pre>
</details>
</dd>
<dt id="w4h.read.get_most_recent"><code class="name flex">
<span>def <span class="ident">get_most_recent</span></span>(<span>dir='C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology/resources', glob_pattern='*', verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to find the most recent file with the indicated pattern, using pathlib.glob function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, optional</dt>
<dd>Directory in which to find the most recent file, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>glob_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>String used by the pathlib.glob() function/method for searching, by default '*'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pathlib.Path object</code></dt>
<dd>Pathlib Path object of the most recent file fitting the glob pattern indicated in the glob_pattern parameter.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_most_recent(dir=str(repoDir)+&#39;/resources&#39;, glob_pattern=&#39;*&#39;, verbose=True):
    &#34;&#34;&#34;Function to find the most recent file with the indicated pattern, using pathlib.glob function.

    Parameters
    ----------
    dir : str or pathlib.Path object, optional
        Directory in which to find the most recent file, by default str(repoDir)+&#39;/resources&#39;
    glob_pattern : str, optional
        String used by the pathlib.glob() function/method for searching, by default &#39;*&#39;

    Returns
    -------
    pathlib.Path object
        Pathlib Path object of the most recent file fitting the glob pattern indicated in the glob_pattern parameter.
    &#34;&#34;&#34;
    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)

    files = pathlib.Path(dir).rglob(glob_pattern) #Get all the files that fit the pattern

    fileDates = []
    for f in files: #Get the file dates from their file modification times
        fileDates.append(np.datetime64(datetime.datetime.fromtimestamp(os.path.getmtime(f))))
    globInd = np.argmin(np.datetime64(todayDateStr) - np.array(fileDates)) #Find the index of the most recent file

    #Iterate through glob/files again (need to recreate glob)
    files = pathlib.Path(dir).rglob(glob_pattern)
    for j, f in enumerate(files):
        if j == globInd:
            mostRecentFile=f
            break
    
    if verbose:
        print(&#39;Most Recent version of this file is : &#39;+mostRecentFile.name)

    return mostRecentFile</code></pre>
</details>
</dd>
<dt id="w4h.read.get_search_terms"><code class="name flex">
<span>def <span class="ident">get_search_terms</span></span>(<span>spec_dir='C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology/resources/', specStartPattern='*SearchTerms-Specific*', start_dir=None, startGlobPattern='*SearchTerms-Start*')</span>
</code></dt>
<dd>
<div class="desc"><p>Read in dictionary files for downhole data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spec_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, optional</dt>
<dd>Directory where the file containing the specific search terms is located, by default str(repoDir)+'/resources/'</dd>
<dt><strong><code>specStartPattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default '<em>SearchTerms-Specific</em>'</dd>
<dt><strong><code>start_dir</code></strong> :&ensp;<code>str</code> or <code>None</code>, optional</dt>
<dd>Directory where the file containing the start search terms is located, by default None</dd>
<dt><strong><code>startGlobPattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default '<em>SearchTerms-Start</em>'</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(specTermsPath, startTermsPath) : tuple
Tuple containing the pandas dataframe with specific search terms and one with start search terms</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_search_terms(spec_dir=str(repoDir)+&#39;/resources/&#39;, specStartPattern=&#39;*SearchTerms-Specific*&#39;, start_dir=None, startGlobPattern = &#39;*SearchTerms-Start*&#39;):
    &#34;&#34;&#34;Read in dictionary files for downhole data

    Parameters
    ----------
    spec_dir : str or pathlib.Path, optional
        Directory where the file containing the specific search terms is located, by default str(repoDir)+&#39;/resources/&#39;
    specStartPattern : str, optional
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Specific*&#39;
    start_dir : str or None, optional
        Directory where the file containing the start search terms is located, by default None
    startGlobPattern : str, optional
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Start*&#39;

    Returns
    -------
    (specTermsPath, startTermsPath) : tuple
        Tuple containing the pandas dataframe with specific search terms and one with start search terms
    &#34;&#34;&#34;
    
    #specTermsFile = &#34;SearchTerms-Specific_BedrockOrNo_2022-09.csv&#34; #Specific matches
    #startTermsFile = &#34;SearchTerms-Start_BedrockOrNo.csv&#34; #Wildcard matches for the start of the description
    
    if start_dir is None:
        start_dir = spec_dir

    specTermsPath = get_most_recent(spec_dir, specStartPattern)
    startTermsPath = get_most_recent(start_dir, startGlobPattern)
    
    return specTermsPath, startTermsPath</code></pre>
</details>
</dd>
<dt id="w4h.read.read_dict"><code class="name flex">
<span>def <span class="ident">read_dict</span></span>(<span>file, keytype='np')</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read a text file with a dictionary in it into a python dictionary</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>Filepath to the file of interest containing the dictionary text</dd>
<dt><strong><code>keytype</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>String indicating the datatypes used in the text, currently only 'np' is implemented, by default 'np'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Dictionary translated from text file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_dict(file, keytype=&#39;np&#39;):
    &#34;&#34;&#34;Function to read a text file with a dictionary in it into a python dictionary

    Parameters
    ----------
    file : str or pathlib.Path object
        Filepath to the file of interest containing the dictionary text
    keytype : str, optional
        String indicating the datatypes used in the text, currently only &#39;np&#39; is implemented, by default &#39;np&#39;

    Returns
    -------
    dict
        Dictionary translated from text file.
    &#34;&#34;&#34;

    with open(file, &#39;r&#39;) as f:
        data= f.read()

    jsDict = json.loads(data)
    if keytype==&#39;np&#39;:
        for k in jsDict.keys():
            jsDict[k] = getattr(np, jsDict[k])
    
    return jsDict</code></pre>
</details>
</dd>
<dt id="w4h.read.read_dictionary_terms"><code class="name flex">
<span>def <span class="ident">read_dictionary_terms</span></span>(<span>dict_file, cols=None, col_types=None, dictionary_type=None, class_flag=1, rem_extra_cols=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read dictionary terms from file into pandas dataframe</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dict_file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>list</code> of <code>these</code></dt>
<dd>File or list of files to be read</dd>
<dt><strong><code>cols</code></strong> :&ensp;<code>dict</code> or <code>None</code>, default <code>= None</code></dt>
<dd>Dictionary containing columns to be renamed. If None, see source code for renaming actions, by default None</dd>
<dt><strong><code>col_types</code></strong> :&ensp;<code>dict</code> or <code>None</code>, default <code>= None</code></dt>
<dd>Dictionary containing column types to be set. If None, see source code for renaming actions, by default None, by default None</dd>
<dt><strong><code>dictionary_type</code></strong> :&ensp;<code>str</code> or <code>None, {None, 'exact', 'start'}</code></dt>
<dd>Indicator of which kind of dictionary terms to be read in: None, 'exact' or 'start', by default None.
- If None, uses name of file to try to determine. If it cannot, it will default to using the classification flag from class_flag
- If 'exact', will be used to search for exact matches to geologic descriptions
- If 'start', will be used as with the .startswith() string method to find inexact matches to geologic descriptions</dd>
<dt><strong><code>class_flag</code></strong> :&ensp;<code>int</code>, default <code>= 1</code></dt>
<dd>Classification flag to be used if dictionary_type is None and cannot be otherwise determined, by default 1</dd>
<dt><strong><code>rem_extra_cols</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to remove the extra columns from the input file after it is read in as a pandas dataframe, by default True</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict_terms</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with formatting ready to be used in the classification steps of this package</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_dictionary_terms(dict_file, cols=None, col_types=None, dictionary_type=None, class_flag=1, rem_extra_cols=True):
    &#34;&#34;&#34;Function to read dictionary terms from file into pandas dataframe

    Parameters
    ----------
    dict_file : str or pathlib.Path object, or list of these
        File or list of files to be read
    cols : dict or None, default = None
        Dictionary containing columns to be renamed. If None, see source code for renaming actions, by default None
    col_types : dict or None, default = None
        Dictionary containing column types to be set. If None, see source code for renaming actions, by default None, by default None
    dictionary_type : str or None, {None, &#39;exact&#39;, &#39;start&#39;}
        Indicator of which kind of dictionary terms to be read in: None, &#39;exact&#39; or &#39;start&#39;, by default None.
            - If None, uses name of file to try to determine. If it cannot, it will default to using the classification flag from class_flag
            - If &#39;exact&#39;, will be used to search for exact matches to geologic descriptions
            - If &#39;start&#39;, will be used as with the .startswith() string method to find inexact matches to geologic descriptions
    class_flag : int, default = 1
        Classification flag to be used if dictionary_type is None and cannot be otherwise determined, by default 1
    rem_extra_cols : bool, default = True
        Whether to remove the extra columns from the input file after it is read in as a pandas dataframe, by default True

    Returns
    -------
    dict_terms : pandas.DataFrame
        Pandas dataframe with formatting ready to be used in the classification steps of this package
    &#34;&#34;&#34;
    #Read files into pandas dataframes
    dict_terms = []
    if type(dict_file) is list:
        for f in dict_file:
            dict_terms.append(pd.read_csv(f))
            if &#39;ID&#39; in dict_terms.columns:
                dict_terms.set_index(&#39;ID&#39;, drop=True, inplace=True)
    else:
        dict_terms.append(pd.read_csv(dict_file))
        if &#39;ID&#39; in dict_terms[-1].columns:
            dict_terms[-1].set_index(&#39;ID&#39;, drop=True, inplace=True)
        dict_file = [dict_file]

    #Rename important columns
    searchTermList = [&#39;searchterm&#39;, &#39;search&#39;, &#39;exact&#39;]
    startTermList = [&#39;startterm&#39;, &#39;start&#39;, &#39;startswith&#39;]
                
    #Recast all columns to datatypes of headerData to defined types
    dict_termDtypes = {&#39;FORMATION&#39;:str,&#39;INTERPRETATION&#39;:str, &#39;CLASS_FLAG&#39;:np.uint8}

    if dictionary_type is None:
        dictionary_type=&#39;&#39;

    for i, d in enumerate(dict_terms):
        if dictionary_type.lower() in searchTermList or (dictionary_type==&#39;&#39; and &#39;spec&#39; in str(dict_file[i]).lower()):
            d[&#39;CLASS_FLAG&#39;] = 1
        elif dictionary_type.lower() in startTermList or (dictionary_type==&#39;&#39; and &#39;start&#39; in str(dict_file[i]).lower()):
            d[&#39;CLASS_FLAG&#39;] = 4 #Start term classification flag
        else:
            d[&#39;CLASS_FLAG&#39;] = class_flag #Custom classification flag, defined as argument
            #1=specific match,2 (not defined), 3: bedrock classification for obvious bedrock, 4: Wildcard/start term

        if cols is None:
            if &#39;SearchTerm&#39; in d.columns:
                d.rename(columns={&#39;SearchTerm&#39;:&#39;FORMATION&#39;}, inplace=True)
            if &#39;StartTerm&#39; in d.columns:
                d.rename(columns={&#39;StartTerm&#39;:&#39;FORMATION&#39;}, inplace=True)        
            if &#39;InterpUpdate&#39; in d.columns:
                d.rename(columns={&#39;InterpUpdate&#39;:&#39;INTERPRETATION&#39;}, inplace=True)
        else:
            for k in list(cols.keys()):
                d.rename(columns={k:cols[k]}, inplace=True)
        if col_types is None:
            pass
        else:
            for k in list(col_types.keys()):
                d.rename(columns={k:col_types[k]}, inplace=True)
    
        for i in range(0, np.shape(d)[1]):
            if d.iloc[:,i].name in list(dict_termDtypes.keys()):
                d.iloc[:,i] = d.iloc[:,i].astype(dict_termDtypes[d.iloc[:,i].name])
    
        #Delete duplicate definitions
        d.drop_duplicates(subset=&#39;FORMATION&#39;,inplace=True) #Apparently, there are some duplicate definitions, which need to be deleted first
        d.reset_index(inplace=True, drop=True)

    #If only one file designated, convert it so it&#39;s no longer a list, but just a dataframe
    if len(dict_terms) == 1:
        dict_terms = dict_terms[0]
    
    #Whether to remove extra columns that aren&#39;t needed from dataframe
    if rem_extra_cols:
        dict_terms = dict_terms[[&#39;FORMATION&#39;, &#39;INTERPRETATION&#39;, &#39;CLASS_FLAG&#39;]]

    return dict_terms</code></pre>
</details>
</dd>
<dt id="w4h.read.read_lithologies"><code class="name flex">
<span>def <span class="ident">read_lithologies</span></span>(<span>litho_dir=None, lith_file=None, interp_col='LITHOLOGY', target_col='CODE', use_cols=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read lithology file into pandas dataframe</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>litho_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, default <code>= None</code></dt>
<dd>Directory where lithology file is located. If None, default is in source coude, by default None</dd>
<dt><strong><code>lith_file</code></strong> :&ensp;<code>str</code>, default <code>= None</code></dt>
<dd>Filename of lithology file. If None, default is in source coude, by default None</dd>
<dt><strong><code>interp_col</code></strong> :&ensp;<code>str</code>, default <code>= 'LITHOLOGY'</code></dt>
<dd>Column to used to match interpretations</dd>
<dt><strong><code>target_col</code></strong> :&ensp;<code>str</code>, default <code>= 'CODE'</code></dt>
<dd>Column to be used as target code</dd>
<dt><strong><code>use_cols</code></strong> :&ensp;<code>list</code>, default <code>= None</code></dt>
<dd>Which columns to use when reading in dataframe. If None, defaults to ['LITHOLOGY', 'CODE'].</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with lithology information</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_lithologies(litho_dir=None, lith_file=None, interp_col=&#39;LITHOLOGY&#39;, target_col=&#39;CODE&#39;, use_cols=None):
    &#34;&#34;&#34;Function to read lithology file into pandas dataframe

    Parameters
    ----------
    litho_dir : str or pathlib.Path object, default = None
        Directory where lithology file is located. If None, default is in source coude, by default None
    lith_file : str, default = None
        Filename of lithology file. If None, default is in source coude, by default None
    interp_col : str, default = &#39;LITHOLOGY&#39;
        Column to used to match interpretations
    target_col : str, default = &#39;CODE&#39;
        Column to be used as target code
    use_cols : list, default = None
        Which columns to use when reading in dataframe. If None, defaults to [&#39;LITHOLOGY&#39;, &#39;CODE&#39;].
    Returns
    -------
    pandas.DataFrame
        Pandas dataframe with lithology information
    &#34;&#34;&#34;
    #dictDir = &#34;\\\\isgs-sinkhole\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SupportingDocs\\&#34;
    if litho_dir is None:
        litho_dir=str(repoDir)+&#39;/resources/&#39;
    elif isinstance(litho_dir, pathlib.PurePath):
        litho_dir = litho_dir.as_posix()
    
    litho_dir.replace(&#39;\\&#39;, &#39;/&#39;)
    litho_dir.replace(&#39;\\&#39;[-1], &#39;/&#39;)
    if litho_dir[-1] != &#39;/&#39;:
        litho_dir = litho_dir+&#39;/&#39;

    if lith_file is None:
        lith_file=&#39;Lithology_Interp_FineCoarse.csv&#39;
    
    if use_cols is None:
        use_cols = [&#39;LITHOLOGY&#39;, &#39;CODE&#39;]

    lithFPath = pathlib.Path(litho_dir+lith_file)
    lithoDF = pd.read_csv(lithFPath, usecols=use_cols)

    lithoDF.rename(columns={interp_col:&#39;INTERPRETATION&#39;, target_col:&#39;TARGET&#39;}, inplace=True)

    return lithoDF</code></pre>
</details>
</dd>
<dt id="w4h.read.read_raw_txt"><code class="name flex">
<span>def <span class="ident">read_raw_txt</span></span>(<span>raw_dir, data_filename, metadata_filename, data_cols=None, metadata_cols=None, x_col='LONGITUDE', ycol='LATITUDE', id_col='API_NUMBER', encoding='latin-1', verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Easy function to read raw .txt files output from (for example), an Access database</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>raw_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>String or pathlib.Path to directory containing the files.</dd>
<dt><strong><code>data_filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the file containing data, including the extension.</dd>
<dt><strong><code>metadata_filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the file containing metadata, including the extension.</dd>
<dt><strong><code>data_cols</code></strong> :&ensp;<code>list</code>, default <code>= None</code></dt>
<dd>List with strings with names of columns from txt file to keep after reading. If None, ["API_NUMBER","TABLE_NAME","FORMATION","THICKNESS","TOP","BOTTOM"], by default None.</dd>
<dt><strong><code>metadata_cols</code></strong> :&ensp;<code>list</code>, default <code>= None</code></dt>
<dd>List with strings with names of columns from txt file to keep after reading. If None, ['API_NUMBER',"TOTAL_DEPTH","SECTION","TWP","TDIR","RNG","RDIR","MERIDIAN","QUARTERS","ELEVATION","ELEVREF","COUNTY_CODE","LATITUDE","LONGITUDE","ELEVSOURCE"], by default None</dd>
<dt><strong><code>x_col</code></strong> :&ensp;<code>str</code>, default <code>= 'LONGITUDE'</code></dt>
<dd>Name of column in metadata file indicating the x-location of the well, by default 'LONGITUDE'</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default <code>= 'LATITUDE'</code></dt>
<dd>Name of the column in metadata file indicating the y-location of the well, by default 'LATITUDE'</dd>
<dt><strong><code>id_col</code></strong> :&ensp;<code>str</code>, default <code>= 'API_NUMBER'</code></dt>
<dd>Name of the column with the key/identifier that will be used to merge data later, by default 'API_NUMBER'</dd>
<dt><strong><code>encoding</code></strong> :&ensp;<code>str</code>, default <code>= 'latin-1'</code></dt>
<dd>Encoding of the data in the input files, by default 'latin-1'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print the number of rows in the input columns, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(pandas.DataFrame, pandas.DataFrame)
Tuple/list with two pandas dataframes: (data, metadata)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_raw_txt(raw_dir, data_filename, metadata_filename, data_cols=None, metadata_cols=None, x_col=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, id_col=&#39;API_NUMBER&#39;, encoding=&#39;latin-1&#39;, verbose=False):
    &#34;&#34;&#34;Easy function to read raw .txt files output from (for example), an Access database

    Parameters
    ----------
    raw_dir : str or pathlib.Path object
        String or pathlib.Path to directory containing the files.
    data_filename : str
        Filename of the file containing data, including the extension.
    metadata_filename : str
        Filename of the file containing metadata, including the extension.
    data_cols : list, default = None
        List with strings with names of columns from txt file to keep after reading. If None, [&#34;API_NUMBER&#34;,&#34;TABLE_NAME&#34;,&#34;FORMATION&#34;,&#34;THICKNESS&#34;,&#34;TOP&#34;,&#34;BOTTOM&#34;], by default None.
    metadata_cols : list, default = None
        List with strings with names of columns from txt file to keep after reading. If None, [&#39;API_NUMBER&#39;,&#34;TOTAL_DEPTH&#34;,&#34;SECTION&#34;,&#34;TWP&#34;,&#34;TDIR&#34;,&#34;RNG&#34;,&#34;RDIR&#34;,&#34;MERIDIAN&#34;,&#34;QUARTERS&#34;,&#34;ELEVATION&#34;,&#34;ELEVREF&#34;,&#34;COUNTY_CODE&#34;,&#34;LATITUDE&#34;,&#34;LONGITUDE&#34;,&#34;ELEVSOURCE&#34;], by default None
    x_col : str, default = &#39;LONGITUDE&#39;
        Name of column in metadata file indicating the x-location of the well, by default &#39;LONGITUDE&#39;
    ycol : str, default = &#39;LATITUDE&#39;
        Name of the column in metadata file indicating the y-location of the well, by default &#39;LATITUDE&#39;
    id_col : str, default = &#39;API_NUMBER&#39;
        Name of the column with the key/identifier that will be used to merge data later, by default &#39;API_NUMBER&#39;
    encoding : str, default = &#39;latin-1&#39;
        Encoding of the data in the input files, by default &#39;latin-1&#39;
    verbose : bool, default = False
        Whether to print the number of rows in the input columns, by default False

    Returns
    -------
    (pandas.DataFrame, pandas.DataFrame)
        Tuple/list with two pandas dataframes: (data, metadata)
    &#34;&#34;&#34;

    if data_cols is None:
        data_useCols = [&#34;API_NUMBER&#34;,&#34;TABLE_NAME&#34;,&#34;FORMATION&#34;,&#34;THICKNESS&#34;,&#34;TOP&#34;,&#34;BOTTOM&#34;]
    if metadata_cols is None:
        metadata_useCols = [&#39;API_NUMBER&#39;,&#34;TOTAL_DEPTH&#34;,&#34;SECTION&#34;,&#34;TWP&#34;,&#34;TDIR&#34;,&#34;RNG&#34;,&#34;RDIR&#34;,&#34;MERIDIAN&#34;,&#34;QUARTERS&#34;,&#34;ELEVATION&#34;,&#34;ELEVREF&#34;,&#34;COUNTY_CODE&#34;,&#34;LATITUDE&#34;,&#34;LONGITUDE&#34;,&#34;ELEVSOURCE&#34;]
   
    raw_dir = pathlib.path(raw_dir)
    raw_dir = raw_dir.as_posix()
    if raw_dir[-1] != &#39;/&#39;:
        raw_dir = raw_dir + &#39;/&#39;

    downholeDataIN = pd.read_csv(raw_dir+str(data_filename), sep=&#39;,&#39;, header=&#39;infer&#39;, encoding=encoding, usecols=data_useCols)
    headerDataIN = pd.read_csv(raw_dir+str(metadata_filename), sep=&#39;,&#39;, header=&#39;infer&#39;, encoding=encoding, usecols=metadata_useCols)

    #Drop data with no API        
    downholeDataIN = downholeDataIN.dropna(subset=[id_col]) #Drop data with no API
    headerDataIN = headerDataIN.dropna(subset=[id_col]) #Drop metadata with no API

    #Drop data with no or missing location information
    headerDataIN = headerDataIN.dropna(subset=[ycol]) 
    headerDataIN = headerDataIN.dropna(subset=[x_col])
    
    #Reset index so index goes from 0 in numerical/integer order
    headerDataIN.reset_index(inplace=True, drop=True)
    downholeDataIN.reset_index(inplace=True, drop=True)
    
    #Print outputs to terminal, if designated
    if verbose:
        print(&#39;Data file has &#39; + str(downholeDataIN.shape[0])+&#39; valid well records.&#39;)
        print(&#34;Metadata file has &#34;+str(headerDataIN.shape[0])+&#34; unique wells with valid location information.&#34;)
    
    return downholeDataIN, headerDataIN</code></pre>
</details>
</dd>
<dt id="w4h.read.read_xyz"><code class="name flex">
<span>def <span class="ident">read_xyz</span></span>(<span>rawdir, xyzfile, dtypes=None, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read file containing xyz data (elevation/location)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>rawdir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>String to the directory in which the xyz file is contained</dd>
<dt><strong><code>xyzfile</code></strong> :&ensp;<code>str</code></dt>
<dd>String with the filename of the xyz file, including extension.</dd>
<dt><strong><code>dtypes</code></strong> :&ensp;<code>dict</code>, default <code>= None</code></dt>
<dd>Dictionary containing the datatypes for the columns int he xyz file. If None, {'ID':np.uint32,'API_NUMBER':np.uint64,'LATITUDE':np.float64,'LONGITUDE':np.float64,'ELEV_FT':np.float64}, by default None</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print the number of xyz records to the terminal, by default False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing the elevation and location data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_xyz(rawdir, xyzfile, dtypes=None, verbose=False):
    &#34;&#34;&#34;Function to read file containing xyz data (elevation/location)

    Parameters
    ----------
    rawdir : str or pathlib.Path object
        String to the directory in which the xyz file is contained
    xyzfile : str
        String with the filename of the xyz file, including extension.
    dtypes : dict, default = None
        Dictionary containing the datatypes for the columns int he xyz file. If None, {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#39;LATITUDE&#39;:np.float64,&#39;LONGITUDE&#39;:np.float64,&#39;ELEV_FT&#39;:np.float64}, by default None
    verbose : bool, default = False
        Whether to print the number of xyz records to the terminal, by default False

    Returns
    -------
    pandas.DataFrame
        Pandas dataframe containing the elevation and location data
    &#34;&#34;&#34;

    if dtypes is None:
        xyzDTypes = {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#39;LATITUDE&#39;:np.float64,&#39;LONGITUDE&#39;:np.float64,&#39;ELEV_FT&#39;:np.float64}

    xyzDataIN = pd.read_csv(rawdir+str(xyzfile), sep=&#39;,&#39;, header=&#39;infer&#39;, dtype=xyzDTypes, index_col=&#39;ID&#39;)
    
    if verbose:
        print(&#39;XYZ file has &#39; + str(xyzDataIN.shape[0])+&#39; records with elevation and location.&#39;)
    return xyzDataIN</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="w4h" href="index.html">w4h</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="w4h.read.define_dtypes" href="#w4h.read.define_dtypes">define_dtypes</a></code></li>
<li><code><a title="w4h.read.file_setup" href="#w4h.read.file_setup">file_setup</a></code></li>
<li><code><a title="w4h.read.get_current_date" href="#w4h.read.get_current_date">get_current_date</a></code></li>
<li><code><a title="w4h.read.get_most_recent" href="#w4h.read.get_most_recent">get_most_recent</a></code></li>
<li><code><a title="w4h.read.get_search_terms" href="#w4h.read.get_search_terms">get_search_terms</a></code></li>
<li><code><a title="w4h.read.read_dict" href="#w4h.read.read_dict">read_dict</a></code></li>
<li><code><a title="w4h.read.read_dictionary_terms" href="#w4h.read.read_dictionary_terms">read_dictionary_terms</a></code></li>
<li><code><a title="w4h.read.read_lithologies" href="#w4h.read.read_lithologies">read_lithologies</a></code></li>
<li><code><a title="w4h.read.read_raw_txt" href="#w4h.read.read_raw_txt">read_raw_txt</a></code></li>
<li><code><a title="w4h.read.read_xyz" href="#w4h.read.read_xyz">read_xyz</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>