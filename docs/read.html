<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>w4h.read API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>w4h.read</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import datetime
import inspect
import json
import os
import pathlib

import pandas as pd
import numpy as np

repoDir = pathlib.Path(os.getcwd())

from w4h import logger_function

# Gets the current date for use with in code
def get_current_date():
    &#34;&#34;&#34; Gets the current date to help with finding the most recent file
        ---------------------
        Parameters:
            None

        ---------------------
        Returns:
            todayDate   : datetime object with today&#39;s date
            dateSuffix  : str to use for naming output files
    &#34;&#34;&#34;
    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)
    dateSuffix = &#39;_&#39;+todayDateStr
    return todayDate, dateSuffix

#Function to get most recent file 
def get_most_recent(dir=str(repoDir)+&#39;/resources&#39;, glob_pattern=&#39;*&#39;, verbose=True):
    &#34;&#34;&#34;Function to find the most recent file with the indicated pattern, using pathlib.glob function.

    Parameters
    ----------
    dir : str or pathlib.Path object, optional
        Directory in which to find the most recent file, by default str(repoDir)+&#39;/resources&#39;
    glob_pattern : str, optional
        String used by the pathlib.glob() function/method for searching, by default &#39;*&#39;

    Returns
    -------
    pathlib.Path object
        Pathlib Path object of the most recent file fitting the glob pattern indicated in the glob_pattern parameter.
    &#34;&#34;&#34;
    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)

    files = pathlib.Path(dir).rglob(glob_pattern) #Get all the files that fit the pattern
    fileDates = []
    for f in files: #Get the file dates from their file modification times
        fileDates.append(np.datetime64(datetime.datetime.fromtimestamp(os.path.getmtime(f))))
    
    if fileDates == []:
        #If no files found that match pattern, return an empty pathlib.Path()
        if verbose:
            print(&#39;No file found in {} matching {} pattern&#39;.format(dir, glob_pattern))
        mostRecentFile = pathlib.Path()
        return mostRecentFile
    else:
        globInd = np.argmin(np.datetime64(todayDateStr) - np.array(fileDates)) #Find the index of the most recent file

    #Iterate through glob/files again (need to recreate glob)
    files = pathlib.Path(dir).rglob(glob_pattern)
    for j, f in enumerate(files):
        if j == globInd:
            mostRecentFile=f
            break
    
    if verbose:
        print(&#39;Most Recent version of this file is : &#39;+mostRecentFile.name)

    return mostRecentFile

#Function to setup files of interest
def file_setup(well_data, metadata=None, data_filename=&#39;*ISGS_DOWNHOLE_DATA*.txt&#39;, metadata_filename=&#39;*ISGS_HEADER*.txt&#39;, log_dir=None, verbose=False, log=False):
    &#34;&#34;&#34;Function to setup files, assuming data, metadata, and elevation/location are in separate files (there should be one &#34;key&#34;/identifying column consistent across all files to join/merge them later)

    This function may not be useful if files are organized differently than this structure. 
    If that is the case, it is recommended to use the get_most_recent() function for each individual file if needed.
    It may also be of use to simply skip this function altogether and directly define each filepath in a manner that can be used by pandas.read_csv()

    Parameters
    ----------
    well_data : str or pathlib.Path object
        Str or pathlib.Path to directory containing input files, by default str(repoDir)+&#39;/resources&#39;
    metadata : str or pathlib.Path object, optional
        Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+&#39;/resources&#39;
    data_filename : str, optional
        Pattern used by pathlib.glob() to get the most recent data file, by default &#39;*ISGS_DOWNHOLE_DATA*.txt&#39;
    metadata_filename : str, optional
        Pattern used by pathlib.glob() to get the most recent metadata file, by default &#39;*ISGS_HEADER*.txt&#39;
    verbose : bool, default = False
        Whether to print name of files to terminal, by default True
    log : bool, default = True
        Whether to log inputs and outputs to log file.

    Returns
    -------
    tuple
        Tuple with (well_data, metadata)
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    #Define  filepath variables to be used later for reading/writing files
    data_path = pathlib.Path(well_data)
    if metadata is None:
        origMetaPath = None
        metadata=data_path
    else:
        origMetaPath = metadata
        metadata=pathlib.Path(metadata)

    #If input path is a directory, find most recent version of the file. If file, just read the file
    if data_path.is_dir():
        downholeDataFILE = get_most_recent(data_path, data_filename, verbose=verbose)
    else:
        downholeDataFILE = data_path
    
    if metadata.is_dir():
        headerDataFILE = get_most_recent(metadata, metadata_filename, verbose=verbose)
        if headerDataFILE == []:
            headerDataFILE = downholeDataFILE
    else:
        if origMetaPath is None:
            headerDataFILE = downholeDataFILE
        else:
            headerDataFILE = metadata
       #Set all input as pathlib.Path objects (may be redundant, but just in case)
    downholeDataPATH = pathlib.Path(downholeDataFILE)
    headerDataPATH = pathlib.Path(headerDataFILE)

    if verbose:
        print(&#39;Using the following files:&#39;)
        print(&#39;\t&#39;, downholeDataFILE)
        print(&#39;\t&#39;, headerDataFILE)

    #Define datatypes, to use later
    #downholeDataDTYPES = {&#39;ID&#39;:np.uint32, &#34;API_NUMBER&#34;:np.uint64,&#34;TABLE_NAME&#34;:str,&#34;WHO&#34;:str,&#34;INTERPRET_DATE&#34;:str,&#34;FORMATION&#34;:str,&#34;THICKNESS&#34;:np.float64,&#34;TOP&#34;:np.float64,&#34;BOTTOM&#34;:np.float64}
    #headerDataDTYPES = {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#34;TDFORMATION&#34;:str,&#34;PRODFORM&#34;:str,&#34;TOTAL_DEPTH&#34;:np.float64,&#34;SECTION&#34;:np.float64,&#34;TWP&#34;:np.float64,&#34;TDIR&#34;:str,&#34;RNG&#34;:np.float64,&#34;RDIR&#34;:str,&#34;MERIDIAN&#34;:np.float64,&#34;FARM_NAME&#34;:str,&#34;NSFOOT&#34;:np.float64,&#34;NSDIR&#34;:str,&#34;EWFOOT&#34;:np.float64,&#34;EWDIR&#34;:str,&#34;QUARTERS&#34;:str,&#34;ELEVATION&#34;:np.float64,&#34;ELEVREF&#34;:str,&#34;COMP_DATE&#34;:str,&#34;STATUS&#34;:str,&#34;FARM_NUM&#34;:str,&#34;COUNTY_CODE&#34;:np.float64,&#34;PERMIT_NUMBER&#34;:str,&#34;COMPANY_NAME&#34;:str,&#34;COMPANY_CODE&#34;:str,&#34;PERMIT_DATE&#34;:str,&#34;CORNER&#34;:str,&#34;LATITUDE&#34;:np.float64,&#34;LONGITUDE&#34;:np.float64,&#34;ENTERED_BY&#34;:str,&#34;UPDDATE&#34;:str,&#34;ELEVSOURCE&#34;:str, &#34;ELEV_FT&#34;:np.float64}
    return downholeDataPATH, headerDataPATH

#Read raw data by text
def read_raw_txt(data_filepath, metadata_filepath, data_cols=None, metadata_cols=None, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, id_col=&#39;API_NUMBER&#39;, encoding=&#39;latin-1&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Easy function to read raw .txt files output from (for example), an Access database

    Parameters
    ----------
    data_filepath : str
        Filename of the file containing data, including the extension.
    metadata_filepath : str
        Filename of the file containing metadata, including the extension.
    data_cols : list, default = None
        List with strings with names of columns from txt file to keep after reading. If None, [&#34;API_NUMBER&#34;,&#34;TABLE_NAME&#34;,&#34;FORMATION&#34;,&#34;THICKNESS&#34;,&#34;TOP&#34;,&#34;BOTTOM&#34;], by default None.
    metadata_cols : list, default = None
        List with strings with names of columns from txt file to keep after reading. If None, [&#39;API_NUMBER&#39;,&#34;TOTAL_DEPTH&#34;,&#34;SECTION&#34;,&#34;TWP&#34;,&#34;TDIR&#34;,&#34;RNG&#34;,&#34;RDIR&#34;,&#34;MERIDIAN&#34;,&#34;QUARTERS&#34;,&#34;ELEVATION&#34;,&#34;ELEVREF&#34;,&#34;COUNTY_CODE&#34;,&#34;LATITUDE&#34;,&#34;LONGITUDE&#34;,&#34;ELEVSOURCE&#34;], by default None
    x_col : str, default = &#39;LONGITUDE&#39;
        Name of column in metadata file indicating the x-location of the well, by default &#39;LONGITUDE&#39;
    ycol : str, default = &#39;LATITUDE&#39;
        Name of the column in metadata file indicating the y-location of the well, by default &#39;LATITUDE&#39;
    id_col : str, default = &#39;API_NUMBER&#39;
        Name of the column with the key/identifier that will be used to merge data later, by default &#39;API_NUMBER&#39;
    encoding : str, default = &#39;latin-1&#39;
        Encoding of the data in the input files, by default &#39;latin-1&#39;
    verbose : bool, default = False
        Whether to print the number of rows in the input columns, by default False
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    (pandas.DataFrame, pandas.DataFrame)
        Tuple/list with two pandas dataframes: (data, metadata)
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if data_cols is None:
        data_useCols = [&#34;API_NUMBER&#34;,&#34;TABLE_NAME&#34;,&#34;FORMATION&#34;,&#34;THICKNESS&#34;,&#34;TOP&#34;,&#34;BOTTOM&#34;]
    else:
        data_useCols= data_cols

    if metadata_cols is None:
        metadata_useCols = [&#39;API_NUMBER&#39;,&#34;TOTAL_DEPTH&#34;,&#34;SECTION&#34;,&#34;TWP&#34;,&#34;TDIR&#34;,&#34;RNG&#34;,&#34;RDIR&#34;,&#34;MERIDIAN&#34;,&#34;QUARTERS&#34;,&#34;ELEVATION&#34;,&#34;ELEVREF&#34;,&#34;COUNTY_CODE&#34;,&#34;LATITUDE&#34;,&#34;LONGITUDE&#34;,&#34;ELEVSOURCE&#34;]
    else:
        metadata_useCols= metadata_cols

    #Check if input data is already 
    if isinstance(data_filepath, pd.DataFrame):
        downholeDataIN = data_filepath[data_useCols]
    else:
        downholeDataIN = pd.read_csv(data_filepath, sep=&#39;,&#39;, header=&#39;infer&#39;, encoding=encoding, usecols=data_useCols)
    
    if isinstance(metadata_filepath, pd.DataFrame):
        headerDataIN = metadata_filepath[metadata_useCols]
    else:
        headerDataIN = pd.read_csv(metadata_filepath, sep=&#39;,&#39;, header=&#39;infer&#39;, encoding=encoding, usecols=metadata_useCols)

    #Drop data with no API        
    downholeDataIN = downholeDataIN.dropna(subset=[id_col]) #Drop data with no API
    headerDataIN = headerDataIN.dropna(subset=[id_col]) #Drop metadata with no API

    #Drop data with no or missing location information
    headerDataIN = headerDataIN.dropna(subset=[ycol]) 
    headerDataIN = headerDataIN.dropna(subset=[xcol])
    
    #Reset index so index goes from 0 in numerical/integer order
    headerDataIN.reset_index(inplace=True, drop=True)
    downholeDataIN.reset_index(inplace=True, drop=True)
    
    #Print outputs to terminal, if designated
    if verbose:
        print(&#39;Data file has &#39; + str(downholeDataIN.shape[0])+&#39; valid well records.&#39;)
        print(&#34;Metadata file has &#34;+str(headerDataIN.shape[0])+&#34; unique wells with valid location information.&#34;)
    
    return downholeDataIN, headerDataIN

#Read file with xyz data
def read_xyz(xyzpath, dtypes=None, verbose=False, log=False):
    &#34;&#34;&#34;Function to read file containing xyz data (elevation/location)

    Parameters
    ----------
    xyzpath : str or pathlib.Path
        Filepath of the xyz file, including extension
    dtypes : dict, default = None
        Dictionary containing the datatypes for the columns int he xyz file. If None, {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#39;LATITUDE&#39;:np.float64,&#39;LONGITUDE&#39;:np.float64,&#39;ELEV_FT&#39;:np.float64}, by default None
    verbose : bool, default = False
        Whether to print the number of xyz records to the terminal, by default False
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    pandas.DataFrame
        Pandas dataframe containing the elevation and location data
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if dtypes is None:
        xyzDTypes = {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#39;LATITUDE&#39;:np.float64,&#39;LONGITUDE&#39;:np.float64,&#39;ELEV_FT&#39;:np.float64}

    xyzDataIN = pd.read_csv(xyzpath, sep=&#39;,&#39;, header=&#39;infer&#39;, dtype=xyzDTypes, index_col=&#39;ID&#39;)
    
    if verbose:
        print(&#39;XYZ file has &#39; + str(xyzDataIN.shape[0])+&#39; records with elevation and location.&#39;)
    return xyzDataIN

#Get filepath of resource in resource folder
def __get_resource_path(res):
    &#34;&#34;&#34;Function to get the path to the resource folder (used in other functions)

    Parameters
    ----------
    res : _type_
        _description_

    Returns
    -------
    _type_
        _description_
    &#34;&#34;&#34;
    repoPath = pathlib.Path(__file__).parent.parent
    repoPathStr = str(repoPath).replace(&#39;\\&#39;, &#39;/&#39;).replace(&#39;\\&#39;[0], &#39;/&#39;)
    resource = repoPathStr+&#39;/resources/&#39;+res
    return resource

#Read dictionary file into dictionary variable
def read_dict(file, keytype=&#39;np&#39;):
    &#34;&#34;&#34;Function to read a text file with a dictionary in it into a python dictionary

    Parameters
    ----------
    file : str or pathlib.Path object
        Filepath to the file of interest containing the dictionary text
    keytype : str, optional
        String indicating the datatypes used in the text, currently only &#39;np&#39; is implemented, by default &#39;np&#39;

    Returns
    -------
    dict
        Dictionary translated from text file.
    &#34;&#34;&#34;

    with open(file, &#39;r&#39;) as f:
        data= f.read()

    jsDict = json.loads(data)
    if keytype==&#39;np&#39;:
        for k in jsDict.keys():
            jsDict[k] = getattr(np, jsDict[k])
    
    return jsDict

#Define the datatypes for a dataframe
def define_dtypes(df, dtypes=None, dtype_file=None, dtype_dir=str(repoDir)+&#39;/resources/&#39;, log=False):
    &#34;&#34;&#34;Function to define datatypes of a dataframe, especially with file-indicated dyptes

    Parameters
    ----------
    df : pandas.DataFrame
        Pandas dataframe with columns whose datatypes need to be (re)defined
    dtypes : dict or None, default = None
        Dictionary containing datatypes, to be used in pandas.DataFrame.astype() function. If None, will read from file indicated by dtype_file (which must be defined, along with dtype_dir), by default None
    dtype_file : str or None, default = None
        Filename of file containing datatypes (txt file in dictionary format). If None, dtypes must be defined, by default None.
    dtype_dir : str or pathlib.Path obejct, default = str(repoDir)+&#39;/resources/&#39;
        Directory containing dtype_file, by default 
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    dfout : pandas.DataFrame
        Pandas dataframe containing redefined columns
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    dfout = df.copy()
    
    if dtypes is None:
        if isinstance(dtype_dir, pathlib.PurePath):
            dtype_dir = dtype_dir.as_posix()
        if dtype_dir[-1] != &#39;/&#39;:
            dtype_dir = dtype_dir + &#39;/&#39;

        if dtype_file is None:
            print(&#39;ERROR: Either dtype_file (and dtype_dir) or dtypes must be defined&#39;)
            return 
        
        dtype_file = pathlib.Path(dtype_dir).joinpath(dtype_file)
        dtypes = read_dict(file=dtype_file)
    
    dfcols = dfout.columns
    for i in range(0, np.shape(dfout)[1]):
        dfout.iloc[:,i] = df.iloc[:,i].astype(dtypes[dfcols[i]])

    return dfout

#Define the search term filepaths
def get_search_terms(spec_path=str(repoDir)+&#39;/resources/&#39;, spec_glob_pattern=&#39;*SearchTerms-Specific*&#39;, 
                     start_path=None, start_glob_pattern = &#39;*SearchTerms-Start*&#39;, 
                     wildcard_path=None, wildcard_glob_pattern=&#39;*SearchTerms-Wildcard&#39;,
                     log=False):
    &#34;&#34;&#34;Read in dictionary files for downhole data

    Parameters
    ----------
    spec_path : str or pathlib.Path, optional
        Directory where the file containing the specific search terms is located, by default str(repoDir)+&#39;/resources/&#39;
    spec_glob_pattern : str, optional
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Specific*&#39;
    start_path : str or None, optional
        Directory where the file containing the start search terms is located, by default None
    start_glob_pattern : str, optional
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Start*&#39;
    wildcard_path : str or pathlib.Path, default = None
        Directory where the file containing the wildcard search terms is located, by default None    
    wildcard_glob_pattern : str, default = &#39;*SearchTerms-Wildcard&#39;
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Wildcard*&#39;
    log : bool, default = True
        Whether to log inputs and outputs to log file.        

    Returns
    -------
    (specTermsPath, startTermsPath, wilcardTermsPath) : tuple
        Tuple containing the pandas dataframes with specific search terms,  with start search terms, and with wildcard search terms
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    
    #specTermsFile = &#34;SearchTerms-Specific_BedrockOrNo_2022-09.csv&#34; #Specific matches
    #startTermsFile = &#34;SearchTerms-Start_BedrockOrNo.csv&#34; #Wildcard matches for the start of the description

    #Exact match path
    if spec_path is None:
        specTermsPath = spec_path        
    else:
        spec_path = pathlib.Path(spec_path)

        if spec_path.is_dir():
            specTermsPath = get_most_recent(spec_path, spec_glob_pattern)
        else:
            specTermsPath = spec_path

    #Startswith path
    if start_path is None:
        startTermsPath = start_path        
    else:
        start_path = pathlib.Path(start_path)

        if start_path.is_dir():
            startTermsPath = get_most_recent(start_path, start_glob_pattern)
        else:
            startTermsPath = start_path

    #Wildcard Path
    if wildcard_path is None:
        wilcardTermsPath = wildcard_path        
    else:
        wildcard_path = pathlib.Path(wildcard_path)

        if wildcard_path.is_dir():
            wilcardTermsPath = get_most_recent(wildcard_path, wildcard_glob_pattern)
        else:
            wilcardTermsPath = wildcard_path
    
    return specTermsPath, startTermsPath, wilcardTermsPath

#Read files into pandas dataframes
def read_dictionary_terms(dict_file, id_col=&#39;ID&#39;, search_col=&#39;DESCRIPTION&#39;, definition_col=&#39;LITHOLOGY&#39;, class_flag_col=&#39;CLASS_FLAG&#39;, dictionary_type=None, class_flag=6, rem_extra_cols=True, log=False):
    &#34;&#34;&#34;Function to read dictionary terms from file into pandas dataframe

    Parameters
    ----------
    dict_file : str or pathlib.Path object, or list of these
        File or list of files to be read
    search_col : str, default = &#39;DESCRIPTION&#39;
        Name of column containing search terms (geologic formations)
    definition_col : str, default = &#39;LITHOLOGY&#39;
        Name of column containing interpretations of search terms (lithologies)
    dictionary_type : str or None, {None, &#39;exact&#39;, &#39;start&#39;, &#39;wildcard&#39;,}
        Indicator of which kind of dictionary terms to be read in: None, &#39;exact&#39;, &#39;start&#39;, or &#39;wildcard&#39; by default None.
            - If None, uses name of file to try to determine. If it cannot, it will default to using the classification flag from class_flag
            - If &#39;exact&#39;, will be used to search for exact matches to geologic descriptions
            - If &#39;start&#39;, will be used as with the .startswith() string method to find inexact matches to geologic descriptions
            - If &#39;wildcard&#39;, will be used to find any matching substring for inexact geologic matches
    class_flag : int, default = 1
        Classification flag to be used if dictionary_type is None and cannot be otherwise determined, by default 1
    rem_extra_cols : bool, default = True
        Whether to remove the extra columns from the input file after it is read in as a pandas dataframe, by default True
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    dict_terms : pandas.DataFrame
        Pandas dataframe with formatting ready to be used in the classification steps of this package
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    #Read files into pandas dataframes
    dict_terms = []
    if dict_file is None:
        df = pd.DataFrame(columns=[&#39;ID&#39;, &#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#39;CLASS_FLAGS&#39;])
        dict_terms.append(df)
        dict_file = [&#39;&#39;]
    elif type(dict_file) is list:
        for f in dict_file:
            if not f.exists():
                df = pd.DataFrame(columns=[&#39;ID&#39;, &#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#39;CLASS_FLAGS&#39;])
                dict_terms.append(df)
            else:
                dict_terms.append(pd.read_csv(f))

            if id_col in dict_terms.columns:
                dict_terms.set_index(id_col, drop=True, inplace=True)
    else:
        dict_file = pathlib.Path(dict_file)
        if dict_file.exists() and dict_file.is_file():
            dict_terms.append(pd.read_csv(dict_file, low_memory=False))
            if id_col in dict_terms[-1].columns:
                dict_terms[-1].set_index(id_col, drop=True, inplace=True)
            dict_file = [dict_file]
        else:
            print(&#39;ERROR: dict_file ({}) does not exist.&#39;.format(dict_file))
            #Create empty dataframe to return
            dict_terms = pd.DataFrame(columns=[&#39;ID&#39;, &#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#34;CLASS_FLAGS&#34;])
            return dict_terms

    #Rename important columns
    searchTermList = [&#39;searchterm&#39;, &#39;search&#39;, &#39;exact&#39;]
    startTermList = [&#39;startterm&#39;, &#39;start&#39;, &#39;startswith&#39;]
    wildcardTermList = [&#39;wildcard&#39;, &#39;substring&#39;, ]

    #Recast all columns to datatypes of headerData to defined types
    dict_termDtypes = {search_col:str, definition_col:str, class_flag_col:np.uint8}

    if dictionary_type is None:
        dictionary_type=&#39;&#39; #Allow string methods on this variable

    #Iterating, to allow reading of multiple dict file at once (also works with just one at at time)
    for i, d in enumerate(dict_terms):
        if dictionary_type.lower() in searchTermList or (dictionary_type==&#39;&#39; and &#39;spec&#39; in str(dict_file[i]).lower()):
            d[class_flag_col] = 1
        elif dictionary_type.lower() in startTermList or (dictionary_type==&#39;&#39; and &#39;start&#39; in str(dict_file[i]).lower()):
            d[class_flag_col] = 4 #Start term classification flag
        elif dictionary_type.lower() in wildcardTermList or (dictionary_type==&#39;&#39; and &#39;wildcard&#39; in str(dict_file[i]).lower()):
            d[class_flag_col] = 5 #Wildcard term classification flag
        else:
            d[class_flag_col] = class_flag #Custom classification flag, defined as argument
        #1: exact classification match, 2: (not defined...ML?), 3: bedrock classification for obvious bedrock, 4: start term, 5: wildcard/substring, 6: Undefined

        #Rename columns so it is consistent through rest of code
        if search_col != &#39;DESCRIPTION&#39;:
            d.rename(columns={search_col:&#39;DESCRIPTION&#39;}, inplace=True)
        if definition_col != &#39;LITHOLOGY&#39;:
            d.rename(columns={definition_col:&#39;LITHOLOGY&#39;}, inplace=True)
        if class_flag_col != &#39;CLASS_FLAG&#39;:
            d.rename(columns={class_flag_col:&#39;CLASS_FLAG&#39;}, inplace=True)

        #Cast all columns as type str, if not already
        for i in range(0, np.shape(d)[1]):
            if d.iloc[:,i].name in list(dict_termDtypes.keys()):
                d.iloc[:,i] = d.iloc[:,i].astype(dict_termDtypes[d.iloc[:,i].name])
        
        #Delete duplicate definitions
        d.drop_duplicates(subset=&#39;DESCRIPTION&#39;,inplace=True) #Apparently, there are some duplicate definitions, which need to be deleted first
        d.reset_index(inplace=True, drop=True)

        #Whether to remove extra columns that aren&#39;t needed from dataframe
        if rem_extra_cols:
            d = d[[&#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#39;CLASS_FLAG&#39;]]

    #If only one file designated, convert it so it&#39;s no longer a list, but just a dataframe

    if len(dict_terms) == 1:
        dict_terms = dict_terms[0]

    return dict_terms

#Function to read lithology file into pandas dataframe
def read_lithologies(lith_file=None, interp_col=&#39;LITHOLOGY&#39;, target_col=&#39;CODE&#39;, use_cols=None, log=False):
    &#34;&#34;&#34;Function to read lithology file into pandas dataframe

    Parameters
    ----------
    lith_file : str or pathlib.Path object, default = None
        Filename of lithology file. If None, default is contained within repository, by default None
    interp_col : str, default = &#39;LITHOLOGY&#39;
        Column to used to match interpretations
    target_col : str, default = &#39;CODE&#39;
        Column to be used as target code
    use_cols : list, default = None
        Which columns to use when reading in dataframe. If None, defaults to [&#39;LITHOLOGY&#39;, &#39;CODE&#39;].
    log : bool, default = True
        Whether to log inputs and outputs to log file.

    Returns
    -------
    pandas.DataFrame
        Pandas dataframe with lithology information
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if lith_file is None:
        #Find resources
        lith_file=&#39;Lithology_Interp_FineCoarse.csv&#39;
    
    if not isinstance(lith_file, pathlib.PurePath):
        lith_file = pathlib.Path(lith_file)

    if use_cols is None:
        use_cols = [&#39;LITHOLOGY&#39;, &#39;CODE&#39;]

    lithoDF = pd.read_csv(lith_file, usecols=use_cols)

    lithoDF.rename(columns={interp_col:&#39;INTERPRETATION&#39;, target_col:&#39;TARGET&#39;}, inplace=True)

    return lithoDF</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="w4h.read.define_dtypes"><code class="name flex">
<span>def <span class="ident">define_dtypes</span></span>(<span>df, dtypes=None, dtype_file=None, dtype_dir='C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology/resources/', log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to define datatypes of a dataframe, especially with file-indicated dyptes</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with columns whose datatypes need to be (re)defined</dd>
<dt><strong><code>dtypes</code></strong> :&ensp;<code>dict</code> or <code>None</code>, default <code>= None</code></dt>
<dd>Dictionary containing datatypes, to be used in pandas.DataFrame.astype() function. If None, will read from file indicated by dtype_file (which must be defined, along with dtype_dir), by default None</dd>
<dt><strong><code>dtype_file</code></strong> :&ensp;<code>str</code> or <code>None</code>, default <code>= None</code></dt>
<dd>Filename of file containing datatypes (txt file in dictionary format). If None, dtypes must be defined, by default None.</dd>
<dt><strong><code>dtype_dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path obejct</code>, default <code>= str(repoDir)+'/resources/'</code></dt>
<dd>Directory containing dtype_file, by default</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dfout</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing redefined columns</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def define_dtypes(df, dtypes=None, dtype_file=None, dtype_dir=str(repoDir)+&#39;/resources/&#39;, log=False):
    &#34;&#34;&#34;Function to define datatypes of a dataframe, especially with file-indicated dyptes

    Parameters
    ----------
    df : pandas.DataFrame
        Pandas dataframe with columns whose datatypes need to be (re)defined
    dtypes : dict or None, default = None
        Dictionary containing datatypes, to be used in pandas.DataFrame.astype() function. If None, will read from file indicated by dtype_file (which must be defined, along with dtype_dir), by default None
    dtype_file : str or None, default = None
        Filename of file containing datatypes (txt file in dictionary format). If None, dtypes must be defined, by default None.
    dtype_dir : str or pathlib.Path obejct, default = str(repoDir)+&#39;/resources/&#39;
        Directory containing dtype_file, by default 
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    dfout : pandas.DataFrame
        Pandas dataframe containing redefined columns
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    dfout = df.copy()
    
    if dtypes is None:
        if isinstance(dtype_dir, pathlib.PurePath):
            dtype_dir = dtype_dir.as_posix()
        if dtype_dir[-1] != &#39;/&#39;:
            dtype_dir = dtype_dir + &#39;/&#39;

        if dtype_file is None:
            print(&#39;ERROR: Either dtype_file (and dtype_dir) or dtypes must be defined&#39;)
            return 
        
        dtype_file = pathlib.Path(dtype_dir).joinpath(dtype_file)
        dtypes = read_dict(file=dtype_file)
    
    dfcols = dfout.columns
    for i in range(0, np.shape(dfout)[1]):
        dfout.iloc[:,i] = df.iloc[:,i].astype(dtypes[dfcols[i]])

    return dfout</code></pre>
</details>
</dd>
<dt id="w4h.read.file_setup"><code class="name flex">
<span>def <span class="ident">file_setup</span></span>(<span>well_data, metadata=None, data_filename='*ISGS_DOWNHOLE_DATA*.txt', metadata_filename='*ISGS_HEADER*.txt', log_dir=None, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to setup files, assuming data, metadata, and elevation/location are in separate files (there should be one "key"/identifying column consistent across all files to join/merge them later)</p>
<p>This function may not be useful if files are organized differently than this structure.
If that is the case, it is recommended to use the get_most_recent() function for each individual file if needed.
It may also be of use to simply skip this function altogether and directly define each filepath in a manner that can be used by pandas.read_csv()</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>well_data</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>Str or pathlib.Path to directory containing input files, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, optional</dt>
<dd>Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>data_filename</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Pattern used by pathlib.glob() to get the most recent data file, by default '<em>ISGS_DOWNHOLE_DATA</em>.txt'</dd>
<dt><strong><code>metadata_filename</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Pattern used by pathlib.glob() to get the most recent metadata file, by default '<em>ISGS_HEADER</em>.txt'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print name of files to terminal, by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Tuple with (well_data, metadata)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def file_setup(well_data, metadata=None, data_filename=&#39;*ISGS_DOWNHOLE_DATA*.txt&#39;, metadata_filename=&#39;*ISGS_HEADER*.txt&#39;, log_dir=None, verbose=False, log=False):
    &#34;&#34;&#34;Function to setup files, assuming data, metadata, and elevation/location are in separate files (there should be one &#34;key&#34;/identifying column consistent across all files to join/merge them later)

    This function may not be useful if files are organized differently than this structure. 
    If that is the case, it is recommended to use the get_most_recent() function for each individual file if needed.
    It may also be of use to simply skip this function altogether and directly define each filepath in a manner that can be used by pandas.read_csv()

    Parameters
    ----------
    well_data : str or pathlib.Path object
        Str or pathlib.Path to directory containing input files, by default str(repoDir)+&#39;/resources&#39;
    metadata : str or pathlib.Path object, optional
        Str or pathlib.Path to directory containing input metadata files, by default str(repoDir)+&#39;/resources&#39;
    data_filename : str, optional
        Pattern used by pathlib.glob() to get the most recent data file, by default &#39;*ISGS_DOWNHOLE_DATA*.txt&#39;
    metadata_filename : str, optional
        Pattern used by pathlib.glob() to get the most recent metadata file, by default &#39;*ISGS_HEADER*.txt&#39;
    verbose : bool, default = False
        Whether to print name of files to terminal, by default True
    log : bool, default = True
        Whether to log inputs and outputs to log file.

    Returns
    -------
    tuple
        Tuple with (well_data, metadata)
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    #Define  filepath variables to be used later for reading/writing files
    data_path = pathlib.Path(well_data)
    if metadata is None:
        origMetaPath = None
        metadata=data_path
    else:
        origMetaPath = metadata
        metadata=pathlib.Path(metadata)

    #If input path is a directory, find most recent version of the file. If file, just read the file
    if data_path.is_dir():
        downholeDataFILE = get_most_recent(data_path, data_filename, verbose=verbose)
    else:
        downholeDataFILE = data_path
    
    if metadata.is_dir():
        headerDataFILE = get_most_recent(metadata, metadata_filename, verbose=verbose)
        if headerDataFILE == []:
            headerDataFILE = downholeDataFILE
    else:
        if origMetaPath is None:
            headerDataFILE = downholeDataFILE
        else:
            headerDataFILE = metadata
       #Set all input as pathlib.Path objects (may be redundant, but just in case)
    downholeDataPATH = pathlib.Path(downholeDataFILE)
    headerDataPATH = pathlib.Path(headerDataFILE)

    if verbose:
        print(&#39;Using the following files:&#39;)
        print(&#39;\t&#39;, downholeDataFILE)
        print(&#39;\t&#39;, headerDataFILE)

    #Define datatypes, to use later
    #downholeDataDTYPES = {&#39;ID&#39;:np.uint32, &#34;API_NUMBER&#34;:np.uint64,&#34;TABLE_NAME&#34;:str,&#34;WHO&#34;:str,&#34;INTERPRET_DATE&#34;:str,&#34;FORMATION&#34;:str,&#34;THICKNESS&#34;:np.float64,&#34;TOP&#34;:np.float64,&#34;BOTTOM&#34;:np.float64}
    #headerDataDTYPES = {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#34;TDFORMATION&#34;:str,&#34;PRODFORM&#34;:str,&#34;TOTAL_DEPTH&#34;:np.float64,&#34;SECTION&#34;:np.float64,&#34;TWP&#34;:np.float64,&#34;TDIR&#34;:str,&#34;RNG&#34;:np.float64,&#34;RDIR&#34;:str,&#34;MERIDIAN&#34;:np.float64,&#34;FARM_NAME&#34;:str,&#34;NSFOOT&#34;:np.float64,&#34;NSDIR&#34;:str,&#34;EWFOOT&#34;:np.float64,&#34;EWDIR&#34;:str,&#34;QUARTERS&#34;:str,&#34;ELEVATION&#34;:np.float64,&#34;ELEVREF&#34;:str,&#34;COMP_DATE&#34;:str,&#34;STATUS&#34;:str,&#34;FARM_NUM&#34;:str,&#34;COUNTY_CODE&#34;:np.float64,&#34;PERMIT_NUMBER&#34;:str,&#34;COMPANY_NAME&#34;:str,&#34;COMPANY_CODE&#34;:str,&#34;PERMIT_DATE&#34;:str,&#34;CORNER&#34;:str,&#34;LATITUDE&#34;:np.float64,&#34;LONGITUDE&#34;:np.float64,&#34;ENTERED_BY&#34;:str,&#34;UPDDATE&#34;:str,&#34;ELEVSOURCE&#34;:str, &#34;ELEV_FT&#34;:np.float64}
    return downholeDataPATH, headerDataPATH</code></pre>
</details>
</dd>
<dt id="w4h.read.get_current_date"><code class="name flex">
<span>def <span class="ident">get_current_date</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="gets-the-current-date-to-help-with-finding-the-most-recent-file">Gets The Current Date To Help With Finding The Most Recent File</h2>
<h2 id="parameters">Parameters</h2>
<p>None</p>
<hr>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>todayDate
</code></dt>
<dd>datetime object with today's date</dd>
<dt><code>dateSuffix
</code></dt>
<dd>str to use for naming output files</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_current_date():
    &#34;&#34;&#34; Gets the current date to help with finding the most recent file
        ---------------------
        Parameters:
            None

        ---------------------
        Returns:
            todayDate   : datetime object with today&#39;s date
            dateSuffix  : str to use for naming output files
    &#34;&#34;&#34;
    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)
    dateSuffix = &#39;_&#39;+todayDateStr
    return todayDate, dateSuffix</code></pre>
</details>
</dd>
<dt id="w4h.read.get_most_recent"><code class="name flex">
<span>def <span class="ident">get_most_recent</span></span>(<span>dir='C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology/resources', glob_pattern='*', verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to find the most recent file with the indicated pattern, using pathlib.glob function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dir</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, optional</dt>
<dd>Directory in which to find the most recent file, by default str(repoDir)+'/resources'</dd>
<dt><strong><code>glob_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>String used by the pathlib.glob() function/method for searching, by default '*'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pathlib.Path object</code></dt>
<dd>Pathlib Path object of the most recent file fitting the glob pattern indicated in the glob_pattern parameter.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_most_recent(dir=str(repoDir)+&#39;/resources&#39;, glob_pattern=&#39;*&#39;, verbose=True):
    &#34;&#34;&#34;Function to find the most recent file with the indicated pattern, using pathlib.glob function.

    Parameters
    ----------
    dir : str or pathlib.Path object, optional
        Directory in which to find the most recent file, by default str(repoDir)+&#39;/resources&#39;
    glob_pattern : str, optional
        String used by the pathlib.glob() function/method for searching, by default &#39;*&#39;

    Returns
    -------
    pathlib.Path object
        Pathlib Path object of the most recent file fitting the glob pattern indicated in the glob_pattern parameter.
    &#34;&#34;&#34;
    todayDate = datetime.date.today()
    todayDateStr = str(todayDate)

    files = pathlib.Path(dir).rglob(glob_pattern) #Get all the files that fit the pattern
    fileDates = []
    for f in files: #Get the file dates from their file modification times
        fileDates.append(np.datetime64(datetime.datetime.fromtimestamp(os.path.getmtime(f))))
    
    if fileDates == []:
        #If no files found that match pattern, return an empty pathlib.Path()
        if verbose:
            print(&#39;No file found in {} matching {} pattern&#39;.format(dir, glob_pattern))
        mostRecentFile = pathlib.Path()
        return mostRecentFile
    else:
        globInd = np.argmin(np.datetime64(todayDateStr) - np.array(fileDates)) #Find the index of the most recent file

    #Iterate through glob/files again (need to recreate glob)
    files = pathlib.Path(dir).rglob(glob_pattern)
    for j, f in enumerate(files):
        if j == globInd:
            mostRecentFile=f
            break
    
    if verbose:
        print(&#39;Most Recent version of this file is : &#39;+mostRecentFile.name)

    return mostRecentFile</code></pre>
</details>
</dd>
<dt id="w4h.read.get_search_terms"><code class="name flex">
<span>def <span class="ident">get_search_terms</span></span>(<span>spec_path='C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology/resources/', spec_glob_pattern='*SearchTerms-Specific*', start_path=None, start_glob_pattern='*SearchTerms-Start*', wildcard_path=None, wildcard_glob_pattern='*SearchTerms-Wildcard', log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Read in dictionary files for downhole data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spec_path</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, optional</dt>
<dd>Directory where the file containing the specific search terms is located, by default str(repoDir)+'/resources/'</dd>
<dt><strong><code>spec_glob_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default '<em>SearchTerms-Specific</em>'</dd>
<dt><strong><code>start_path</code></strong> :&ensp;<code>str</code> or <code>None</code>, optional</dt>
<dd>Directory where the file containing the start search terms is located, by default None</dd>
<dt><strong><code>start_glob_pattern</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default '<em>SearchTerms-Start</em>'</dd>
<dt><strong><code>wildcard_path</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code>, default <code>= None</code></dt>
<dd>Directory where the file containing the wildcard search terms is located, by default None</dd>
<dt><strong><code>wildcard_glob_pattern</code></strong> :&ensp;<code>str</code>, default <code>= '*SearchTerms-Wildcard'</code></dt>
<dd>Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default '<em>SearchTerms-Wildcard</em>'</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(specTermsPath, startTermsPath, wilcardTermsPath) : tuple
Tuple containing the pandas dataframes with specific search terms,
with start search terms, and with wildcard search terms</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_search_terms(spec_path=str(repoDir)+&#39;/resources/&#39;, spec_glob_pattern=&#39;*SearchTerms-Specific*&#39;, 
                     start_path=None, start_glob_pattern = &#39;*SearchTerms-Start*&#39;, 
                     wildcard_path=None, wildcard_glob_pattern=&#39;*SearchTerms-Wildcard&#39;,
                     log=False):
    &#34;&#34;&#34;Read in dictionary files for downhole data

    Parameters
    ----------
    spec_path : str or pathlib.Path, optional
        Directory where the file containing the specific search terms is located, by default str(repoDir)+&#39;/resources/&#39;
    spec_glob_pattern : str, optional
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Specific*&#39;
    start_path : str or None, optional
        Directory where the file containing the start search terms is located, by default None
    start_glob_pattern : str, optional
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Start*&#39;
    wildcard_path : str or pathlib.Path, default = None
        Directory where the file containing the wildcard search terms is located, by default None    
    wildcard_glob_pattern : str, default = &#39;*SearchTerms-Wildcard&#39;
        Search string used by pathlib.glob() to find the most recent file of interest, uses get_most_recent() function, by default &#39;*SearchTerms-Wildcard*&#39;
    log : bool, default = True
        Whether to log inputs and outputs to log file.        

    Returns
    -------
    (specTermsPath, startTermsPath, wilcardTermsPath) : tuple
        Tuple containing the pandas dataframes with specific search terms,  with start search terms, and with wildcard search terms
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)
    
    #specTermsFile = &#34;SearchTerms-Specific_BedrockOrNo_2022-09.csv&#34; #Specific matches
    #startTermsFile = &#34;SearchTerms-Start_BedrockOrNo.csv&#34; #Wildcard matches for the start of the description

    #Exact match path
    if spec_path is None:
        specTermsPath = spec_path        
    else:
        spec_path = pathlib.Path(spec_path)

        if spec_path.is_dir():
            specTermsPath = get_most_recent(spec_path, spec_glob_pattern)
        else:
            specTermsPath = spec_path

    #Startswith path
    if start_path is None:
        startTermsPath = start_path        
    else:
        start_path = pathlib.Path(start_path)

        if start_path.is_dir():
            startTermsPath = get_most_recent(start_path, start_glob_pattern)
        else:
            startTermsPath = start_path

    #Wildcard Path
    if wildcard_path is None:
        wilcardTermsPath = wildcard_path        
    else:
        wildcard_path = pathlib.Path(wildcard_path)

        if wildcard_path.is_dir():
            wilcardTermsPath = get_most_recent(wildcard_path, wildcard_glob_pattern)
        else:
            wilcardTermsPath = wildcard_path
    
    return specTermsPath, startTermsPath, wilcardTermsPath</code></pre>
</details>
</dd>
<dt id="w4h.read.read_dict"><code class="name flex">
<span>def <span class="ident">read_dict</span></span>(<span>file, keytype='np')</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read a text file with a dictionary in it into a python dictionary</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>Filepath to the file of interest containing the dictionary text</dd>
<dt><strong><code>keytype</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>String indicating the datatypes used in the text, currently only 'np' is implemented, by default 'np'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Dictionary translated from text file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_dict(file, keytype=&#39;np&#39;):
    &#34;&#34;&#34;Function to read a text file with a dictionary in it into a python dictionary

    Parameters
    ----------
    file : str or pathlib.Path object
        Filepath to the file of interest containing the dictionary text
    keytype : str, optional
        String indicating the datatypes used in the text, currently only &#39;np&#39; is implemented, by default &#39;np&#39;

    Returns
    -------
    dict
        Dictionary translated from text file.
    &#34;&#34;&#34;

    with open(file, &#39;r&#39;) as f:
        data= f.read()

    jsDict = json.loads(data)
    if keytype==&#39;np&#39;:
        for k in jsDict.keys():
            jsDict[k] = getattr(np, jsDict[k])
    
    return jsDict</code></pre>
</details>
</dd>
<dt id="w4h.read.read_dictionary_terms"><code class="name flex">
<span>def <span class="ident">read_dictionary_terms</span></span>(<span>dict_file, id_col='ID', search_col='DESCRIPTION', definition_col='LITHOLOGY', class_flag_col='CLASS_FLAG', dictionary_type=None, class_flag=6, rem_extra_cols=True, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read dictionary terms from file into pandas dataframe</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dict_file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object,</code> or <code>list</code> of <code>these</code></dt>
<dd>File or list of files to be read</dd>
<dt><strong><code>search_col</code></strong> :&ensp;<code>str</code>, default <code>= 'DESCRIPTION'</code></dt>
<dd>Name of column containing search terms (geologic formations)</dd>
<dt><strong><code>definition_col</code></strong> :&ensp;<code>str</code>, default <code>= 'LITHOLOGY'</code></dt>
<dd>Name of column containing interpretations of search terms (lithologies)</dd>
<dt><strong><code>dictionary_type</code></strong> :&ensp;<code>str</code> or <code>None, {None, 'exact', 'start', 'wildcard',}</code></dt>
<dd>Indicator of which kind of dictionary terms to be read in: None, 'exact', 'start', or 'wildcard' by default None.
- If None, uses name of file to try to determine. If it cannot, it will default to using the classification flag from class_flag
- If 'exact', will be used to search for exact matches to geologic descriptions
- If 'start', will be used as with the .startswith() string method to find inexact matches to geologic descriptions
- If 'wildcard', will be used to find any matching substring for inexact geologic matches</dd>
<dt><strong><code>class_flag</code></strong> :&ensp;<code>int</code>, default <code>= 1</code></dt>
<dd>Classification flag to be used if dictionary_type is None and cannot be otherwise determined, by default 1</dd>
<dt><strong><code>rem_extra_cols</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to remove the extra columns from the input file after it is read in as a pandas dataframe, by default True</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict_terms</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with formatting ready to be used in the classification steps of this package</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_dictionary_terms(dict_file, id_col=&#39;ID&#39;, search_col=&#39;DESCRIPTION&#39;, definition_col=&#39;LITHOLOGY&#39;, class_flag_col=&#39;CLASS_FLAG&#39;, dictionary_type=None, class_flag=6, rem_extra_cols=True, log=False):
    &#34;&#34;&#34;Function to read dictionary terms from file into pandas dataframe

    Parameters
    ----------
    dict_file : str or pathlib.Path object, or list of these
        File or list of files to be read
    search_col : str, default = &#39;DESCRIPTION&#39;
        Name of column containing search terms (geologic formations)
    definition_col : str, default = &#39;LITHOLOGY&#39;
        Name of column containing interpretations of search terms (lithologies)
    dictionary_type : str or None, {None, &#39;exact&#39;, &#39;start&#39;, &#39;wildcard&#39;,}
        Indicator of which kind of dictionary terms to be read in: None, &#39;exact&#39;, &#39;start&#39;, or &#39;wildcard&#39; by default None.
            - If None, uses name of file to try to determine. If it cannot, it will default to using the classification flag from class_flag
            - If &#39;exact&#39;, will be used to search for exact matches to geologic descriptions
            - If &#39;start&#39;, will be used as with the .startswith() string method to find inexact matches to geologic descriptions
            - If &#39;wildcard&#39;, will be used to find any matching substring for inexact geologic matches
    class_flag : int, default = 1
        Classification flag to be used if dictionary_type is None and cannot be otherwise determined, by default 1
    rem_extra_cols : bool, default = True
        Whether to remove the extra columns from the input file after it is read in as a pandas dataframe, by default True
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    dict_terms : pandas.DataFrame
        Pandas dataframe with formatting ready to be used in the classification steps of this package
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    #Read files into pandas dataframes
    dict_terms = []
    if dict_file is None:
        df = pd.DataFrame(columns=[&#39;ID&#39;, &#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#39;CLASS_FLAGS&#39;])
        dict_terms.append(df)
        dict_file = [&#39;&#39;]
    elif type(dict_file) is list:
        for f in dict_file:
            if not f.exists():
                df = pd.DataFrame(columns=[&#39;ID&#39;, &#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#39;CLASS_FLAGS&#39;])
                dict_terms.append(df)
            else:
                dict_terms.append(pd.read_csv(f))

            if id_col in dict_terms.columns:
                dict_terms.set_index(id_col, drop=True, inplace=True)
    else:
        dict_file = pathlib.Path(dict_file)
        if dict_file.exists() and dict_file.is_file():
            dict_terms.append(pd.read_csv(dict_file, low_memory=False))
            if id_col in dict_terms[-1].columns:
                dict_terms[-1].set_index(id_col, drop=True, inplace=True)
            dict_file = [dict_file]
        else:
            print(&#39;ERROR: dict_file ({}) does not exist.&#39;.format(dict_file))
            #Create empty dataframe to return
            dict_terms = pd.DataFrame(columns=[&#39;ID&#39;, &#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#34;CLASS_FLAGS&#34;])
            return dict_terms

    #Rename important columns
    searchTermList = [&#39;searchterm&#39;, &#39;search&#39;, &#39;exact&#39;]
    startTermList = [&#39;startterm&#39;, &#39;start&#39;, &#39;startswith&#39;]
    wildcardTermList = [&#39;wildcard&#39;, &#39;substring&#39;, ]

    #Recast all columns to datatypes of headerData to defined types
    dict_termDtypes = {search_col:str, definition_col:str, class_flag_col:np.uint8}

    if dictionary_type is None:
        dictionary_type=&#39;&#39; #Allow string methods on this variable

    #Iterating, to allow reading of multiple dict file at once (also works with just one at at time)
    for i, d in enumerate(dict_terms):
        if dictionary_type.lower() in searchTermList or (dictionary_type==&#39;&#39; and &#39;spec&#39; in str(dict_file[i]).lower()):
            d[class_flag_col] = 1
        elif dictionary_type.lower() in startTermList or (dictionary_type==&#39;&#39; and &#39;start&#39; in str(dict_file[i]).lower()):
            d[class_flag_col] = 4 #Start term classification flag
        elif dictionary_type.lower() in wildcardTermList or (dictionary_type==&#39;&#39; and &#39;wildcard&#39; in str(dict_file[i]).lower()):
            d[class_flag_col] = 5 #Wildcard term classification flag
        else:
            d[class_flag_col] = class_flag #Custom classification flag, defined as argument
        #1: exact classification match, 2: (not defined...ML?), 3: bedrock classification for obvious bedrock, 4: start term, 5: wildcard/substring, 6: Undefined

        #Rename columns so it is consistent through rest of code
        if search_col != &#39;DESCRIPTION&#39;:
            d.rename(columns={search_col:&#39;DESCRIPTION&#39;}, inplace=True)
        if definition_col != &#39;LITHOLOGY&#39;:
            d.rename(columns={definition_col:&#39;LITHOLOGY&#39;}, inplace=True)
        if class_flag_col != &#39;CLASS_FLAG&#39;:
            d.rename(columns={class_flag_col:&#39;CLASS_FLAG&#39;}, inplace=True)

        #Cast all columns as type str, if not already
        for i in range(0, np.shape(d)[1]):
            if d.iloc[:,i].name in list(dict_termDtypes.keys()):
                d.iloc[:,i] = d.iloc[:,i].astype(dict_termDtypes[d.iloc[:,i].name])
        
        #Delete duplicate definitions
        d.drop_duplicates(subset=&#39;DESCRIPTION&#39;,inplace=True) #Apparently, there are some duplicate definitions, which need to be deleted first
        d.reset_index(inplace=True, drop=True)

        #Whether to remove extra columns that aren&#39;t needed from dataframe
        if rem_extra_cols:
            d = d[[&#39;DESCRIPTION&#39;, &#39;LITHOLOGY&#39;, &#39;CLASS_FLAG&#39;]]

    #If only one file designated, convert it so it&#39;s no longer a list, but just a dataframe

    if len(dict_terms) == 1:
        dict_terms = dict_terms[0]

    return dict_terms</code></pre>
</details>
</dd>
<dt id="w4h.read.read_lithologies"><code class="name flex">
<span>def <span class="ident">read_lithologies</span></span>(<span>lith_file=None, interp_col='LITHOLOGY', target_col='CODE', use_cols=None, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read lithology file into pandas dataframe</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lith_file</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, default <code>= None</code></dt>
<dd>Filename of lithology file. If None, default is contained within repository, by default None</dd>
<dt><strong><code>interp_col</code></strong> :&ensp;<code>str</code>, default <code>= 'LITHOLOGY'</code></dt>
<dd>Column to used to match interpretations</dd>
<dt><strong><code>target_col</code></strong> :&ensp;<code>str</code>, default <code>= 'CODE'</code></dt>
<dd>Column to be used as target code</dd>
<dt><strong><code>use_cols</code></strong> :&ensp;<code>list</code>, default <code>= None</code></dt>
<dd>Which columns to use when reading in dataframe. If None, defaults to ['LITHOLOGY', 'CODE'].</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe with lithology information</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_lithologies(lith_file=None, interp_col=&#39;LITHOLOGY&#39;, target_col=&#39;CODE&#39;, use_cols=None, log=False):
    &#34;&#34;&#34;Function to read lithology file into pandas dataframe

    Parameters
    ----------
    lith_file : str or pathlib.Path object, default = None
        Filename of lithology file. If None, default is contained within repository, by default None
    interp_col : str, default = &#39;LITHOLOGY&#39;
        Column to used to match interpretations
    target_col : str, default = &#39;CODE&#39;
        Column to be used as target code
    use_cols : list, default = None
        Which columns to use when reading in dataframe. If None, defaults to [&#39;LITHOLOGY&#39;, &#39;CODE&#39;].
    log : bool, default = True
        Whether to log inputs and outputs to log file.

    Returns
    -------
    pandas.DataFrame
        Pandas dataframe with lithology information
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if lith_file is None:
        #Find resources
        lith_file=&#39;Lithology_Interp_FineCoarse.csv&#39;
    
    if not isinstance(lith_file, pathlib.PurePath):
        lith_file = pathlib.Path(lith_file)

    if use_cols is None:
        use_cols = [&#39;LITHOLOGY&#39;, &#39;CODE&#39;]

    lithoDF = pd.read_csv(lith_file, usecols=use_cols)

    lithoDF.rename(columns={interp_col:&#39;INTERPRETATION&#39;, target_col:&#39;TARGET&#39;}, inplace=True)

    return lithoDF</code></pre>
</details>
</dd>
<dt id="w4h.read.read_raw_txt"><code class="name flex">
<span>def <span class="ident">read_raw_txt</span></span>(<span>data_filepath, metadata_filepath, data_cols=None, metadata_cols=None, xcol='LONGITUDE', ycol='LATITUDE', id_col='API_NUMBER', encoding='latin-1', verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Easy function to read raw .txt files output from (for example), an Access database</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the file containing data, including the extension.</dd>
<dt><strong><code>metadata_filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the file containing metadata, including the extension.</dd>
<dt><strong><code>data_cols</code></strong> :&ensp;<code>list</code>, default <code>= None</code></dt>
<dd>List with strings with names of columns from txt file to keep after reading. If None, ["API_NUMBER","TABLE_NAME","FORMATION","THICKNESS","TOP","BOTTOM"], by default None.</dd>
<dt><strong><code>metadata_cols</code></strong> :&ensp;<code>list</code>, default <code>= None</code></dt>
<dd>List with strings with names of columns from txt file to keep after reading. If None, ['API_NUMBER',"TOTAL_DEPTH","SECTION","TWP","TDIR","RNG","RDIR","MERIDIAN","QUARTERS","ELEVATION","ELEVREF","COUNTY_CODE","LATITUDE","LONGITUDE","ELEVSOURCE"], by default None</dd>
<dt><strong><code>x_col</code></strong> :&ensp;<code>str</code>, default <code>= 'LONGITUDE'</code></dt>
<dd>Name of column in metadata file indicating the x-location of the well, by default 'LONGITUDE'</dd>
<dt><strong><code>ycol</code></strong> :&ensp;<code>str</code>, default <code>= 'LATITUDE'</code></dt>
<dd>Name of the column in metadata file indicating the y-location of the well, by default 'LATITUDE'</dd>
<dt><strong><code>id_col</code></strong> :&ensp;<code>str</code>, default <code>= 'API_NUMBER'</code></dt>
<dd>Name of the column with the key/identifier that will be used to merge data later, by default 'API_NUMBER'</dd>
<dt><strong><code>encoding</code></strong> :&ensp;<code>str</code>, default <code>= 'latin-1'</code></dt>
<dd>Encoding of the data in the input files, by default 'latin-1'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print the number of rows in the input columns, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(pandas.DataFrame, pandas.DataFrame)
Tuple/list with two pandas dataframes: (data, metadata)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_raw_txt(data_filepath, metadata_filepath, data_cols=None, metadata_cols=None, xcol=&#39;LONGITUDE&#39;, ycol=&#39;LATITUDE&#39;, id_col=&#39;API_NUMBER&#39;, encoding=&#39;latin-1&#39;, verbose=False, log=False):
    &#34;&#34;&#34;Easy function to read raw .txt files output from (for example), an Access database

    Parameters
    ----------
    data_filepath : str
        Filename of the file containing data, including the extension.
    metadata_filepath : str
        Filename of the file containing metadata, including the extension.
    data_cols : list, default = None
        List with strings with names of columns from txt file to keep after reading. If None, [&#34;API_NUMBER&#34;,&#34;TABLE_NAME&#34;,&#34;FORMATION&#34;,&#34;THICKNESS&#34;,&#34;TOP&#34;,&#34;BOTTOM&#34;], by default None.
    metadata_cols : list, default = None
        List with strings with names of columns from txt file to keep after reading. If None, [&#39;API_NUMBER&#39;,&#34;TOTAL_DEPTH&#34;,&#34;SECTION&#34;,&#34;TWP&#34;,&#34;TDIR&#34;,&#34;RNG&#34;,&#34;RDIR&#34;,&#34;MERIDIAN&#34;,&#34;QUARTERS&#34;,&#34;ELEVATION&#34;,&#34;ELEVREF&#34;,&#34;COUNTY_CODE&#34;,&#34;LATITUDE&#34;,&#34;LONGITUDE&#34;,&#34;ELEVSOURCE&#34;], by default None
    x_col : str, default = &#39;LONGITUDE&#39;
        Name of column in metadata file indicating the x-location of the well, by default &#39;LONGITUDE&#39;
    ycol : str, default = &#39;LATITUDE&#39;
        Name of the column in metadata file indicating the y-location of the well, by default &#39;LATITUDE&#39;
    id_col : str, default = &#39;API_NUMBER&#39;
        Name of the column with the key/identifier that will be used to merge data later, by default &#39;API_NUMBER&#39;
    encoding : str, default = &#39;latin-1&#39;
        Encoding of the data in the input files, by default &#39;latin-1&#39;
    verbose : bool, default = False
        Whether to print the number of rows in the input columns, by default False
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    (pandas.DataFrame, pandas.DataFrame)
        Tuple/list with two pandas dataframes: (data, metadata)
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if data_cols is None:
        data_useCols = [&#34;API_NUMBER&#34;,&#34;TABLE_NAME&#34;,&#34;FORMATION&#34;,&#34;THICKNESS&#34;,&#34;TOP&#34;,&#34;BOTTOM&#34;]
    else:
        data_useCols= data_cols

    if metadata_cols is None:
        metadata_useCols = [&#39;API_NUMBER&#39;,&#34;TOTAL_DEPTH&#34;,&#34;SECTION&#34;,&#34;TWP&#34;,&#34;TDIR&#34;,&#34;RNG&#34;,&#34;RDIR&#34;,&#34;MERIDIAN&#34;,&#34;QUARTERS&#34;,&#34;ELEVATION&#34;,&#34;ELEVREF&#34;,&#34;COUNTY_CODE&#34;,&#34;LATITUDE&#34;,&#34;LONGITUDE&#34;,&#34;ELEVSOURCE&#34;]
    else:
        metadata_useCols= metadata_cols

    #Check if input data is already 
    if isinstance(data_filepath, pd.DataFrame):
        downholeDataIN = data_filepath[data_useCols]
    else:
        downholeDataIN = pd.read_csv(data_filepath, sep=&#39;,&#39;, header=&#39;infer&#39;, encoding=encoding, usecols=data_useCols)
    
    if isinstance(metadata_filepath, pd.DataFrame):
        headerDataIN = metadata_filepath[metadata_useCols]
    else:
        headerDataIN = pd.read_csv(metadata_filepath, sep=&#39;,&#39;, header=&#39;infer&#39;, encoding=encoding, usecols=metadata_useCols)

    #Drop data with no API        
    downholeDataIN = downholeDataIN.dropna(subset=[id_col]) #Drop data with no API
    headerDataIN = headerDataIN.dropna(subset=[id_col]) #Drop metadata with no API

    #Drop data with no or missing location information
    headerDataIN = headerDataIN.dropna(subset=[ycol]) 
    headerDataIN = headerDataIN.dropna(subset=[xcol])
    
    #Reset index so index goes from 0 in numerical/integer order
    headerDataIN.reset_index(inplace=True, drop=True)
    downholeDataIN.reset_index(inplace=True, drop=True)
    
    #Print outputs to terminal, if designated
    if verbose:
        print(&#39;Data file has &#39; + str(downholeDataIN.shape[0])+&#39; valid well records.&#39;)
        print(&#34;Metadata file has &#34;+str(headerDataIN.shape[0])+&#34; unique wells with valid location information.&#34;)
    
    return downholeDataIN, headerDataIN</code></pre>
</details>
</dd>
<dt id="w4h.read.read_xyz"><code class="name flex">
<span>def <span class="ident">read_xyz</span></span>(<span>xyzpath, dtypes=None, verbose=False, log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read file containing xyz data (elevation/location)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>xyzpath</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path</code></dt>
<dd>Filepath of the xyz file, including extension</dd>
<dt><strong><code>dtypes</code></strong> :&ensp;<code>dict</code>, default <code>= None</code></dt>
<dd>Dictionary containing the datatypes for the columns int he xyz file. If None, {'ID':np.uint32,'API_NUMBER':np.uint64,'LATITUDE':np.float64,'LONGITUDE':np.float64,'ELEV_FT':np.float64}, by default None</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to print the number of xyz records to the terminal, by default False</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, default <code>= False</code></dt>
<dd>Whether to log inputs and outputs to log file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code></dt>
<dd>Pandas dataframe containing the elevation and location data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_xyz(xyzpath, dtypes=None, verbose=False, log=False):
    &#34;&#34;&#34;Function to read file containing xyz data (elevation/location)

    Parameters
    ----------
    xyzpath : str or pathlib.Path
        Filepath of the xyz file, including extension
    dtypes : dict, default = None
        Dictionary containing the datatypes for the columns int he xyz file. If None, {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#39;LATITUDE&#39;:np.float64,&#39;LONGITUDE&#39;:np.float64,&#39;ELEV_FT&#39;:np.float64}, by default None
    verbose : bool, default = False
        Whether to print the number of xyz records to the terminal, by default False
    log : bool, default = False
        Whether to log inputs and outputs to log file.

    Returns
    -------
    pandas.DataFrame
        Pandas dataframe containing the elevation and location data
    &#34;&#34;&#34;
    logger_function(log, locals(), inspect.currentframe().f_code.co_name)

    if dtypes is None:
        xyzDTypes = {&#39;ID&#39;:np.uint32,&#39;API_NUMBER&#39;:np.uint64,&#39;LATITUDE&#39;:np.float64,&#39;LONGITUDE&#39;:np.float64,&#39;ELEV_FT&#39;:np.float64}

    xyzDataIN = pd.read_csv(xyzpath, sep=&#39;,&#39;, header=&#39;infer&#39;, dtype=xyzDTypes, index_col=&#39;ID&#39;)
    
    if verbose:
        print(&#39;XYZ file has &#39; + str(xyzDataIN.shape[0])+&#39; records with elevation and location.&#39;)
    return xyzDataIN</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="w4h" href="index.html">w4h</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="w4h.read.define_dtypes" href="#w4h.read.define_dtypes">define_dtypes</a></code></li>
<li><code><a title="w4h.read.file_setup" href="#w4h.read.file_setup">file_setup</a></code></li>
<li><code><a title="w4h.read.get_current_date" href="#w4h.read.get_current_date">get_current_date</a></code></li>
<li><code><a title="w4h.read.get_most_recent" href="#w4h.read.get_most_recent">get_most_recent</a></code></li>
<li><code><a title="w4h.read.get_search_terms" href="#w4h.read.get_search_terms">get_search_terms</a></code></li>
<li><code><a title="w4h.read.read_dict" href="#w4h.read.read_dict">read_dict</a></code></li>
<li><code><a title="w4h.read.read_dictionary_terms" href="#w4h.read.read_dictionary_terms">read_dictionary_terms</a></code></li>
<li><code><a title="w4h.read.read_lithologies" href="#w4h.read.read_lithologies">read_lithologies</a></code></li>
<li><code><a title="w4h.read.read_raw_txt" href="#w4h.read.read_raw_txt">read_raw_txt</a></code></li>
<li><code><a title="w4h.read.read_xyz" href="#w4h.read.read_xyz">read_xyz</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>