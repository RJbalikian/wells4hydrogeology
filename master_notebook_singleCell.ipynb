{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{Fill in with information about this notebook}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import numpy as np #Data manipulation\n",
    "import pandas as pd #Point data manipulation and organization\n",
    "import xarray as xr #Raster data manipulation and organization\n",
    "\n",
    "import pathlib  #For filepaths, io, etc.\n",
    "import os       #For several system-based commands\n",
    "import datetime #For manipulation of time data, including file creation/modification times\n",
    "import json     #For dictionary io, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt #For plotting and data vizualization\n",
    "import geopandas as gpd         #For organization and manipulation of vector data in space (study area and some data points)\n",
    "import rioxarray as rxr         #For orgnaization and manipulation of raster data\n",
    "from scipy import interpolate\n",
    "import shapely                  #For converting coordinates to point geometry\n",
    "#Not sure if this cell is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scripts with functions made for this specific application\n",
    "import w4h\n",
    "import pathlib\n",
    "import os\n",
    "#Variables needed throughout, best to just assign now\n",
    "todayDate, dateSuffix = w4h.get_current_date() \n",
    "repoDir = pathlib.Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging data to C:\\Users\\riley\\Desktop\\log_2023-06-03_13-22-04.txt\n",
      "Most Recent version of this file is : ISGS_DOWNHOLE_DATA_2023-06-03.txt\n",
      "Most Recent version of this file is : ISGS_HEADER_2023-06-03.txt\n",
      "Most Recent version of this file is : xyzData.csv\n",
      "Using the following files:\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\ISGS_DOWNHOLE_DATA_2023-06-03.txt\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\ISGS_HEADER_2023-06-03.txt\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\xyzData.csv\n"
     ]
    }
   ],
   "source": [
    "directoryDir = r'\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\\\'[:-1]\n",
    "downholeDataPATH, headerDataPATH  = w4h.file_setup(db_dir=directoryDir, log_dir=r'C:\\Users\\riley\\Desktop', verbose=True, log=True)\n",
    "downholeDataIN, headerDataIN = w4h.read_raw_txt(data_filepath=downholeDataPATH, metadata_filepath=headerDataPATH, log=True) #Functions to read data into dataframes. Also excludes extraneous columns, and drops header data with no location information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Recent version of this file is : ISGS_DOWNHOLE_DATA_2023-06-03.txt\n",
      "Most Recent version of this file is : ISGS_HEADER_2023-06-03.txt\n",
      "Using the following files:\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\WellData\\RawWellData_OracleDatabase\\TxtData\\ISGS_DOWNHOLE_DATA_2023-06-03.txt\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\WellData\\RawWellData_OracleDatabase\\TxtData\\ISGS_HEADER_2023-06-03.txt\n",
      "Data file has 3079484 valid well records.\n",
      "Metadata file has 637801 unique wells with valid location information.\n",
      "3022963 records removed without location information.\n",
      "56521 wells remain from 7213 geolocated wells in study area.\n",
      "Well records removed: 793\n",
      "Number of rows before dropping those without surface elevation information: 8182\n",
      "Number of rows after dropping those without surface elevation information: 7389\n",
      "Number of rows before dropping those without record depth information: 56521\n",
      "Number of rows after dropping those without record depth information: 55937\n",
      "Number of well records without formation information deleted: 584\n",
      "Number of rows before dropping those with obviously bad depth information: 56521\n",
      "Number of rows after dropping those with obviously bad depth information: 55915\n",
      "Well records deleted: 606\n",
      "Number of rows before dropping those without FORMATION information: 56521\n",
      "Number of rows after dropping those without FORMATION information: 56521\n",
      "Well records deleted: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology\\w4h\\clean.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[zcol].replace(no_data_val, np.nan, inplace=True)\n",
      "c:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology\\w4h\\clean.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(subset=[zcol], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      FORMATION          INTERPRETATION   \n",
      "ID                                                                        \n",
      "0                yellow clay with a little gray                    CLAY  \\\n",
      "1                                 tan clay hard                    CLAY   \n",
      "2                                   drift: till                    CLAY   \n",
      "3       clay with boulders & little sand/gravel                    CLAY   \n",
      "4         clay, sandy w/gravel & gravel streaks  CLAY WITH GRAVEL SEAMS   \n",
      "...                                         ...                     ...   \n",
      "183452                      yellow sandstone at                 BEDROCK   \n",
      "183453                    yellow sandstone/clay          CLAY AND STONE   \n",
      "183454                         yellow St. Peter                 BEDROCK   \n",
      "183455                   yellow/brown sandstone                 BEDROCK   \n",
      "183456                                Yorkville                   DRIFT   \n",
      "\n",
      "         COLOR CONSISTENCY       MOD1      MOD2             INTERPRETED   \n",
      "ID                                                                        \n",
      "0       YELLOW         NaN        NaN       NaN                    CLAY  \\\n",
      "1        BROWN        HARD        NaN       NaN                    CLAY   \n",
      "2          NaN         NaN        NaN       NaN                    CLAY   \n",
      "3          NaN         NaN   BOULDERY     SANDY                    CLAY   \n",
      "4          NaN         NaN      SANDY  GRAVELLY  CLAY WITH GRAVEL SEAMS   \n",
      "...        ...         ...        ...       ...                     ...   \n",
      "183452  YELLOW         NaN  SANDSTONE       NaN                 BEDROCK   \n",
      "183453  YELLOW         NaN        NaN       NaN       BEDROCK AND OTHER   \n",
      "183454  YELLOW         NaN  SANDSTONE       NaN                 BEDROCK   \n",
      "183455  YELLOW         NaN  SANDSTONE       NaN                 BEDROCK   \n",
      "183456     NaN         NaN        NaN       NaN                 GENERIC   \n",
      "\n",
      "        COMPLETED  ORIGIN_INDIANA  SELECTED  SELECTED_TEMP  SELECTED_UNDO   \n",
      "ID                                                                          \n",
      "0           False           False     False          False          False  \\\n",
      "1           False           False     False          False          False   \n",
      "2           False           False     False          False          False   \n",
      "3           False           False     False          False          False   \n",
      "4           False           False     False          False          False   \n",
      "...           ...             ...       ...            ...            ...   \n",
      "183452       True           False     False          False          False   \n",
      "183453       True           False     False          False          False   \n",
      "183454       True           False     False          False          False   \n",
      "183455       True           False     False          False          False   \n",
      "183456       True           False     False          False          False   \n",
      "\n",
      "        CLASS_FLAG  \n",
      "ID                  \n",
      "0                6  \n",
      "1                6  \n",
      "2                6  \n",
      "3                6  \n",
      "4                6  \n",
      "...            ...  \n",
      "183452           6  \n",
      "183453           6  \n",
      "183454           6  \n",
      "183455           6  \n",
      "183456           6  \n",
      "\n",
      "[183457 rows x 13 columns]\n",
      "                   StartTerm InterpUpdate  CLASS_FLAG\n",
      "ID                                                   \n",
      "1                 s,med,brn;         SAND           4\n",
      "2    s,med,calc,lgt gry brn,         SAND           4\n",
      "3                 s,crs,brn,         SAND           4\n",
      "4               cly st,calc,         SILT           4\n",
      "5            sty cl,lea,gry,         CLAY           4\n",
      "..                       ...          ...         ...\n",
      "752           brown sand F-M         SAND           4\n",
      "753                   sh,blk      BEDROCK           4\n",
      "754                  sh,sty,      BEDROCK           4\n",
      "756                    till,         CLAY           4\n",
      "757                      sh,      BEDROCK           4\n",
      "\n",
      "[750 rows x 3 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Index(['FORMATION'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9200\\1678633110.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtargetPath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology\\resources\\Lithology_Interp_FineCoarse.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprocDir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\\runTest\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m w4h.run(well_data=dataDir,\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mwell_data_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mwell_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mwell_metadata_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology\\w4h\\utilities.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(well_data, well_data_cols, well_metadata, well_metadata_cols, description_col, top_col, bottom_col, depth_type, study_area, xcol, ycol, zcol, idcol, output_crs, surf_elev_file, bedrock_elev_file, model_grid, lith_dict, lith_dict_start, lith_dict_wildcard, target_dict, target_name, export_dir, verbose, log, **keyword_parameters)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[0mget_search_terms_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keyword_parameters'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw4h\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_search_terms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_varnames\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[0mspecTermsPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstartTermsPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwildcardTermsPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw4h\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_search_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlith_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlith_dict_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwildcard_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlith_dict_wildcard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mget_search_terms_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[0mread_dictionary_terms_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keyword_parameters'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw4h\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_dictionary_terms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_varnames\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[0mspecTerms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw4h\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_dictionary_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspecTermsPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mread_dictionary_terms_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m     \u001b[0mstartTerms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw4h\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_dictionary_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartTermsPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mread_dictionary_terms_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m     \u001b[0mwildcardTerms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw4h\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_dictionary_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwildcardTermsPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mread_dictionary_terms_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;31m#Clean up dictionary terms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology\\w4h\\read.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(dict_file, id_col, search_col, definition_col, class_flag_col, dictionary_type, class_flag, rem_extra_cols, log)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[1;31m#Delete duplicate definitions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m         \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'FORMATION'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Apparently, there are some duplicate definitions, which need to be deleted first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m         \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[1;31m#Whether to remove extra columns that aren't needed from dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\riley\\LocalData\\virtual_envs\\w4h_venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6529\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inplace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6530\u001b[0m         \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ignore_index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6532\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6533\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6534\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\riley\\LocalData\\virtual_envs\\w4h_venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, subset, keep)\u001b[0m\n\u001b[0;32m   6660\u001b[0m         \u001b[1;31m# Otherwise, raise a KeyError, same as if you try to __getitem__ with a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6661\u001b[0m         \u001b[1;31m# key that doesn't exist.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6662\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6663\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6664\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6666\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6667\u001b[0m             \u001b[1;31m# GH#45236 This is faster than get_group_index below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: Index(['FORMATION'], dtype='object')"
     ]
    }
   ],
   "source": [
    "import w4h\n",
    "\n",
    "dataDir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\WellData\\RawWellData_OracleDatabase\\TxtData\"\n",
    "\n",
    "modelGridPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\grid_625_raster.tif\"\n",
    "surfaceElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ILStateLidar_ClipExtentESL.tif\"\n",
    "bedrockElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESLBedrock.tif\"\n",
    "\n",
    "studyAreaPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESL_StudyArea_5mi.shp\"\n",
    "\n",
    "specDictPath = r\"C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology\\resources\\DICTIONARY_SearchTerms-Specific_2023-06-27.csv\"\n",
    "startDictPath = r\"C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology\\resources\\SearchTerms-Start.csv\"\n",
    "\n",
    "targetPath = r\"C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology\\resources\\Lithology_Interp_FineCoarse.csv\"\n",
    "\n",
    "procDir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\\runTest\"\n",
    "\n",
    "w4h.run(well_data=dataDir,\n",
    "        well_data_cols=None, \n",
    "        well_metadata=None, \n",
    "        well_metadata_cols=None, \n",
    "        description_col='FORMATION', top_col='TOP', bottom_col='BOTTOM', depth_type='depth',\n",
    "        study_area=studyAreaPath, xcol='LONGITUDE', ycol='LATITUDE', zcol='ELEVATION', idcol='API_NUMBER', output_crs='EPSG:4269',\n",
    "        surf_elev_file=surfaceElevPath, bedrock_elev_file=bedrockElevPath, model_grid=modelGridPath,\n",
    "        lith_dict=specDictPath, lith_dict_start=startDictPath, lith_dict_wildcard=None,\n",
    "        target_dict=targetPath,\n",
    "        export_dir=procDir,\n",
    "        verbose=True,\n",
    "        log=False,\n",
    "        search_col='DESCRIPTION',\n",
    "        definition_col = 'LITHOLOGY'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "test = pathlib.Path()\n",
    "test.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\.conda\\envs\\raster38\\lib\\site-packages\\geopandas\\array.py:275: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  return GeometryArray(vectorized.points_from_xy(x, y, z), crs=crs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well records removed: 0\n",
      "Number of rows before dropping those without surface elevation information: 8150\n",
      "Number of rows after dropping those without surface elevation information: 8150\n",
      "Number of rows before dropping those without record depth information: 56331\n",
      "Number of rows after dropping those without record depth information: 55747\n",
      "Number of well records without formation information deleted: 584\n",
      "Number of rows before dropping those with obviously bad depth information: 56331\n",
      "Number of rows after dropping those with obviously bad depth information: 55725\n",
      "Well records deleted: 606\n",
      "Number of rows before dropping those without FORMATION information: 56331\n",
      "Number of rows after dropping those without FORMATION information: 56331\n",
      "Well records deleted: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m downholeData \u001b[39m=\u001b[39m w4h\u001b[39m.\u001b[39mdrop_no_formation(downholeData, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[39m#downholeData.reset_index(inplace=True,drop=True) #These may not be necessary\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m#headerData.reset_index(inplace=True,drop=True) #These may not be necessary\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m downholeData \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(left \u001b[39m=\u001b[39m downholeData, right \u001b[39m=\u001b[39m headerData, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAPI_NUMBER\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "xyzDataIN = w4h.read_xyz(xyzpath=xyzInPATH, log=True)\n",
    "downholeData = w4h.define_dtypes(df=downholeDataIN, dtype_file='downholeDataTypes.txt', log=True) #Define datatypes of each column of the new dataframes\n",
    "headerData = w4h.define_dtypes(df=headerDataIN, dtype_file='headerDataTypes.txt', log=True)#Define datatypes of each column of the new dataframes\n",
    "xyzData = w4h.define_dtypes(df=xyzDataIN, dtype_file='xyzDataTypes.txt', log=True)\n",
    "studyAreaPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESL_StudyArea_5mi.shp\"\n",
    "studyAreaIN = w4h.read_study_area(studyareapath=studyAreaPath, log=True)\n",
    "modelGridPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\grid_625_raster.tif\"\n",
    "surfaceElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ILStateLidar_ClipExtentESL.tif\"\n",
    "bedrockElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESLBedrock.tif\"\n",
    "modelGrid = w4h.read_grid(datapath=modelGridPath, grid_type='model', study_area=studyAreaIN,  read_grid=True, clip_to_studyarea=True, log=True)#, gridcrs='EPSG:26715', studyAreacrs='EPSG:26715')\n",
    "surfaceElevGridIN = w4h.read_grid(datapath=surfaceElevPath, grid_type='surface', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "bedrockElevGridIN = w4h.read_grid(datapath=bedrockElevPath, grid_type='bedrock', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "#Code here for adding in control points\n",
    "headerData = w4h.xyz_metadata_merge(xyz=xyzData, metadata=headerData, log=True) #This probably needs to be updated\n",
    "headerData = w4h.coords2geometry(df=headerData, xcol='LONGITUDE', ycol='LATITUDE', crs='EPSG:4269', log=True)\n",
    "headerData = w4h.clip_gdf2study_area(study_area=studyAreaIN, gdf=headerData, gdf_crs='EPSG:4269', log=True)\n",
    "downholeData = w4h.remove_nonlocated(downholeData, headerData, log=True)\n",
    "headerData = w4h.remove_no_topo(df=headerData, verbose=True, log=True)\n",
    "donwholeData = w4h.drop_no_depth(downholeData, verbose=True, log=True) #Drop records with no depth information\n",
    "donwholeData = w4h.drop_bad_depth(downholeData, verbose=True, log=True)#Drop records with bad depth information (i.e., top depth > bottom depth) (Also calculates thickness of each record)\n",
    "downholeData = w4h.drop_no_formation(downholeData, verbose=True, log=True)\n",
    "#downholeData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "#headerData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "downholeData = pd.merge(left = downholeData, right = headerData, on='API_NUMBER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Recent version of this file is : SearchTerms-Specific_2022-11-16_essCols.csv\n",
      "Most Recent version of this file is : SearchTerms-Start.csv\n",
      "Records Classified with full search term: 31582\n",
      "Records Classified with full search term: 56.07% of data\n",
      "Start Term process should be done by 13:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\LocalData\\Github\\wells4hydrogeology\\w4h\\classify.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['CLASS_FLAG'].where(~df[description_col].str.startswith(s,na=False),4,inplace=True)\n",
      "c:\\Users\\riley\\LocalData\\Github\\wells4hydrogeology\\w4h\\classify.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['INTERPRETATION'].where(~df[description_col].str.startswith(s,na=False),terms_df.loc[i,'INTERPRETATION'],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records classified with start search term: 3586\n",
      "Records classified with start search term: 14.49% of remaining data\n",
      "Records classified as bedrock that were deeper than 550': 359\n",
      "This represents 1.7% of the unclassified data in this dataframe.\n",
      "BEDROCK_ELEV_FT sampling should be done by 13:51\n",
      "SURFACE_ELEV_FT sampling should be done by 13:51\n",
      "BEDROCK_DEPTH_FT sampling should be done by 13:51\n",
      "LAYER_THICK_FT sampling should be done by 13:51\n",
      "REMOVING LATITUDE\n",
      "REMOVING LONGITUDE\n",
      "REMOVING ELEV_FT\n",
      "REMOVING geometry\n",
      "Completed interpolation for Layer 1\n",
      "Completed interpolation for Layer 2\n",
      "Completed interpolation for Layer 3\n",
      "Completed interpolation for Layer 4\n",
      "Completed interpolation for Layer 5\n",
      "Completed interpolation for Layer 6\n",
      "Completed interpolation for Layer 7\n",
      "Completed interpolation for Layer 8\n",
      "Completed interpolation for Layer 9\n"
     ]
    }
   ],
   "source": [
    "specTermsPATH, startTermsPATH = w4h.get_search_terms(spec_dir=str(repoDir)+'/resources/', spec_glob_pattern='*SearchTerms-Specific*', start_glob_pattern='*SearchTerms-Start*', log=True)\n",
    "specTerms = w4h.read_dictionary_terms(dict_file=specTermsPATH, log=True)\n",
    "startTerms = w4h.read_dictionary_terms(dict_file=startTermsPATH, log=True)\n",
    "oldDictPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\WellData\\Dictionaries\\DICTIONARY_Updated-06-2018.csv\"\n",
    "oldDict = w4h.read_dictionary_terms(dict_file=oldDictPath, cols={'DESCRIPTION':'FORMATION', 'LITHOLOGY':'INTERPRETATION'}, class_flag=1, log=True)\n",
    "\n",
    "specTerms = pd.concat([specTerms, oldDict])\n",
    "specTerms.drop_duplicates(subset='FORMATION', inplace=True)\n",
    "specTerms.reset_index(inplace=True, drop=True)\n",
    "downholeData = w4h.specific_define(downholeData, specTerms, verbose=True, log=True)\n",
    "classifedDF, searchDF = w4h.split_defined(downholeData)\n",
    "searchDF = w4h.start_define(df=searchDF, terms_df=startTerms, verbose=True)\n",
    "downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF)\n",
    "classifedDF, searchDF = w4h.split_defined(downholeData)\n",
    "searchDF = w4h.depth_define(searchDF, thresh=550, verbose=True, log=True)\n",
    "downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF)\n",
    "downholeData = w4h.fill_unclassified(downholeData)\n",
    "#dictDir = \"\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\ISWS_HydroGeo\\\\WellDataAutoClassification\\\\SupportingDocs\\\\\"\n",
    "targetInterpDF = w4h.read_lithologies(log=True)\n",
    "downholeData = w4h.merge_lithologies(df=downholeData, targinterps_df=targetInterpDF)\n",
    "wellsDF = w4h.get_unique_wells(downholeData)\n",
    "downholeData = w4h.sort_dataframe(df=downholeData, sort_cols=['API_NUMBER','TOP'], remove_nans=True)\n",
    "inGrids = [bedrockElevGridIN, surfaceElevGridIN]\n",
    "bedrockGrid, surfaceGrid = w4h.align_rasters(grids_unaligned=inGrids, modelgrid=modelGrid, log=True)\n",
    "driftThickGrid, layerThickGrid = w4h.get_drift_thick(surface=surfaceGrid, bedrock=bedrockGrid, layers=9, plot=False, log=True)\n",
    "headerData = w4h.sample_raster_points(raster=bedrockGrid, points_df=headerData, new_col='BEDROCK_ELEV_FT', log=True)\n",
    "headerData = w4h.sample_raster_points(raster=surfaceGrid, points_df=headerData, new_col='SURFACE_ELEV_FT', log=True)\n",
    "headerData = w4h.sample_raster_points(raster=driftThickGrid, points_df=headerData, new_col='BEDROCK_DEPTH_FT', log=True)\n",
    "headerData = w4h.sample_raster_points(raster=layerThickGrid, points_df=headerData, new_col='LAYER_THICK_FT', log=True)\n",
    "headerData = w4h.get_layer_depths(well_metadata=headerData, no_layers=9, log=True)\n",
    "downholeData_layerInfo = w4h.merge_tables(data_df=downholeData,  data_cols=None, header_cols=None, header_df=headerData,on='API_NUMBER', how='inner', auto_pick_cols=True, log=True)\n",
    "#downholeData = downholeData_layerInfo.copy()\n",
    "resdf = w4h.layer_target_thick(downholeData_layerInfo, layers=9, outfile_prefix='CoarseFine', log=True)\n",
    "layers_data = w4h.layer_interp(points=resdf, layers=9, grid=modelGrid, method='lin', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING LATITUDE\n",
      "REMOVING LONGITUDE\n"
     ]
    }
   ],
   "source": [
    "header_cols = ['API_NUMBER', 'LATITUDE', 'LONGITUDE', 'BEDROCK_ELEV_FT', 'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT']\n",
    "data_cols = downholeData.columns\n",
    "on='API_NUMBER'\n",
    "#Drop duplicate columns\n",
    "drop_duplicate_cols=True\n",
    "if drop_duplicate_cols:\n",
    "    header_colCopy= header_cols.copy()\n",
    "    remCount = 0\n",
    "    for i, c in enumerate(header_colCopy):\n",
    "        if c in data_cols and c != on:\n",
    "            print('REMOVING', header_cols[i-remCount])\n",
    "            header_cols.pop(i - remCount)\n",
    "            remCount += 1\n",
    "\n",
    "downholeData_layerInfo = pd.merge(left=downholeData, right=headerData,on='API_NUMBER', how='inner',)\n",
    "#downholeData_layerInfo = w4h.merge_tables(data_df=downholeData,  data_cols=None, header_cols=None, header_df=headerData,on='API_NUMBER', how='inner', auto_pick_cols=True, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['API_NUMBER', 'TABLE_NAME', 'FORMATION', 'THICKNESS', 'TOP', 'BOTTOM',\n",
       "       'TOTAL_DEPTH_x', 'SECTION_x', 'TWP_x', 'TDIR_x', 'RNG_x', 'RDIR_x',\n",
       "       'MERIDIAN_x', 'QUARTERS_x', 'ELEVATION_x', 'ELEVREF_x', 'COUNTY_CODE_x',\n",
       "       'ELEVSOURCE_x', 'LATITUDE_x', 'LONGITUDE_x', 'ELEV_FT_x', 'geometry_x',\n",
       "       'INTERPRETATION', 'CLASS_FLAG', 'BEDROCK_FLAG', 'TARGET',\n",
       "       'TOTAL_DEPTH_y', 'SECTION_y', 'TWP_y', 'TDIR_y', 'RNG_y', 'RDIR_y',\n",
       "       'MERIDIAN_y', 'QUARTERS_y', 'ELEVATION_y', 'ELEVREF_y', 'COUNTY_CODE_y',\n",
       "       'ELEVSOURCE_y', 'LATITUDE', 'LONGITUDE', 'ELEV_FT_y', 'geometry',\n",
       "       'LONGITUDE_PROJ', 'LATITUDE_PROJ', 'BEDROCK_ELEV_FT', 'SURFACE_ELEV_FT',\n",
       "       'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT', 'DEPTH_FT_LAYER1',\n",
       "       'DEPTH_FT_LAYER2', 'DEPTH_FT_LAYER3', 'DEPTH_FT_LAYER4',\n",
       "       'DEPTH_FT_LAYER5', 'DEPTH_FT_LAYER6', 'DEPTH_FT_LAYER7',\n",
       "       'DEPTH_FT_LAYER8', 'DEPTH_FT_LAYER9', 'ELEV_FT_LAYER1',\n",
       "       'ELEV_FT_LAYER2', 'ELEV_FT_LAYER3', 'ELEV_FT_LAYER4', 'ELEV_FT_LAYER5',\n",
       "       'ELEV_FT_LAYER6', 'ELEV_FT_LAYER7', 'ELEV_FT_LAYER8', 'ELEV_FT_LAYER9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downholeData_layerInfo=downholeData_layerInfo.rename(columns={'LATITUDE_y':\"LATITUDE\",'LONGITUDE_y':\"LONGITUDE\", 'geometry_y':'geometry'})\n",
    "downholeData_layerInfo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf = w4h.layer_target_thick(downholeData_layerInfo, layers=9, outfile_prefix='CoarseFine', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed interpolation for Layer 1\n",
      "Completed interpolation for Layer 2\n",
      "Completed interpolation for Layer 3\n",
      "Completed interpolation for Layer 4\n",
      "Completed interpolation for Layer 5\n",
      "Completed interpolation for Layer 6\n",
      "Completed interpolation for Layer 7\n",
      "Completed interpolation for Layer 8\n",
      "Completed interpolation for Layer 9\n"
     ]
    }
   ],
   "source": [
    "layers_data = w4h.layer_interp(points=resdf, layers=9, grid=modelGrid, method='nn', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir=r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "w4h.export_grids(grid_data=layers_data, out_path=out_dir, file_id='',filetype='tif', variable_sep=True, date_stamp=True, log=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export (rio)xarray dataarrays and datasets\n",
    "def export_grids(grid_data, out_path, file_id='',filetype='tif', variable_sep=True, date_stamp=True):\n",
    "    \"\"\"Function to export grids to files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grid_data : xarray DataArray or xarray Dataset\n",
    "        Dataset or dataarray to be exported\n",
    "    out_path : str or pathlib.Path object\n",
    "        Output location for data export. If variable_sep=True, this should be a directory. Otherwise, this should also include the filename. The file extension should not be included here.\n",
    "    filetype : str, optional\n",
    "        Output filetype. Can either be pickle or any file extension supported by rioxarray.rio.to_raster(). Can either include period or not., by default 'tif'\n",
    "    variable_sep : bool, optional\n",
    "        If grid_data is an xarray Dataset, this will export each variable in the dataset as a separate file, including the variable name in the filename, by default False\n",
    "    date_stamp : bool, optional\n",
    "        Whether to include a date stamp in the file name., by default True\n",
    "    \"\"\"\n",
    "\n",
    "    #Initialize lists to determine which filetype will be used for export\n",
    "    ncdfList = ['netcdf', 'ncdf', 'n']\n",
    "    tifList = ['tif', 'tiff', 'geotiff', 'geotif', 't']\n",
    "    pickleList = ['pickle', 'pkl', 'p']\n",
    "\n",
    "    #Format output string(s)\n",
    "    #Format output filepath\n",
    "    if type(out_path) is str or isinstance(out_path, pathlib.PurePath):\n",
    "        if isinstance(out_path, pathlib.PurePath):\n",
    "            pass\n",
    "        else:\n",
    "            out_path = pathlib.Path(out_path)\n",
    "        if out_path.parent.exists()==False:\n",
    "            print('Directory does not exist. Please enter a different value for the out_path parameter.')\n",
    "            return        \n",
    "\n",
    "        if out_path.is_dir():\n",
    "            if isinstance(grid_data, xr.DataArray):\n",
    "                if variable_sep:\n",
    "                    lyrs = grid_data.coords['Layer'].values\n",
    "                    filenames = []\n",
    "                    for l in lyrs:\n",
    "                        filenames.append('Layer'+str(l))\n",
    "                else:\n",
    "                    filenames = ['AllLayers']\n",
    "            if isinstance(grid_data, xr.Dataset):\n",
    "                if variable_sep:\n",
    "                    filenames = []\n",
    "                    for var in grid_data:\n",
    "                        filenames.append(var)\n",
    "                else:\n",
    "                    filenames = ['AllLayers']    \n",
    "        else:\n",
    "            filenames = [out_path.stem]\n",
    "            out_path = out_path.parent\n",
    "\n",
    "    else:\n",
    "        print('Please input string or pathlib object for out_path parameters')\n",
    "        return\n",
    "    \n",
    "    #Format datestamp, if desired in output filename\n",
    "    if date_stamp:\n",
    "        todayDate = datetime.date.today()\n",
    "        todayDateStr = '_'+str(todayDate)\n",
    "    else:\n",
    "        todayDateStr=''\n",
    "\n",
    "    #Ensure the file suffix includes .\n",
    "    if filetype[0] == '.':\n",
    "        pass\n",
    "    else:\n",
    "        filetype = '.' + filetype\n",
    "\n",
    "    if file_id != '':\n",
    "        file_id = '_'+file_id\n",
    "\n",
    "    out_path = out_path.as_posix()+'/'\n",
    "    outPaths = []\n",
    "    for f in filenames:\n",
    "        outPaths.append(out_path+f+file_id+todayDateStr+filetype)\n",
    "\n",
    "    #Do export\n",
    "    if filetype.lower() in pickleList:\n",
    "        import pickle\n",
    "        for op in outPaths:\n",
    "            try:\n",
    "                with open(op, 'wb') as f:\n",
    "                    pickle.dump(grid_data, f)\n",
    "            except:\n",
    "                print('An error occured during export.')\n",
    "                print(op, 'could not be exported as a pickle object.')\n",
    "                print('Try again using different parameters.')\n",
    "    else:\n",
    "        import rioxarray as rxr\n",
    "        try:\n",
    "            if isinstance(grid_data, xr.Dataset):\n",
    "                if variable_sep:\n",
    "                    for i, var in enumerate(grid_data.data_vars):\n",
    "                        grid_data[var].rio.to_raster(outPaths[i])\n",
    "                else:\n",
    "                    grid_data.rio.to_raster(outPaths[0])\n",
    "            elif isinstance(grid_data, xr.DataArray):\n",
    "                if variable_sep:\n",
    "                    lyrs = grid_data.coords['Layer'].values\n",
    "                    for i, l in enumerate(lyrs):\n",
    "                        out_grid = grid_data.sel(Layer = l).copy()\n",
    "                        out_grid.rio.to_raster(outPaths[i])\n",
    "                else:\n",
    "                    grid_data.rio.to_raster(outPaths[0])\n",
    "            else:\n",
    "                grid_data.rio.to_raster(outPaths[0])\n",
    "        except:\n",
    "            print('An error occured during export.')\n",
    "            print('{} could not be exported as {} file.'.format(outPaths, filetype))\n",
    "            print('Try again using different parameters.')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('//isgs-sinkhole.ad.uillinois.edu/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "out_dir = pathlib.Path(out_dir)\n",
    "out_dir.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "export_grids(grid_data=layers_data, out_path=out_dir, file_id='Coarse', filetype='tif', variable_sep=True, date_stamp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyzDataIN = w4h.read_xyz(xyzpath=xyzInPATH, log=True)\n",
    "downholeData = w4h.define_dtypes(df=downholeDataIN, dtype_file='downholeDataTypes.txt', log=True) #Define datatypes of each column of the new dataframes\n",
    "headerData = w4h.define_dtypes(df=headerDataIN, dtype_file='headerDataTypes.txt', log=True)#Define datatypes of each column of the new dataframes\n",
    "xyzData = w4h.define_dtypes(df=xyzDataIN, dtype_file='xyzDataTypes.txt', log=True)\n",
    "studyAreaPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESL_StudyArea_5mi.shp\"\n",
    "studyAreaIN = w4h.read_study_area(studyareapath=studyAreaPath, log=True)\n",
    "modelGridPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\grid_625_raster.tif\"\n",
    "surfaceElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ILStateLidar_ClipExtentESL.tif\"\n",
    "bedrockElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESLBedrock.tif\"\n",
    "modelGrid = w4h.read_grid(datapath=modelGridPath, grid_type='model', study_area=studyAreaIN,  read_grid=True, clip_to_studyarea=True, log=True)#, gridcrs='EPSG:26715', studyAreacrs='EPSG:26715')\n",
    "surfaceElevGridIN = w4h.read_grid(datapath=surfaceElevPath, grid_type='surface', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "bedrockElevGridIN = w4h.read_grid(datapath=bedrockElevPath, grid_type='bedrock', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "#Code here for adding in control points\n",
    "headerData = w4h.xyz_metadata_merge(xyz=xyzData, metadata=headerData, log=True) #This probably needs to be updated\n",
    "headerData = w4h.coords2geometry(df=headerData, xcol='LONGITUDE', ycol='LATITUDE', crs='EPSG:4269', log=True)\n",
    "headerData = w4h.clip_gdf2study_area(study_area=studyAreaIN, gdf=headerData, gdf_crs='EPSG:4269', log=True)\n",
    "downholeData = w4h.remove_nonlocated(downholeData, headerData, log=True)\n",
    "headerData = w4h.remove_no_topo(df=headerData, verbose=True, log=True)\n",
    "donwholeData = w4h.drop_no_depth(downholeData, verbose=True, log=True) #Drop records with no depth information\n",
    "donwholeData = w4h.drop_bad_depth(downholeData, verbose=True, log=True)#Drop records with bad depth information (i.e., top depth > bottom depth) (Also calculates thickness of each record)\n",
    "downholeData = w4h.drop_no_formation(downholeData, verbose=True, log=True)\n",
    "#downholeData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "#headerData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "downholeData = pd.merge(left = downholeData, right = headerData, on='API_NUMBER')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "1eab39f8790fa4ef06ab4ebada9c1405c2ef16219adfedb21e90bf3fb356ecb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
