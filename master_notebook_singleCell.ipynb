{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{Fill in with information about this notebook}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scripts with functions made for this specific application\n",
    "import w4h\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "#Variables needed throughout, best to just assign now\n",
    "todayDate, dateSuffix = w4h.get_current_date() \n",
    "repoDir = pathlib.Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging data to C:\\Users\\riley\\Desktop\\log_2023-05-04_16-42-14.txt\n",
      "Most Recent version of this file is : ISGS_DOWNHOLE_DATA_2023-01-06.txt\n",
      "Most Recent version of this file is : ISGS_HEADER_2023-01-06.txt\n",
      "Most Recent version of this file is : xyzData.csv\n",
      "Using the following files:\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\ISGS_DOWNHOLE_DATA_2023-01-06.txt\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\ISGS_HEADER_2023-01-06.txt\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\xyzData.csv\n"
     ]
    }
   ],
   "source": [
    "directoryDir = r'\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\\\'[:-1]\n",
    "downholeDataPATH, headerDataPATH, xyzInPATH  = w4h.file_setup(db_dir=directoryDir, out_dir=r'C:\\Users\\riley\\Desktop', verbose=True, log=True)\n",
    "downholeDataIN, headerDataIN = w4h.read_raw_txt(data_filepath=downholeDataPATH, metadata_filepath=headerDataPATH, log=True) #Functions to read data into dataframes. Also excludes extraneous columns, and drops header data with no location information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyzDataIN = w4h.read_xyz(xyzpath=xyzInPATH, log=True)\n",
    "downholeData = w4h.define_dtypes(df=downholeDataIN, dtype_file='downholeDataTypes.txt', log=True) #Define datatypes of each column of the new dataframes\n",
    "headerData = w4h.define_dtypes(df=headerDataIN, dtype_file='headerDataTypes.txt', log=True)#Define datatypes of each column of the new dataframes\n",
    "xyzData = w4h.define_dtypes(df=xyzDataIN, dtype_file='xyzDataTypes.txt', log=True)\n",
    "studyAreaPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESL_StudyArea_5mi.shp\"\n",
    "studyAreaIN = w4h.read_study_area(studyareapath=studyAreaPath, log=True)\n",
    "modelGridPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\grid_625_raster.tif\"\n",
    "surfaceElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ILStateLidar_ClipExtentESL.tif\"\n",
    "bedrockElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESLBedrock.tif\"\n",
    "modelGrid = w4h.read_grid(datapath=modelGridPath, grid_type='model', study_area=studyAreaIN,  read_grid=True, clip_to_studyarea=True, log=True)#, gridcrs='EPSG:26715', studyAreacrs='EPSG:26715')\n",
    "surfaceElevGridIN = w4h.read_grid(datapath=surfaceElevPath, grid_type='surface', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "bedrockElevGridIN = w4h.read_grid(datapath=bedrockElevPath, grid_type='bedrock', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "#Code here for adding in control points\n",
    "headerData = w4h.xyz_metadata_merge(xyz=xyzData, metadata=headerData, log=True) #This probably needs to be updated\n",
    "headerData = w4h.coords2geometry(df=headerData, xcol='LONGITUDE', ycol='LATITUDE', crs='EPSG:4269', log=True)\n",
    "headerData = w4h.clip_gdf2study_area(study_area=studyAreaIN, gdf=headerData, gdf_crs='EPSG:4269', log=True)\n",
    "downholeData = w4h.remove_nonlocated(downholeData, headerData, log=True)\n",
    "headerData = w4h.remove_no_topo(df=headerData, verbose=True, log=True)\n",
    "donwholeData = w4h.drop_no_depth(downholeData, verbose=True, log=True) #Drop records with no depth information\n",
    "donwholeData = w4h.drop_bad_depth(downholeData, verbose=True, log=True)#Drop records with bad depth information (i.e., top depth > bottom depth) (Also calculates thickness of each record)\n",
    "downholeData = w4h.drop_no_formation(downholeData, verbose=True, log=True)\n",
    "#downholeData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "#headerData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "downholeData = pd.merge(left = downholeData, right = headerData, on='API_NUMBER')specTermsPATH, startTermsPATH = w4h.get_search_terms(spec_dir=str(repoDir)+'/resources/', specStartPattern='*SearchTerms-Specific*', startGlobPattern = '*SearchTerms-Start*', log=True)\n",
    "specTerms = w4h.read_dictionary_terms(dict_file=specTermsPATH, log=True)\n",
    "startTerms = w4h.read_dictionary_terms(dict_file=startTermsPATH, log=True)\n",
    "oldDictPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\WellData\\Dictionaries\\DICTIONARY_Updated-06-2018.csv\"\n",
    "oldDict = w4h.read_dictionary_terms(dict_file=oldDictPath, cols={'DESCRIPTION':'FORMATION', 'LITHOLOGY':'INTERPRETATION'}, class_flag=1, log=True)\n",
    "\n",
    "specTerms = pd.concat([specTerms, oldDict])\n",
    "specTerms.drop_duplicates(subset='FORMATION', inplace=True)\n",
    "specTerms.reset_index(inplace=True, drop=True)\n",
    "downholeData = w4h.specific_define(downholeData, specTerms, verbose=True, log=True)\n",
    "classifedDF, searchDF = w4h.split_defined(downholeData)\n",
    "searchDF = w4h.start_define(df=searchDF, terms_df=startTerms, verbose=True)\n",
    "downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF)\n",
    "classifedDF, searchDF = w4h.split_defined(downholeData)\n",
    "searchDF = w4h.depth_define(searchDF, thresh=550, verbose=True, log=True)\n",
    "downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF)\n",
    "downholeData = w4h.fill_unclassified(downholeData)\n",
    "#dictDir = \"\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\ISWS_HydroGeo\\\\WellDataAutoClassification\\\\SupportingDocs\\\\\"\n",
    "targetInterpDF = w4h.read_lithologies(log=True)\n",
    "downholeData = w4h.merge_lithologies(df=downholeData, targinterps_df=targetInterpDF)\n",
    "wellsDF = w4h.get_unique_wells(downholeData)\n",
    "downholeData = w4h.sort_dataframe(df=downholeData, sort_cols=['API_NUMBER','TOP'], remove_nans=True)\n",
    "inGrids = [bedrockElevGridIN, surfaceElevGridIN]\n",
    "bedrockGrid, surfaceGrid = w4h.align_rasters(grids_unaligned=inGrids, modelgrid=modelGrid, log=True)\n",
    "driftThickGrid, layerThickGrid = w4h.get_drift_thick(surface=surfaceGrid, bedrock=bedrockGrid, layers=9, plot=False, log=True)\n",
    "headerData = w4h.sample_raster_points(raster=bedrockGrid, points_df=headerData, new_col='BEDROCK_ELEV_FT', log=True)\n",
    "headerData = w4h.sample_raster_points(raster=surfaceGrid, points_df=headerData, new_col='SURFACE_ELEV_FT', log=True)\n",
    "headerData = w4h.sample_raster_points(raster=driftThickGrid, points_df=headerData, new_col='BEDROCK_DEPTH_FT', log=True)\n",
    "headerData = w4h.sample_raster_points(raster=layerThickGrid, points_df=headerData, new_col='LAYER_THICK_FT', log=True)\n",
    "headerData = w4h.get_layer_depths(well_metadata=headerData, no_layers=9, log=True)\n",
    "downholeData_layerInfo = w4h.merge_tables(data_df=downholeData,  data_cols=None, header_cols=None, header_df=headerData,on='API_NUMBER', how='inner', auto_pick_cols=True, log=True)\n",
    "#downholeData = downholeData_layerInfo.copy()\n",
    "resdf = w4h.layer_target_thick(downholeData_layerInfo, layers=9, outfile_prefix='CoarseFine', log=True)\n",
    "layers_data = w4h.layer_interp(points=resdf, layers=9, grid=modelGrid, method='lin', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xyzData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m bedrockElevGridIN \u001b[39m=\u001b[39m w4h\u001b[39m.\u001b[39mread_grid(datapath\u001b[39m=\u001b[39mbedrockElevPath, grid_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbedrock\u001b[39m\u001b[39m'\u001b[39m, study_area\u001b[39m=\u001b[39mstudyAreaIN, use_service\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, clip_to_studyarea\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[39m#Code here for adding in control points\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m headerData \u001b[39m=\u001b[39m w4h\u001b[39m.\u001b[39mxyz_metadata_merge(xyz\u001b[39m=\u001b[39mxyzData, metadata\u001b[39m=\u001b[39mheaderData, log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m#This probably needs to be updated\u001b[39;00m\n\u001b[0;32m     11\u001b[0m headerData \u001b[39m=\u001b[39m w4h\u001b[39m.\u001b[39mcoords2geometry(df\u001b[39m=\u001b[39mheaderData, xCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLONGITUDE\u001b[39m\u001b[39m'\u001b[39m, yCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLATITUDE\u001b[39m\u001b[39m'\u001b[39m, crs\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEPSG:4269\u001b[39m\u001b[39m'\u001b[39m, log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m headerData \u001b[39m=\u001b[39m w4h\u001b[39m.\u001b[39mclip_gdf2study_area(studyarea\u001b[39m=\u001b[39mstudyAreaIN, headerdata\u001b[39m=\u001b[39mheaderData, headerCRS\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEPSG:4269\u001b[39m\u001b[39m'\u001b[39m, log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xyzData' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well records removed: 0\n",
      "Number of rows before dropping those without surface elevation information: 8151\n",
      "Number of rows after dropping those without surface elevation information: 8151\n",
      "Number of rows before dropping those without record depth information: 56343\n",
      "Number of rows after dropping those without record depth information: 55759\n",
      "Number of well records without formation information deleted: 584\n",
      "Number of rows before dropping those with obviously bad depth information: 56343\n",
      "Number of rows after dropping those with obviously bad depth information: 55737\n",
      "Well records deleted: 606\n",
      "Number of rows before dropping those without FORMATION information: 56343\n",
      "Number of rows after dropping those without FORMATION information: 56343\n",
      "Well records deleted: 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Recent version of this file is : SearchTerms-Specific_2022-11-16_essCols.csv\n",
      "Most Recent version of this file is : SearchTerms-Start.csv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "specific_define() got an unexpected keyword argument 'printouts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m specTerms\u001b[39m.\u001b[39mdrop_duplicates(subset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFORMATION\u001b[39m\u001b[39m'\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m specTerms\u001b[39m.\u001b[39mreset_index(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m downholeData \u001b[39m=\u001b[39m w4h\u001b[39m.\u001b[39;49mspecific_define(downholeData, specTerms, printouts\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, log\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     12\u001b[0m classifedDF, searchDF \u001b[39m=\u001b[39m w4h\u001b[39m.\u001b[39msplit_defined(downholeData)\n\u001b[0;32m     13\u001b[0m searchDF \u001b[39m=\u001b[39m w4h\u001b[39m.\u001b[39mstart_define(df\u001b[39m=\u001b[39msearchDF, starterms\u001b[39m=\u001b[39mstartTerms, printouts\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: specific_define() got an unexpected keyword argument 'printouts'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export (rio)xarray dataarrays and datasets\n",
    "def export_grids(grid_data, out_path, file_id='',filetype='tif', variable_sep=True, date_stamp=True):\n",
    "    \"\"\"Function to export grids to files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grid_data : xarray DataArray or xarray Dataset\n",
    "        Dataset or dataarray to be exported\n",
    "    out_path : str or pathlib.Path object\n",
    "        Output location for data export. If variable_sep=True, this should be a directory. Otherwise, this should also include the filename. The file extension should not be included here.\n",
    "    filetype : str, optional\n",
    "        Output filetype. Can either be pickle or any file extension supported by rioxarray.rio.to_raster(). Can either include period or not., by default 'tif'\n",
    "    variable_sep : bool, optional\n",
    "        If grid_data is an xarray Dataset, this will export each variable in the dataset as a separate file, including the variable name in the filename, by default False\n",
    "    date_stamp : bool, optional\n",
    "        Whether to include a date stamp in the file name., by default True\n",
    "    \"\"\"\n",
    "\n",
    "    #Initialize lists to determine which filetype will be used for export\n",
    "    ncdfList = ['netcdf', 'ncdf', 'n']\n",
    "    tifList = ['tif', 'tiff', 'geotiff', 'geotif', 't']\n",
    "    pickleList = ['pickle', 'pkl', 'p']\n",
    "\n",
    "    #Format output string(s)\n",
    "    #Format output filepath\n",
    "    if type(out_path) is str or isinstance(out_path, pathlib.PurePath):\n",
    "        if isinstance(out_path, pathlib.PurePath):\n",
    "            pass\n",
    "        else:\n",
    "            out_path = pathlib.Path(out_path)\n",
    "        if out_path.parent.exists()==False:\n",
    "            print('Directory does not exist. Please enter a different value for the out_path parameter.')\n",
    "            return        \n",
    "\n",
    "        if out_path.is_dir():\n",
    "            if isinstance(grid_data, xr.DataArray):\n",
    "                if variable_sep:\n",
    "                    lyrs = grid_data.coords['Layer'].values\n",
    "                    filenames = []\n",
    "                    for l in lyrs:\n",
    "                        filenames.append('Layer'+str(l))\n",
    "                else:\n",
    "                    filenames = ['AllLayers']\n",
    "            if isinstance(grid_data, xr.Dataset):\n",
    "                if variable_sep:\n",
    "                    filenames = []\n",
    "                    for var in grid_data:\n",
    "                        filenames.append(var)\n",
    "                else:\n",
    "                    filenames = ['AllLayers']    \n",
    "        else:\n",
    "            filenames = [out_path.stem]\n",
    "            out_path = out_path.parent\n",
    "\n",
    "    else:\n",
    "        print('Please input string or pathlib object for out_path parameters')\n",
    "        return\n",
    "    \n",
    "    #Format datestamp, if desired in output filename\n",
    "    if date_stamp:\n",
    "        todayDate = datetime.date.today()\n",
    "        todayDateStr = '_'+str(todayDate)\n",
    "    else:\n",
    "        todayDateStr=''\n",
    "\n",
    "    #Ensure the file suffix includes .\n",
    "    if filetype[0] == '.':\n",
    "        pass\n",
    "    else:\n",
    "        filetype = '.' + filetype\n",
    "\n",
    "    if file_id != '':\n",
    "        file_id = '_'+file_id\n",
    "\n",
    "    out_path = out_path.as_posix()+'/'\n",
    "    outPaths = []\n",
    "    for f in filenames:\n",
    "        outPaths.append(out_path+f+file_id+todayDateStr+filetype)\n",
    "\n",
    "    #Do export\n",
    "    if filetype.lower() in pickleList:\n",
    "        import pickle\n",
    "        for op in outPaths:\n",
    "            try:\n",
    "                with open(op, 'wb') as f:\n",
    "                    pickle.dump(grid_data, f)\n",
    "            except:\n",
    "                print('An error occured during export.')\n",
    "                print(op, 'could not be exported as a pickle object.')\n",
    "                print('Try again using different parameters.')\n",
    "    else:\n",
    "        import rioxarray as rxr\n",
    "        try:\n",
    "            if isinstance(grid_data, xr.Dataset):\n",
    "                if variable_sep:\n",
    "                    for i, var in enumerate(grid_data.data_vars):\n",
    "                        grid_data[var].rio.to_raster(outPaths[i])\n",
    "                else:\n",
    "                    grid_data.rio.to_raster(outPaths[0])\n",
    "            elif isinstance(grid_data, xr.DataArray):\n",
    "                if variable_sep:\n",
    "                    lyrs = grid_data.coords['Layer'].values\n",
    "                    for i, l in enumerate(lyrs):\n",
    "                        out_grid = grid_data.sel(Layer = l).copy()\n",
    "                        out_grid.rio.to_raster(outPaths[i])\n",
    "                else:\n",
    "                    grid_data.rio.to_raster(outPaths[0])\n",
    "            else:\n",
    "                grid_data.rio.to_raster(outPaths[0])\n",
    "        except:\n",
    "            print('An error occured during export.')\n",
    "            print('{} could not be exported as {} file.'.format(outPaths, filetype))\n",
    "            print('Try again using different parameters.')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('//isgs-sinkhole.ad.uillinois.edu/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "out_dir = pathlib.Path(out_dir)\n",
    "out_dir.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "export_grids(grid_data=layers_data, out_path=out_dir, file_id='Coarse', filetype='tif', variable_sep=True, date_stamp=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "1eab39f8790fa4ef06ab4ebada9c1405c2ef16219adfedb21e90bf3fb356ecb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
