{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{Fill in with information about this notebook}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import numpy as np #Data manipulation\n",
    "import pandas as pd #Point data manipulation and organization\n",
    "import xarray as xr #Raster data manipulation and organization\n",
    "\n",
    "import pathlib  #For filepaths, io, etc.\n",
    "import os       #For several system-based commands\n",
    "import datetime #For manipulation of time data, including file creation/modification times\n",
    "import json     #For dictionary io, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt #For plotting and data vizualization\n",
    "import geopandas as gpd         #For organization and manipulation of vector data in space (study area and some data points)\n",
    "import rioxarray as rxr         #For orgnaization and manipulation of raster data\n",
    "from scipy import interpolate\n",
    "import shapely                  #For converting coordinates to point geometry\n",
    "#Not sure if this cell is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import w4h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scripts with functions made for this specific application\n",
    "import pathlib\n",
    "import os\n",
    "#Variables needed throughout, best to just assign now\n",
    "todayDate, dateSuffix = w4h.get_current_date() \n",
    "repoDir = pathlib.Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Recent version of this file is : ISGS_DOWNHOLE_DATA_2023-01-06.txt\n",
      "Most Recent version of this file is : ISGS_HEADER_2023-01-06.txt\n",
      "Most Recent version of this file is : xyzData.csv\n",
      "Using the following files:\n",
      "\n",
      "\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\ISGS_DOWNHOLE_DATA_2023-01-06.txt\n",
      "\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\ISGS_HEADER_2023-01-06.txt\n",
      "\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\xyzData.csv\n",
      "Downhole Data has 3054409 valid well records.\n",
      "Header Data has 636855 unique wells with valid location information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\.conda\\envs\\raster38\\lib\\site-packages\\geopandas\\array.py:275: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  return GeometryArray(vectorized.points_from_xy(x, y, z), crs=crs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2998078 records removed without location information.\n",
      "56331 wells remain from 7188 located wells in study area.\n",
      "Well records removed: 0\n",
      "Number of rows before dropping those without surface elevation information: 8150\n",
      "Number of rows after dropping those without surface elevation information: 8150\n",
      "Number of rows before dropping those without record depth information: 56331\n",
      "Number of rows after dropping those without record depth information: 55747\n",
      "Number of well records without formation information deleted: 584\n",
      "Number of rows before dropping those with obviously bad depth information: 56331\n",
      "Number of rows after dropping those with obviously bad depth information: 55725\n",
      "Well records deleted: 606\n",
      "Number of rows before dropping those without FORMATION information: 56331\n",
      "Number of rows after dropping those without FORMATION information: 56331\n",
      "Well records deleted: 0\n",
      "Most Recent version of this file is : SearchTerms-Specific_2022-11-16_essCols.csv\n",
      "Most Recent version of this file is : SearchTerms-Start.csv\n",
      "0                                clay brown\n",
      "1                     clay dark brown, wood\n",
      "2                     clay gray sandy, soft\n",
      "3                         clay gray, sticky\n",
      "4                 sandrock gray hard, dirty\n",
      "                        ...                \n",
      "56326               brown silt (levee fill)\n",
      "56327                             gray clay\n",
      "56328    gray clay (with thin sand streaks)\n",
      "56329                       gray, fine sand\n",
      "56330         gray, mottled with brown clay\n",
      "Name: FORMATION, Length: 56331, dtype: object\n",
      "0                                clay brown\n",
      "1                     clay dark brown, wood\n",
      "2                     clay gray sandy, soft\n",
      "3                         clay gray, sticky\n",
      "4                 sandrock gray hard, dirty\n",
      "                        ...                \n",
      "56326               brown silt (levee fill)\n",
      "56327                             gray clay\n",
      "56328    gray clay (with thin sand streaks)\n",
      "56329                       gray, fine sand\n",
      "56330         gray, mottled with brown clay\n",
      "Name: FORMATION, Length: 56331, dtype: object\n",
      "0                                clay brown\n",
      "1                     clay dark brown, wood\n",
      "2                     clay gray sandy, soft\n",
      "3                         clay gray, sticky\n",
      "4                 sandrock gray hard, dirty\n",
      "                        ...                \n",
      "56326               brown silt (levee fill)\n",
      "56327                             gray clay\n",
      "56328    gray clay (with thin sand streaks)\n",
      "56329                       gray, fine sand\n",
      "56330         gray, mottled with brown clay\n",
      "Name: FORMATION, Length: 56331, dtype: object\n",
      "Records Classified with full search term: 31582\n",
      "Records Classified with full search term: 56.07% of data\n",
      "Start Term process should be done by 10:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\LocalData\\Github\\wells4hydrogeology\\w4h\\classify.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['CLASS_FLAG'].where(~df['FORMATION'].str.startswith(s,na=False),4,inplace=True)\n",
      "c:\\Users\\riley\\LocalData\\Github\\wells4hydrogeology\\w4h\\classify.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['INTERPRETATION'].where(~df['FORMATION'].str.startswith(s,na=False),starterms.loc[i,'INTERPRETATION'],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records classified with start search term: 3586\n",
      "Records classified with start search term: 14.49% of remaining data\n",
      "Records classified as bedrock that were deeper than 550': 359\n",
      "This represents 1.7% of the unclassified data in this dataframe.\n",
      "Number of unique wells in downholeData: 7188\n",
      "BEDROCK_ELEV_FT sampling should be done by 10:50\n",
      "SURFACE_ELEV_FT sampling should be done by 10:50\n",
      "BEDROCK_DEPTH_FT sampling should be done by 10:50\n",
      "LAYER_THICK_FT sampling should be done by 10:50\n",
      "REMOVING LATITUDE\n",
      "REMOVING LONGITUDE\n",
      "REMOVING ELEV_FT\n",
      "REMOVING geometry\n",
      "Index(['API_NUMBER', 'TABLE_NAME', 'FORMATION', 'THICKNESS', 'TOP', 'BOTTOM',\n",
      "       'TOTAL_DEPTH', 'SECTION', 'TWP', 'TDIR', 'RNG', 'RDIR', 'MERIDIAN',\n",
      "       'QUARTERS', 'ELEVATION', 'ELEVREF', 'COUNTY_CODE', 'ELEVSOURCE',\n",
      "       'LATITUDE', 'LONGITUDE', 'ELEV_FT', 'geometry', 'INTERPRETATION',\n",
      "       'CLASS_FLAG', 'BEDROCK_FLAG', 'TARGET', 'BEDROCK_ELEV_FT',\n",
      "       'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT',\n",
      "       'LONGITUDE_PROJ', 'LATITUDE_PROJ', 'DEPTH_FT_LAYER1', 'DEPTH_FT_LAYER2',\n",
      "       'DEPTH_FT_LAYER3', 'DEPTH_FT_LAYER4', 'DEPTH_FT_LAYER5',\n",
      "       'DEPTH_FT_LAYER6', 'DEPTH_FT_LAYER7', 'DEPTH_FT_LAYER8',\n",
      "       'DEPTH_FT_LAYER9', 'ELEV_FT_LAYER1', 'ELEV_FT_LAYER2', 'ELEV_FT_LAYER3',\n",
      "       'ELEV_FT_LAYER4', 'ELEV_FT_LAYER5', 'ELEV_FT_LAYER6', 'ELEV_FT_LAYER7',\n",
      "       'ELEV_FT_LAYER8', 'ELEV_FT_LAYER9', 'TOP_ELEV_FT', 'BOT_ELEV_FT',\n",
      "       'TARG_THICK_FT'],\n",
      "      dtype='object')\n",
      "Index(['API_NUMBER', 'TABLE_NAME', 'FORMATION', 'THICKNESS', 'TOP', 'BOTTOM',\n",
      "       'TOTAL_DEPTH', 'SECTION', 'TWP', 'TDIR', 'RNG', 'RDIR', 'MERIDIAN',\n",
      "       'QUARTERS', 'ELEVATION', 'ELEVREF', 'COUNTY_CODE', 'ELEVSOURCE',\n",
      "       'LATITUDE', 'LONGITUDE', 'ELEV_FT', 'geometry', 'INTERPRETATION',\n",
      "       'CLASS_FLAG', 'BEDROCK_FLAG', 'TARGET', 'BEDROCK_ELEV_FT',\n",
      "       'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT',\n",
      "       'LONGITUDE_PROJ', 'LATITUDE_PROJ', 'DEPTH_FT_LAYER1', 'DEPTH_FT_LAYER2',\n",
      "       'DEPTH_FT_LAYER3', 'DEPTH_FT_LAYER4', 'DEPTH_FT_LAYER5',\n",
      "       'DEPTH_FT_LAYER6', 'DEPTH_FT_LAYER7', 'DEPTH_FT_LAYER8',\n",
      "       'DEPTH_FT_LAYER9', 'ELEV_FT_LAYER1', 'ELEV_FT_LAYER2', 'ELEV_FT_LAYER3',\n",
      "       'ELEV_FT_LAYER4', 'ELEV_FT_LAYER5', 'ELEV_FT_LAYER6', 'ELEV_FT_LAYER7',\n",
      "       'ELEV_FT_LAYER8', 'ELEV_FT_LAYER9', 'TOP_ELEV_FT', 'BOT_ELEV_FT',\n",
      "       'TARG_THICK_FT'],\n",
      "      dtype='object')\n",
      "Index(['API_NUMBER', 'TABLE_NAME', 'FORMATION', 'THICKNESS', 'TOP', 'BOTTOM',\n",
      "       'TOTAL_DEPTH', 'SECTION', 'TWP', 'TDIR', 'RNG', 'RDIR', 'MERIDIAN',\n",
      "       'QUARTERS', 'ELEVATION', 'ELEVREF', 'COUNTY_CODE', 'ELEVSOURCE',\n",
      "       'LATITUDE', 'LONGITUDE', 'ELEV_FT', 'geometry', 'INTERPRETATION',\n",
      "       'CLASS_FLAG', 'BEDROCK_FLAG', 'TARGET', 'BEDROCK_ELEV_FT',\n",
      "       'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT',\n",
      "       'LONGITUDE_PROJ', 'LATITUDE_PROJ', 'DEPTH_FT_LAYER1', 'DEPTH_FT_LAYER2',\n",
      "       'DEPTH_FT_LAYER3', 'DEPTH_FT_LAYER4', 'DEPTH_FT_LAYER5',\n",
      "       'DEPTH_FT_LAYER6', 'DEPTH_FT_LAYER7', 'DEPTH_FT_LAYER8',\n",
      "       'DEPTH_FT_LAYER9', 'ELEV_FT_LAYER1', 'ELEV_FT_LAYER2', 'ELEV_FT_LAYER3',\n",
      "       'ELEV_FT_LAYER4', 'ELEV_FT_LAYER5', 'ELEV_FT_LAYER6', 'ELEV_FT_LAYER7',\n",
      "       'ELEV_FT_LAYER8', 'ELEV_FT_LAYER9', 'TOP_ELEV_FT', 'BOT_ELEV_FT',\n",
      "       'TARG_THICK_FT'],\n",
      "      dtype='object')\n",
      "Index(['API_NUMBER', 'TABLE_NAME', 'FORMATION', 'THICKNESS', 'TOP', 'BOTTOM',\n",
      "       'TOTAL_DEPTH', 'SECTION', 'TWP', 'TDIR', 'RNG', 'RDIR', 'MERIDIAN',\n",
      "       'QUARTERS', 'ELEVATION', 'ELEVREF', 'COUNTY_CODE', 'ELEVSOURCE',\n",
      "       'LATITUDE', 'LONGITUDE', 'ELEV_FT', 'geometry', 'INTERPRETATION',\n",
      "       'CLASS_FLAG', 'BEDROCK_FLAG', 'TARGET', 'BEDROCK_ELEV_FT',\n",
      "       'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT',\n",
      "       'LONGITUDE_PROJ', 'LATITUDE_PROJ', 'DEPTH_FT_LAYER1', 'DEPTH_FT_LAYER2',\n",
      "       'DEPTH_FT_LAYER3', 'DEPTH_FT_LAYER4', 'DEPTH_FT_LAYER5',\n",
      "       'DEPTH_FT_LAYER6', 'DEPTH_FT_LAYER7', 'DEPTH_FT_LAYER8',\n",
      "       'DEPTH_FT_LAYER9', 'ELEV_FT_LAYER1', 'ELEV_FT_LAYER2', 'ELEV_FT_LAYER3',\n",
      "       'ELEV_FT_LAYER4', 'ELEV_FT_LAYER5', 'ELEV_FT_LAYER6', 'ELEV_FT_LAYER7',\n",
      "       'ELEV_FT_LAYER8', 'ELEV_FT_LAYER9', 'TOP_ELEV_FT', 'BOT_ELEV_FT',\n",
      "       'TARG_THICK_FT'],\n",
      "      dtype='object')\n",
      "Index(['API_NUMBER', 'TABLE_NAME', 'FORMATION', 'THICKNESS', 'TOP', 'BOTTOM',\n",
      "       'TOTAL_DEPTH', 'SECTION', 'TWP', 'TDIR', 'RNG', 'RDIR', 'MERIDIAN',\n",
      "       'QUARTERS', 'ELEVATION', 'ELEVREF', 'COUNTY_CODE', 'ELEVSOURCE',\n",
      "       'LATITUDE', 'LONGITUDE', 'ELEV_FT', 'geometry', 'INTERPRETATION',\n",
      "       'CLASS_FLAG', 'BEDROCK_FLAG', 'TARGET', 'BEDROCK_ELEV_FT',\n",
      "       'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT',\n",
      "       'LONGITUDE_PROJ', 'LATITUDE_PROJ', 'DEPTH_FT_LAYER1', 'DEPTH_FT_LAYER2',\n",
      "       'DEPTH_FT_LAYER3', 'DEPTH_FT_LAYER4', 'DEPTH_FT_LAYER5',\n",
      "       'DEPTH_FT_LAYER6', 'DEPTH_FT_LAYER7', 'DEPTH_FT_LAYER8',\n",
      "       'DEPTH_FT_LAYER9', 'ELEV_FT_LAYER1', 'ELEV_FT_LAYER2', 'ELEV_FT_LAYER3',\n",
      "       'ELEV_FT_LAYER4', 'ELEV_FT_LAYER5', 'ELEV_FT_LAYER6', 'ELEV_FT_LAYER7',\n",
      "       'ELEV_FT_LAYER8', 'ELEV_FT_LAYER9', 'TOP_ELEV_FT', 'BOT_ELEV_FT',\n",
      "       'TARG_THICK_FT'],\n",
      "      dtype='object')\n",
      "Index(['API_NUMBER', 'TABLE_NAME', 'FORMATION', 'THICKNESS', 'TOP', 'BOTTOM',\n",
      "       'TOTAL_DEPTH', 'SECTION', 'TWP', 'TDIR', 'RNG', 'RDIR', 'MERIDIAN',\n",
      "       'QUARTERS', 'ELEVATION', 'ELEVREF', 'COUNTY_CODE', 'ELEVSOURCE',\n",
      "       'LATITUDE', 'LONGITUDE', 'ELEV_FT', 'geometry', 'INTERPRETATION',\n",
      "       'CLASS_FLAG', 'BEDROCK_FLAG', 'TARGET', 'BEDROCK_ELEV_FT',\n",
      "       'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT',\n",
      "       'LONGITUDE_PROJ', 'LATITUDE_PROJ', 'DEPTH_FT_LAYER1', 'DEPTH_FT_LAYER2',\n",
      "       'DEPTH_FT_LAYER3', 'DEPTH_FT_LAYER4', 'DEPTH_FT_LAYER5',\n",
      "       'DEPTH_FT_LAYER6', 'DEPTH_FT_LAYER7', 'DEPTH_FT_LAYER8',\n",
      "       'DEPTH_FT_LAYER9', 'ELEV_FT_LAYER1', 'ELEV_FT_LAYER2', 'ELEV_FT_LAYER3',\n",
      "       'ELEV_FT_LAYER4', 'ELEV_FT_LAYER5', 'ELEV_FT_LAYER6', 'ELEV_FT_LAYER7',\n",
      "       'ELEV_FT_LAYER8', 'ELEV_FT_LAYER9', 'TOP_ELEV_FT', 'BOT_ELEV_FT',\n",
      "       'TARG_THICK_FT'],\n",
      "      dtype='object')\n",
      "Index(['API_NUMBER', 'TABLE_NAME', 'FORMATION', 'THICKNESS', 'TOP', 'BOTTOM',\n",
      "       'TOTAL_DEPTH', 'SECTION', 'TWP', 'TDIR', 'RNG', 'RDIR', 'MERIDIAN',\n",
      "       'QUARTERS', 'ELEVATION', 'ELEVREF', 'COUNTY_CODE', 'ELEVSOURCE',\n",
      "       'LATITUDE', 'LONGITUDE', 'ELEV_FT', 'geometry', 'INTERPRETATION',\n",
      "       'CLASS_FLAG', 'BEDROCK_FLAG', 'TARGET', 'BEDROCK_ELEV_FT',\n",
      "       'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT',\n",
      "       'LONGITUDE_PROJ', 'LATITUDE_PROJ', 'DEPTH_FT_LAYER1', 'DEPTH_FT_LAYER2',\n",
      "       'DEPTH_FT_LAYER3', 'DEPTH_FT_LAYER4', 'DEPTH_FT_LAYER5',\n",
      "       'DEPTH_FT_LAYER6', 'DEPTH_FT_LAYER7', 'DEPTH_FT_LAYER8',\n",
      "       'DEPTH_FT_LAYER9', 'ELEV_FT_LAYER1', 'ELEV_FT_LAYER2', 'ELEV_FT_LAYER3',\n",
      "       'ELEV_FT_LAYER4', 'ELEV_FT_LAYER5', 'ELEV_FT_LAYER6', 'ELEV_FT_LAYER7',\n",
      "       'ELEV_FT_LAYER8', 'ELEV_FT_LAYER9', 'TOP_ELEV_FT', 'BOT_ELEV_FT',\n",
      "       'TARG_THICK_FT'],\n",
      "      dtype='object')\n",
      "Index(['API_NUMBER', 'TABLE_NAME', 'FORMATION', 'THICKNESS', 'TOP', 'BOTTOM',\n",
      "       'TOTAL_DEPTH', 'SECTION', 'TWP', 'TDIR', 'RNG', 'RDIR', 'MERIDIAN',\n",
      "       'QUARTERS', 'ELEVATION', 'ELEVREF', 'COUNTY_CODE', 'ELEVSOURCE',\n",
      "       'LATITUDE', 'LONGITUDE', 'ELEV_FT', 'geometry', 'INTERPRETATION',\n",
      "       'CLASS_FLAG', 'BEDROCK_FLAG', 'TARGET', 'BEDROCK_ELEV_FT',\n",
      "       'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT',\n",
      "       'LONGITUDE_PROJ', 'LATITUDE_PROJ', 'DEPTH_FT_LAYER1', 'DEPTH_FT_LAYER2',\n",
      "       'DEPTH_FT_LAYER3', 'DEPTH_FT_LAYER4', 'DEPTH_FT_LAYER5',\n",
      "       'DEPTH_FT_LAYER6', 'DEPTH_FT_LAYER7', 'DEPTH_FT_LAYER8',\n",
      "       'DEPTH_FT_LAYER9', 'ELEV_FT_LAYER1', 'ELEV_FT_LAYER2', 'ELEV_FT_LAYER3',\n",
      "       'ELEV_FT_LAYER4', 'ELEV_FT_LAYER5', 'ELEV_FT_LAYER6', 'ELEV_FT_LAYER7',\n",
      "       'ELEV_FT_LAYER8', 'ELEV_FT_LAYER9', 'TOP_ELEV_FT', 'BOT_ELEV_FT',\n",
      "       'TARG_THICK_FT'],\n",
      "      dtype='object')\n",
      "Index(['API_NUMBER', 'TABLE_NAME', 'FORMATION', 'THICKNESS', 'TOP', 'BOTTOM',\n",
      "       'TOTAL_DEPTH', 'SECTION', 'TWP', 'TDIR', 'RNG', 'RDIR', 'MERIDIAN',\n",
      "       'QUARTERS', 'ELEVATION', 'ELEVREF', 'COUNTY_CODE', 'ELEVSOURCE',\n",
      "       'LATITUDE', 'LONGITUDE', 'ELEV_FT', 'geometry', 'INTERPRETATION',\n",
      "       'CLASS_FLAG', 'BEDROCK_FLAG', 'TARGET', 'BEDROCK_ELEV_FT',\n",
      "       'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT',\n",
      "       'LONGITUDE_PROJ', 'LATITUDE_PROJ', 'DEPTH_FT_LAYER1', 'DEPTH_FT_LAYER2',\n",
      "       'DEPTH_FT_LAYER3', 'DEPTH_FT_LAYER4', 'DEPTH_FT_LAYER5',\n",
      "       'DEPTH_FT_LAYER6', 'DEPTH_FT_LAYER7', 'DEPTH_FT_LAYER8',\n",
      "       'DEPTH_FT_LAYER9', 'ELEV_FT_LAYER1', 'ELEV_FT_LAYER2', 'ELEV_FT_LAYER3',\n",
      "       'ELEV_FT_LAYER4', 'ELEV_FT_LAYER5', 'ELEV_FT_LAYER6', 'ELEV_FT_LAYER7',\n",
      "       'ELEV_FT_LAYER8', 'ELEV_FT_LAYER9', 'TOP_ELEV_FT', 'BOT_ELEV_FT',\n",
      "       'TARG_THICK_FT'],\n",
      "      dtype='object')\n",
      "Completed interpolation for Layer 1\n",
      "Completed interpolation for Layer 2\n",
      "Completed interpolation for Layer 3\n",
      "Completed interpolation for Layer 4\n",
      "Completed interpolation for Layer 5\n",
      "Completed interpolation for Layer 6\n",
      "Completed interpolation for Layer 7\n",
      "Completed interpolation for Layer 8\n",
      "Completed interpolation for Layer 9\n"
     ]
    }
   ],
   "source": [
    "directoryDir = r'\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\\\'[:-1]\n",
    "downholeDataPATH, headerDataPATH, xyzInPATH  = w4h.filesSetup(db_dir=directoryDir)\n",
    "headerDataIN, downholeDataIN = w4h.readRawTxtData(downholefile=downholeDataPATH, headerfile=headerDataPATH) #Functions to read data into dataframes. Also excludes extraneous columns, and drops header data with no location information\n",
    "xyzDataIN = w4h.readXYZData(xyzfile=xyzInPATH)\n",
    "downholeData = w4h.defineDataTypes(downholeDataIN, dtypeFile='downholeDataTypes.txt') #Define datatypes of each column of the new dataframes\n",
    "headerData = w4h.defineDataTypes(headerDataIN, dtypeFile='headerDataTypes.txt')#Define datatypes of each column of the new dataframes\n",
    "xyzData = w4h.defineDataTypes(xyzDataIN, dtypeFile='xyzDataTypes.txt')\n",
    "studyAreaPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESL_StudyArea_5mi.shp\"\n",
    "studyAreaIN = w4h.read_study_area(studyAreaPath)\n",
    "modelGridPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\grid_625_raster.tif\"\n",
    "surfaceElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ILStateLidar_ClipExtentESL.tif\"\n",
    "bedrockElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESLBedrock.tif\"\n",
    "modelGrid = w4h.read_grid(datapath=modelGridPath, grid_type='model', studyArea=studyAreaIN,  read_grid=True, clip2SA=True)#, gridcrs='EPSG:26715', studyAreacrs='EPSG:26715')\n",
    "surfaceElevGridIN = w4h.read_grid(datapath=surfaceElevPath, grid_type='surface', studyArea=studyAreaIN, use_service=False, clip2SA=True)\n",
    "bedrockElevGridIN = w4h.read_grid(datapath=bedrockElevPath, grid_type='bedrock', studyArea=studyAreaIN, use_service=False, clip2SA=True)\n",
    "#Code here for adding in control points\n",
    "headerData = w4h.addElevtoHeader(xyzData, headerData) #This probably needs to be updated\n",
    "headerData = w4h.coords2Geometry(df=headerData, xCol='LONGITUDE', yCol='LATITUDE', crs='EPSG:4269')\n",
    "headerData = w4h.clipHeader2StudyArea(studyarea=studyAreaIN, headerdata=headerData, headerCRS='EPSG:4269')\n",
    "downholeData = w4h.removeNonlocatedData(downholeData, headerData)\n",
    "headerData = w4h.removenotopo(df=headerData, printouts=True)\n",
    "donwholeData = w4h.dropnodepth(downholeData, printouts=True) #Drop records with no depth information\n",
    "donwholeData = w4h.dropbaddepth(downholeData, printouts=True)#Drop records with bad depth information (i.e., top depth > bottom depth) (Also calculates thickness of each record)\n",
    "downholeData = w4h.dropnoformation(downholeData, printouts=True)\n",
    "downholeData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "headerData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "downholeData = pd.merge(left = downholeData, right = headerData, on='API_NUMBER')\n",
    "specTermsPATH, startTermsPATH = w4h.searchTermFilePaths(dictdir=str(repoDir)+'/resources/', specStartPattern='*SearchTerms-Specific*', startGlobPattern = '*SearchTerms-Start*')\n",
    "specTerms = w4h.read_dictionary_terms(dict_file=specTermsPATH)\n",
    "startTerms = w4h.read_dictionary_terms(dict_file=startTermsPATH)\n",
    "oldDictPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\WellData\\Dictionaries\\DICTIONARY_Updated-06-2018.csv\"\n",
    "oldDict = w4h.read_dictionary_terms(dict_file=oldDictPath, cols={'DESCRIPTION':'FORMATION', 'LITHOLOGY':'INTERPRETATION'}, class_flag=1)\n",
    "specTerms = pd.concat([specTerms, oldDict])\n",
    "specTerms.drop_duplicates(subset='FORMATION', inplace=True)\n",
    "specTerms.reset_index(inplace=True, drop=True)\n",
    "downholeData = w4h.specific_define(downholeData, specTerms, printouts=True)\n",
    "classifedDF, searchDF = w4h.split_defined(downholeData)\n",
    "searchDF = w4h.start_define(df=searchDF, starterms=startTerms, printouts=True)\n",
    "downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF)\n",
    "classifedDF, searchDF = w4h.split_defined(downholeData)\n",
    "searchDF = w4h.depth_define(searchDF, thresh=550, printouts=True)\n",
    "downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF)\n",
    "downholeData = w4h.fill_unclassified(downholeData)\n",
    "#dictDir = \"\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\ISWS_HydroGeo\\\\WellDataAutoClassification\\\\SupportingDocs\\\\\"\n",
    "targetInterpDF = w4h.readLithologies()\n",
    "downholeData = w4h.merge_lithologies(downholedata=downholeData, targinterps=targetInterpDF)\n",
    "wellsDF = w4h.get_unique_wells(downholeData)\n",
    "downholeData = w4h.sort_dataframe(df=downholeData, sort_cols=['API_NUMBER','TOP'], remove_nans=True)\n",
    "inGrids = [bedrockElevGridIN, surfaceElevGridIN]\n",
    "bedrockGrid, surfaceGrid = w4h.alignRasters(unalignedGrids=inGrids, modelgrid=modelGrid)\n",
    "driftThickGrid, layerThickGrid = w4h.get_drift_thick(surface=surfaceGrid, bedrock=bedrockGrid, noLayers=9, plotData=False)\n",
    "headerData = w4h.sample_raster_points(raster=bedrockGrid, ptDF=headerData, newColName='BEDROCK_ELEV_FT')\n",
    "headerData = w4h.sample_raster_points(raster=surfaceGrid, ptDF=headerData, newColName='SURFACE_ELEV_FT')\n",
    "headerData = w4h.sample_raster_points(raster=driftThickGrid, ptDF=headerData, newColName='BEDROCK_DEPTH_FT')\n",
    "headerData = w4h.sample_raster_points(raster=layerThickGrid, ptDF=headerData, newColName='LAYER_THICK_FT')\n",
    "headerData = w4h.get_layer_depths(well_metadata=headerData, no_layers=9)\n",
    "downholeData_layerInfo = w4h.merge_tables(data_df=downholeData,  data_cols=None, header_cols=None, header_df=headerData,on='API_NUMBER', how='inner', auto_pick_cols=True)\n",
    "#downholeData = downholeData_layerInfo.copy()\n",
    "resdf = w4h.layer_target_thick(downholeData_layerInfo, layers=9, outfile_prefix='CoarseFine')\n",
    "layers_data = w4h.layer_interp(points=resdf, layers=9, grid=modelGrid, method='lin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export (rio)xarray dataarrays and datasets\n",
    "def export_grids(grid_data, out_path, file_id='',filetype='tif', variable_sep=True, date_stamp=True):\n",
    "    \"\"\"Function to export grids to files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grid_data : xarray DataArray or xarray Dataset\n",
    "        Dataset or dataarray to be exported\n",
    "    out_path : str or pathlib.Path object\n",
    "        Output location for data export. If variable_sep=True, this should be a directory. Otherwise, this should also include the filename. The file extension should not be included here.\n",
    "    filetype : str, optional\n",
    "        Output filetype. Can either be pickle or any file extension supported by rioxarray.rio.to_raster(). Can either include period or not., by default 'tif'\n",
    "    variable_sep : bool, optional\n",
    "        If grid_data is an xarray Dataset, this will export each variable in the dataset as a separate file, including the variable name in the filename, by default False\n",
    "    date_stamp : bool, optional\n",
    "        Whether to include a date stamp in the file name., by default True\n",
    "    \"\"\"\n",
    "\n",
    "    #Initialize lists to determine which filetype will be used for export\n",
    "    ncdfList = ['netcdf', 'ncdf', 'n']\n",
    "    tifList = ['tif', 'tiff', 'geotiff', 'geotif', 't']\n",
    "    pickleList = ['pickle', 'pkl', 'p']\n",
    "\n",
    "    #Format output string(s)\n",
    "    #Format output filepath\n",
    "    if type(out_path) is str or isinstance(out_path, pathlib.PurePath):\n",
    "        if isinstance(out_path, pathlib.PurePath):\n",
    "            pass\n",
    "        else:\n",
    "            out_path = pathlib.Path(out_path)\n",
    "        if out_path.parent.exists()==False:\n",
    "            print('Directory does not exist. Please enter a different value for the out_path parameter.')\n",
    "            return        \n",
    "\n",
    "        if out_path.is_dir():\n",
    "            if isinstance(grid_data, xr.DataArray):\n",
    "                if variable_sep:\n",
    "                    lyrs = grid_data.coords['Layer'].values\n",
    "                    filenames = []\n",
    "                    for l in lyrs:\n",
    "                        filenames.append('Layer'+str(l))\n",
    "                else:\n",
    "                    filenames = ['AllLayers']\n",
    "            if isinstance(grid_data, xr.Dataset):\n",
    "                if variable_sep:\n",
    "                    filenames = []\n",
    "                    for var in grid_data:\n",
    "                        filenames.append(var)\n",
    "                else:\n",
    "                    filenames = ['AllLayers']    \n",
    "        else:\n",
    "            filenames = [out_path.stem]\n",
    "            out_path = out_path.parent\n",
    "\n",
    "    else:\n",
    "        print('Please input string or pathlib object for out_path parameters')\n",
    "        return\n",
    "    \n",
    "    #Format datestamp, if desired in output filename\n",
    "    if date_stamp:\n",
    "        todayDate = datetime.date.today()\n",
    "        todayDateStr = '_'+str(todayDate)\n",
    "    else:\n",
    "        todayDateStr=''\n",
    "\n",
    "    #Ensure the file suffix includes .\n",
    "    if filetype[0] == '.':\n",
    "        pass\n",
    "    else:\n",
    "        filetype = '.' + filetype\n",
    "\n",
    "    if file_id != '':\n",
    "        file_id = '_'+file_id\n",
    "\n",
    "    out_path = out_path.as_posix()+'/'\n",
    "    outPaths = []\n",
    "    for f in filenames:\n",
    "        outPaths.append(out_path+f+file_id+todayDateStr+filetype)\n",
    "\n",
    "    #Do export\n",
    "    if filetype.lower() in pickleList:\n",
    "        import pickle\n",
    "        for op in outPaths:\n",
    "            try:\n",
    "                with open(op, 'wb') as f:\n",
    "                    pickle.dump(grid_data, f)\n",
    "            except:\n",
    "                print('An error occured during export.')\n",
    "                print(op, 'could not be exported as a pickle object.')\n",
    "                print('Try again using different parameters.')\n",
    "    else:\n",
    "        import rioxarray as rxr\n",
    "        try:\n",
    "            if isinstance(grid_data, xr.Dataset):\n",
    "                if variable_sep:\n",
    "                    for i, var in enumerate(grid_data.data_vars):\n",
    "                        grid_data[var].rio.to_raster(outPaths[i])\n",
    "                else:\n",
    "                    grid_data.rio.to_raster(outPaths[0])\n",
    "            elif isinstance(grid_data, xr.DataArray):\n",
    "                if variable_sep:\n",
    "                    lyrs = grid_data.coords['Layer'].values\n",
    "                    for i, l in enumerate(lyrs):\n",
    "                        out_grid = grid_data.sel(Layer = l).copy()\n",
    "                        out_grid.rio.to_raster(outPaths[i])\n",
    "                else:\n",
    "                    grid_data.rio.to_raster(outPaths[0])\n",
    "            else:\n",
    "                grid_data.rio.to_raster(outPaths[0])\n",
    "        except:\n",
    "            print('An error occured during export.')\n",
    "            print('{} could not be exported as {} file.'.format(outPaths, filetype))\n",
    "            print('Try again using different parameters.')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('//isgs-sinkhole.ad.uillinois.edu/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "out_dir = pathlib.Path(out_dir)\n",
    "out_dir.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "export_grids(grid_data=layers_data, out_path=out_dir, file_id='Coarse', filetype='tif', variable_sep=True, date_stamp=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "1eab39f8790fa4ef06ab4ebada9c1405c2ef16219adfedb21e90bf3fb356ecb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
