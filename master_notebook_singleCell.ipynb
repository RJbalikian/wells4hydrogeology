{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{Fill in with information about this notebook}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import numpy as np #Data manipulation\n",
    "import pandas as pd #Point data manipulation and organization\n",
    "import xarray as xr #Raster data manipulation and organization\n",
    "\n",
    "import pathlib  #For filepaths, io, etc.\n",
    "import os       #For several system-based commands\n",
    "import datetime #For manipulation of time data, including file creation/modification times\n",
    "import json     #For dictionary io, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt #For plotting and data vizualization\n",
    "import geopandas as gpd         #For organization and manipulation of vector data in space (study area and some data points)\n",
    "import rioxarray as rxr         #For orgnaization and manipulation of raster data\n",
    "from scipy import interpolate\n",
    "import shapely                  #For converting coordinates to point geometry\n",
    "#Not sure if this cell is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scripts with functions made for this specific application\n",
    "import w4h\n",
    "import pathlib\n",
    "import os\n",
    "#Variables needed throughout, best to just assign now\n",
    "todayDate, dateSuffix = w4h.get_current_date() \n",
    "repoDir = pathlib.Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging data to C:\\Users\\riley\\Desktop\\log_2023-06-03_13-22-04.txt\n",
      "Most Recent version of this file is : ISGS_DOWNHOLE_DATA_2023-06-03.txt\n",
      "Most Recent version of this file is : ISGS_HEADER_2023-06-03.txt\n",
      "Most Recent version of this file is : xyzData.csv\n",
      "Using the following files:\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\ISGS_DOWNHOLE_DATA_2023-06-03.txt\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\ISGS_HEADER_2023-06-03.txt\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\xyzData.csv\n"
     ]
    }
   ],
   "source": [
    "directoryDir = r'\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\\\'[:-1]\n",
    "downholeDataPATH, headerDataPATH  = w4h.file_setup(db_dir=directoryDir, log_dir=r'C:\\Users\\riley\\Desktop', verbose=True, log=True)\n",
    "downholeDataIN, headerDataIN = w4h.read_raw_txt(data_filepath=downholeDataPATH, metadata_filepath=headerDataPATH, log=True) #Functions to read data into dataframes. Also excludes extraneous columns, and drops header data with no location information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Recent version of this file is : ISGS_DOWNHOLE_DATA_2023-06-03.txt\n",
      "Most Recent version of this file is : ISGS_HEADER_2023-06-03.txt\n",
      "Using the following files:\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\WellData\\RawWellData_OracleDatabase\\TxtData\\ISGS_DOWNHOLE_DATA_2023-06-03.txt\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\WellData\\RawWellData_OracleDatabase\\TxtData\\ISGS_HEADER_2023-06-03.txt\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m targetPath \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mriley\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLocalData\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mGithub\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mwells4hydrogeology\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mresources\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLithology_Interp_FineCoarse.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m procDir \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39misgs-sinkhole.ad.uillinois.edu\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mgeophysics\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mBalikian\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mISWS_HydroGeo\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mWellDataAutoClassification\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mProcessedData\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mrunTest\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m w4h\u001b[39m.\u001b[39;49mrun(well_data\u001b[39m=\u001b[39;49mdataDir,\n\u001b[0;32m     19\u001b[0m         well_data_cols\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, \n\u001b[0;32m     20\u001b[0m         well_metadata\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, \n\u001b[0;32m     21\u001b[0m         well_metadata_cols\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, \n\u001b[0;32m     22\u001b[0m         description_col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mFORMATION\u001b[39;49m\u001b[39m'\u001b[39;49m, top_col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTOP\u001b[39;49m\u001b[39m'\u001b[39;49m, bottom_col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mBOTTOM\u001b[39;49m\u001b[39m'\u001b[39;49m, depth_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdepth\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     23\u001b[0m         xcol\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLONGITUDE\u001b[39;49m\u001b[39m'\u001b[39;49m, ycol\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLATITUDE\u001b[39;49m\u001b[39m'\u001b[39;49m, zcol\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mELEVATION\u001b[39;49m\u001b[39m'\u001b[39;49m, idcol\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mAPI_NUMBER\u001b[39;49m\u001b[39m'\u001b[39;49m, output_crs\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mEPSG:4269\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     24\u001b[0m         surf_elev_file\u001b[39m=\u001b[39;49msurfaceElevPath, bedrock_elev_file\u001b[39m=\u001b[39;49mbedrockElevPath, model_grid\u001b[39m=\u001b[39;49mmodelGridPath,\n\u001b[0;32m     25\u001b[0m         lith_dict\u001b[39m=\u001b[39;49moldDictPath, lith_dict_start\u001b[39m=\u001b[39;49mstartDictPath, lith_dict_wildcard\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     26\u001b[0m         target_dict\u001b[39m=\u001b[39;49mtargetPath,\n\u001b[0;32m     27\u001b[0m         study_area\u001b[39m=\u001b[39;49mstudyAreaPath,\n\u001b[0;32m     28\u001b[0m         export_dir\u001b[39m=\u001b[39;49mprocDir,\n\u001b[0;32m     29\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     30\u001b[0m         log\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology\\w4h\\utilities.py:195\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(well_data, well_data_cols, well_metadata, well_metadata_cols, description_col, top_col, bottom_col, depth_type, xcol, ycol, zcol, idcol, output_crs, surf_elev_file, bedrock_elev_file, model_grid, lith_dict, lith_dict_start, lith_dict_wildcard, target_dict, study_area, export_dir, verbose, log, **parameters)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[39m#Get pandas dataframes from input\u001b[39;00m\n\u001b[0;32m    194\u001b[0m read_raw_txt_kwargs \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mlocals\u001b[39m()[\u001b[39m'\u001b[39m\u001b[39mparameters\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m w4h\u001b[39m.\u001b[39mread_raw_txt\u001b[39m.\u001b[39m\u001b[39m__code__\u001b[39m\u001b[39m.\u001b[39mco_varnames}\n\u001b[1;32m--> 195\u001b[0m downholeDataIN, headerDataIN \u001b[39m=\u001b[39m w4h\u001b[39m.\u001b[39mread_raw_txt(data_filepath\u001b[39m=\u001b[39mdownholeDataPATH, metadata_filepath\u001b[39m=\u001b[39mheaderDataPATH, verbose\u001b[39m=\u001b[39mverbose, log\u001b[39m=\u001b[39mlog, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mread_raw_txt_kwargs) \u001b[39m#Functions to read data into dataframes. Also excludes extraneous columns, and drops header data with no location information\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m#Define data types (file will need to be udpated)\u001b[39;00m\n\u001b[0;32m    198\u001b[0m downholeData \u001b[39m=\u001b[39m w4h\u001b[39m.\u001b[39mdefine_dtypes(df\u001b[39m=\u001b[39mdownholeDataIN, dtype_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdownholeDataTypes.txt\u001b[39m\u001b[39m'\u001b[39m, log\u001b[39m=\u001b[39mlog)\n",
      "File \u001b[1;32mc:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology\\w4h\\read.py:191\u001b[0m, in \u001b[0;36mread_raw_txt\u001b[1;34m(data_filepath, metadata_filepath, data_cols, metadata_cols, xcol, ycol, id_col, encoding, verbose, log)\u001b[0m\n\u001b[0;32m    189\u001b[0m     downholeDataIN \u001b[39m=\u001b[39m data_filepath[data_useCols]\n\u001b[0;32m    190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m     downholeDataIN \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(data_filepath, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39minfer\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49mencoding, usecols\u001b[39m=\u001b[39;49mdata_useCols)\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(metadata_filepath, pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m    194\u001b[0m     headerDataIN \u001b[39m=\u001b[39m metadata_filepath[metadata_useCols]\n",
      "File \u001b[1;32mc:\\Users\\riley\\LocalData\\virtual_envs\\w4h_venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\riley\\LocalData\\virtual_envs\\w4h_venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\riley\\LocalData\\virtual_envs\\w4h_venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1705\u001b[0m         nrows\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1707\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\riley\\LocalData\\virtual_envs\\w4h_venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\riley\\LocalData\\virtual_envs\\w4h_venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\riley\\LocalData\\virtual_envs\\w4h_venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:873\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\riley\\LocalData\\virtual_envs\\w4h_venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:848\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\riley\\LocalData\\virtual_envs\\w4h_venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:859\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\riley\\LocalData\\virtual_envs\\w4h_venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:2025\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import w4h\n",
    "\n",
    "dataDir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\WellData\\RawWellData_OracleDatabase\\TxtData\"\n",
    "\n",
    "modelGridPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\grid_625_raster.tif\"\n",
    "surfaceElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ILStateLidar_ClipExtentESL.tif\"\n",
    "bedrockElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESLBedrock.tif\"\n",
    "\n",
    "studyAreaPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESL_StudyArea_5mi.shp\"\n",
    "\n",
    "oldDictPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\WellData\\Dictionaries\\DICTIONARY_Updated-06-2018.csv\"\n",
    "startDictPath = r\"C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology\\resources\\SearchTerms-Start.csv\"\n",
    "\n",
    "targetPath = r\"C:\\Users\\riley\\LocalData\\Code\\Github\\wells4hydrogeology\\resources\\Lithology_Interp_FineCoarse.csv\"\n",
    "\n",
    "procDir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\\runTest\"\n",
    "\n",
    "w4h.run(well_data=dataDir,\n",
    "        well_data_cols=None, \n",
    "        well_metadata=None, \n",
    "        well_metadata_cols=None, \n",
    "        description_col='FORMATION', top_col='TOP', bottom_col='BOTTOM', depth_type='depth',\n",
    "        xcol='LONGITUDE', ycol='LATITUDE', zcol='ELEVATION', idcol='API_NUMBER', output_crs='EPSG:4269',\n",
    "        surf_elev_file=surfaceElevPath, bedrock_elev_file=bedrockElevPath, model_grid=modelGridPath,\n",
    "        lith_dict=oldDictPath, lith_dict_start=startDictPath, lith_dict_wildcard=None,\n",
    "        target_dict=targetPath,\n",
    "        study_area=studyAreaPath,\n",
    "        export_dir=procDir,\n",
    "        verbose=True,\n",
    "        log=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "test = pathlib.Path()\n",
    "test.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\.conda\\envs\\raster38\\lib\\site-packages\\geopandas\\array.py:275: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  return GeometryArray(vectorized.points_from_xy(x, y, z), crs=crs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well records removed: 0\n",
      "Number of rows before dropping those without surface elevation information: 8150\n",
      "Number of rows after dropping those without surface elevation information: 8150\n",
      "Number of rows before dropping those without record depth information: 56331\n",
      "Number of rows after dropping those without record depth information: 55747\n",
      "Number of well records without formation information deleted: 584\n",
      "Number of rows before dropping those with obviously bad depth information: 56331\n",
      "Number of rows after dropping those with obviously bad depth information: 55725\n",
      "Well records deleted: 606\n",
      "Number of rows before dropping those without FORMATION information: 56331\n",
      "Number of rows after dropping those without FORMATION information: 56331\n",
      "Well records deleted: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m downholeData \u001b[39m=\u001b[39m w4h\u001b[39m.\u001b[39mdrop_no_formation(downholeData, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[39m#downholeData.reset_index(inplace=True,drop=True) #These may not be necessary\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m#headerData.reset_index(inplace=True,drop=True) #These may not be necessary\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m downholeData \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(left \u001b[39m=\u001b[39m downholeData, right \u001b[39m=\u001b[39m headerData, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAPI_NUMBER\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "xyzDataIN = w4h.read_xyz(xyzpath=xyzInPATH, log=True)\n",
    "downholeData = w4h.define_dtypes(df=downholeDataIN, dtype_file='downholeDataTypes.txt', log=True) #Define datatypes of each column of the new dataframes\n",
    "headerData = w4h.define_dtypes(df=headerDataIN, dtype_file='headerDataTypes.txt', log=True)#Define datatypes of each column of the new dataframes\n",
    "xyzData = w4h.define_dtypes(df=xyzDataIN, dtype_file='xyzDataTypes.txt', log=True)\n",
    "studyAreaPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESL_StudyArea_5mi.shp\"\n",
    "studyAreaIN = w4h.read_study_area(studyareapath=studyAreaPath, log=True)\n",
    "modelGridPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\grid_625_raster.tif\"\n",
    "surfaceElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ILStateLidar_ClipExtentESL.tif\"\n",
    "bedrockElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESLBedrock.tif\"\n",
    "modelGrid = w4h.read_grid(datapath=modelGridPath, grid_type='model', study_area=studyAreaIN,  read_grid=True, clip_to_studyarea=True, log=True)#, gridcrs='EPSG:26715', studyAreacrs='EPSG:26715')\n",
    "surfaceElevGridIN = w4h.read_grid(datapath=surfaceElevPath, grid_type='surface', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "bedrockElevGridIN = w4h.read_grid(datapath=bedrockElevPath, grid_type='bedrock', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "#Code here for adding in control points\n",
    "headerData = w4h.xyz_metadata_merge(xyz=xyzData, metadata=headerData, log=True) #This probably needs to be updated\n",
    "headerData = w4h.coords2geometry(df=headerData, xcol='LONGITUDE', ycol='LATITUDE', crs='EPSG:4269', log=True)\n",
    "headerData = w4h.clip_gdf2study_area(study_area=studyAreaIN, gdf=headerData, gdf_crs='EPSG:4269', log=True)\n",
    "downholeData = w4h.remove_nonlocated(downholeData, headerData, log=True)\n",
    "headerData = w4h.remove_no_topo(df=headerData, verbose=True, log=True)\n",
    "donwholeData = w4h.drop_no_depth(downholeData, verbose=True, log=True) #Drop records with no depth information\n",
    "donwholeData = w4h.drop_bad_depth(downholeData, verbose=True, log=True)#Drop records with bad depth information (i.e., top depth > bottom depth) (Also calculates thickness of each record)\n",
    "downholeData = w4h.drop_no_formation(downholeData, verbose=True, log=True)\n",
    "#downholeData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "#headerData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "downholeData = pd.merge(left = downholeData, right = headerData, on='API_NUMBER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Recent version of this file is : SearchTerms-Specific_2022-11-16_essCols.csv\n",
      "Most Recent version of this file is : SearchTerms-Start.csv\n",
      "Records Classified with full search term: 31582\n",
      "Records Classified with full search term: 56.07% of data\n",
      "Start Term process should be done by 13:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\LocalData\\Github\\wells4hydrogeology\\w4h\\classify.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['CLASS_FLAG'].where(~df[description_col].str.startswith(s,na=False),4,inplace=True)\n",
      "c:\\Users\\riley\\LocalData\\Github\\wells4hydrogeology\\w4h\\classify.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['INTERPRETATION'].where(~df[description_col].str.startswith(s,na=False),terms_df.loc[i,'INTERPRETATION'],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records classified with start search term: 3586\n",
      "Records classified with start search term: 14.49% of remaining data\n",
      "Records classified as bedrock that were deeper than 550': 359\n",
      "This represents 1.7% of the unclassified data in this dataframe.\n",
      "BEDROCK_ELEV_FT sampling should be done by 13:51\n",
      "SURFACE_ELEV_FT sampling should be done by 13:51\n",
      "BEDROCK_DEPTH_FT sampling should be done by 13:51\n",
      "LAYER_THICK_FT sampling should be done by 13:51\n",
      "REMOVING LATITUDE\n",
      "REMOVING LONGITUDE\n",
      "REMOVING ELEV_FT\n",
      "REMOVING geometry\n",
      "Completed interpolation for Layer 1\n",
      "Completed interpolation for Layer 2\n",
      "Completed interpolation for Layer 3\n",
      "Completed interpolation for Layer 4\n",
      "Completed interpolation for Layer 5\n",
      "Completed interpolation for Layer 6\n",
      "Completed interpolation for Layer 7\n",
      "Completed interpolation for Layer 8\n",
      "Completed interpolation for Layer 9\n"
     ]
    }
   ],
   "source": [
    "specTermsPATH, startTermsPATH = w4h.get_search_terms(spec_dir=str(repoDir)+'/resources/', spec_glob_pattern='*SearchTerms-Specific*', start_glob_pattern='*SearchTerms-Start*', log=True)\n",
    "specTerms = w4h.read_dictionary_terms(dict_file=specTermsPATH, log=True)\n",
    "startTerms = w4h.read_dictionary_terms(dict_file=startTermsPATH, log=True)\n",
    "oldDictPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\WellData\\Dictionaries\\DICTIONARY_Updated-06-2018.csv\"\n",
    "oldDict = w4h.read_dictionary_terms(dict_file=oldDictPath, cols={'DESCRIPTION':'FORMATION', 'LITHOLOGY':'INTERPRETATION'}, class_flag=1, log=True)\n",
    "\n",
    "specTerms = pd.concat([specTerms, oldDict])\n",
    "specTerms.drop_duplicates(subset='FORMATION', inplace=True)\n",
    "specTerms.reset_index(inplace=True, drop=True)\n",
    "downholeData = w4h.specific_define(downholeData, specTerms, verbose=True, log=True)\n",
    "classifedDF, searchDF = w4h.split_defined(downholeData)\n",
    "searchDF = w4h.start_define(df=searchDF, terms_df=startTerms, verbose=True)\n",
    "downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF)\n",
    "classifedDF, searchDF = w4h.split_defined(downholeData)\n",
    "searchDF = w4h.depth_define(searchDF, thresh=550, verbose=True, log=True)\n",
    "downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF)\n",
    "downholeData = w4h.fill_unclassified(downholeData)\n",
    "#dictDir = \"\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\ISWS_HydroGeo\\\\WellDataAutoClassification\\\\SupportingDocs\\\\\"\n",
    "targetInterpDF = w4h.read_lithologies(log=True)\n",
    "downholeData = w4h.merge_lithologies(df=downholeData, targinterps_df=targetInterpDF)\n",
    "wellsDF = w4h.get_unique_wells(downholeData)\n",
    "downholeData = w4h.sort_dataframe(df=downholeData, sort_cols=['API_NUMBER','TOP'], remove_nans=True)\n",
    "inGrids = [bedrockElevGridIN, surfaceElevGridIN]\n",
    "bedrockGrid, surfaceGrid = w4h.align_rasters(grids_unaligned=inGrids, modelgrid=modelGrid, log=True)\n",
    "driftThickGrid, layerThickGrid = w4h.get_drift_thick(surface=surfaceGrid, bedrock=bedrockGrid, layers=9, plot=False, log=True)\n",
    "headerData = w4h.sample_raster_points(raster=bedrockGrid, points_df=headerData, new_col='BEDROCK_ELEV_FT', log=True)\n",
    "headerData = w4h.sample_raster_points(raster=surfaceGrid, points_df=headerData, new_col='SURFACE_ELEV_FT', log=True)\n",
    "headerData = w4h.sample_raster_points(raster=driftThickGrid, points_df=headerData, new_col='BEDROCK_DEPTH_FT', log=True)\n",
    "headerData = w4h.sample_raster_points(raster=layerThickGrid, points_df=headerData, new_col='LAYER_THICK_FT', log=True)\n",
    "headerData = w4h.get_layer_depths(well_metadata=headerData, no_layers=9, log=True)\n",
    "downholeData_layerInfo = w4h.merge_tables(data_df=downholeData,  data_cols=None, header_cols=None, header_df=headerData,on='API_NUMBER', how='inner', auto_pick_cols=True, log=True)\n",
    "#downholeData = downholeData_layerInfo.copy()\n",
    "resdf = w4h.layer_target_thick(downholeData_layerInfo, layers=9, outfile_prefix='CoarseFine', log=True)\n",
    "layers_data = w4h.layer_interp(points=resdf, layers=9, grid=modelGrid, method='lin', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING LATITUDE\n",
      "REMOVING LONGITUDE\n"
     ]
    }
   ],
   "source": [
    "header_cols = ['API_NUMBER', 'LATITUDE', 'LONGITUDE', 'BEDROCK_ELEV_FT', 'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT']\n",
    "data_cols = downholeData.columns\n",
    "on='API_NUMBER'\n",
    "#Drop duplicate columns\n",
    "drop_duplicate_cols=True\n",
    "if drop_duplicate_cols:\n",
    "    header_colCopy= header_cols.copy()\n",
    "    remCount = 0\n",
    "    for i, c in enumerate(header_colCopy):\n",
    "        if c in data_cols and c != on:\n",
    "            print('REMOVING', header_cols[i-remCount])\n",
    "            header_cols.pop(i - remCount)\n",
    "            remCount += 1\n",
    "\n",
    "downholeData_layerInfo = pd.merge(left=downholeData, right=headerData,on='API_NUMBER', how='inner',)\n",
    "#downholeData_layerInfo = w4h.merge_tables(data_df=downholeData,  data_cols=None, header_cols=None, header_df=headerData,on='API_NUMBER', how='inner', auto_pick_cols=True, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['API_NUMBER', 'TABLE_NAME', 'FORMATION', 'THICKNESS', 'TOP', 'BOTTOM',\n",
       "       'TOTAL_DEPTH_x', 'SECTION_x', 'TWP_x', 'TDIR_x', 'RNG_x', 'RDIR_x',\n",
       "       'MERIDIAN_x', 'QUARTERS_x', 'ELEVATION_x', 'ELEVREF_x', 'COUNTY_CODE_x',\n",
       "       'ELEVSOURCE_x', 'LATITUDE_x', 'LONGITUDE_x', 'ELEV_FT_x', 'geometry_x',\n",
       "       'INTERPRETATION', 'CLASS_FLAG', 'BEDROCK_FLAG', 'TARGET',\n",
       "       'TOTAL_DEPTH_y', 'SECTION_y', 'TWP_y', 'TDIR_y', 'RNG_y', 'RDIR_y',\n",
       "       'MERIDIAN_y', 'QUARTERS_y', 'ELEVATION_y', 'ELEVREF_y', 'COUNTY_CODE_y',\n",
       "       'ELEVSOURCE_y', 'LATITUDE', 'LONGITUDE', 'ELEV_FT_y', 'geometry',\n",
       "       'LONGITUDE_PROJ', 'LATITUDE_PROJ', 'BEDROCK_ELEV_FT', 'SURFACE_ELEV_FT',\n",
       "       'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT', 'DEPTH_FT_LAYER1',\n",
       "       'DEPTH_FT_LAYER2', 'DEPTH_FT_LAYER3', 'DEPTH_FT_LAYER4',\n",
       "       'DEPTH_FT_LAYER5', 'DEPTH_FT_LAYER6', 'DEPTH_FT_LAYER7',\n",
       "       'DEPTH_FT_LAYER8', 'DEPTH_FT_LAYER9', 'ELEV_FT_LAYER1',\n",
       "       'ELEV_FT_LAYER2', 'ELEV_FT_LAYER3', 'ELEV_FT_LAYER4', 'ELEV_FT_LAYER5',\n",
       "       'ELEV_FT_LAYER6', 'ELEV_FT_LAYER7', 'ELEV_FT_LAYER8', 'ELEV_FT_LAYER9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downholeData_layerInfo=downholeData_layerInfo.rename(columns={'LATITUDE_y':\"LATITUDE\",'LONGITUDE_y':\"LONGITUDE\", 'geometry_y':'geometry'})\n",
    "downholeData_layerInfo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf = w4h.layer_target_thick(downholeData_layerInfo, layers=9, outfile_prefix='CoarseFine', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed interpolation for Layer 1\n",
      "Completed interpolation for Layer 2\n",
      "Completed interpolation for Layer 3\n",
      "Completed interpolation for Layer 4\n",
      "Completed interpolation for Layer 5\n",
      "Completed interpolation for Layer 6\n",
      "Completed interpolation for Layer 7\n",
      "Completed interpolation for Layer 8\n",
      "Completed interpolation for Layer 9\n"
     ]
    }
   ],
   "source": [
    "layers_data = w4h.layer_interp(points=resdf, layers=9, grid=modelGrid, method='nn', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir=r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "w4h.export_grids(grid_data=layers_data, out_path=out_dir, file_id='',filetype='tif', variable_sep=True, date_stamp=True, log=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export (rio)xarray dataarrays and datasets\n",
    "def export_grids(grid_data, out_path, file_id='',filetype='tif', variable_sep=True, date_stamp=True):\n",
    "    \"\"\"Function to export grids to files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grid_data : xarray DataArray or xarray Dataset\n",
    "        Dataset or dataarray to be exported\n",
    "    out_path : str or pathlib.Path object\n",
    "        Output location for data export. If variable_sep=True, this should be a directory. Otherwise, this should also include the filename. The file extension should not be included here.\n",
    "    filetype : str, optional\n",
    "        Output filetype. Can either be pickle or any file extension supported by rioxarray.rio.to_raster(). Can either include period or not., by default 'tif'\n",
    "    variable_sep : bool, optional\n",
    "        If grid_data is an xarray Dataset, this will export each variable in the dataset as a separate file, including the variable name in the filename, by default False\n",
    "    date_stamp : bool, optional\n",
    "        Whether to include a date stamp in the file name., by default True\n",
    "    \"\"\"\n",
    "\n",
    "    #Initialize lists to determine which filetype will be used for export\n",
    "    ncdfList = ['netcdf', 'ncdf', 'n']\n",
    "    tifList = ['tif', 'tiff', 'geotiff', 'geotif', 't']\n",
    "    pickleList = ['pickle', 'pkl', 'p']\n",
    "\n",
    "    #Format output string(s)\n",
    "    #Format output filepath\n",
    "    if type(out_path) is str or isinstance(out_path, pathlib.PurePath):\n",
    "        if isinstance(out_path, pathlib.PurePath):\n",
    "            pass\n",
    "        else:\n",
    "            out_path = pathlib.Path(out_path)\n",
    "        if out_path.parent.exists()==False:\n",
    "            print('Directory does not exist. Please enter a different value for the out_path parameter.')\n",
    "            return        \n",
    "\n",
    "        if out_path.is_dir():\n",
    "            if isinstance(grid_data, xr.DataArray):\n",
    "                if variable_sep:\n",
    "                    lyrs = grid_data.coords['Layer'].values\n",
    "                    filenames = []\n",
    "                    for l in lyrs:\n",
    "                        filenames.append('Layer'+str(l))\n",
    "                else:\n",
    "                    filenames = ['AllLayers']\n",
    "            if isinstance(grid_data, xr.Dataset):\n",
    "                if variable_sep:\n",
    "                    filenames = []\n",
    "                    for var in grid_data:\n",
    "                        filenames.append(var)\n",
    "                else:\n",
    "                    filenames = ['AllLayers']    \n",
    "        else:\n",
    "            filenames = [out_path.stem]\n",
    "            out_path = out_path.parent\n",
    "\n",
    "    else:\n",
    "        print('Please input string or pathlib object for out_path parameters')\n",
    "        return\n",
    "    \n",
    "    #Format datestamp, if desired in output filename\n",
    "    if date_stamp:\n",
    "        todayDate = datetime.date.today()\n",
    "        todayDateStr = '_'+str(todayDate)\n",
    "    else:\n",
    "        todayDateStr=''\n",
    "\n",
    "    #Ensure the file suffix includes .\n",
    "    if filetype[0] == '.':\n",
    "        pass\n",
    "    else:\n",
    "        filetype = '.' + filetype\n",
    "\n",
    "    if file_id != '':\n",
    "        file_id = '_'+file_id\n",
    "\n",
    "    out_path = out_path.as_posix()+'/'\n",
    "    outPaths = []\n",
    "    for f in filenames:\n",
    "        outPaths.append(out_path+f+file_id+todayDateStr+filetype)\n",
    "\n",
    "    #Do export\n",
    "    if filetype.lower() in pickleList:\n",
    "        import pickle\n",
    "        for op in outPaths:\n",
    "            try:\n",
    "                with open(op, 'wb') as f:\n",
    "                    pickle.dump(grid_data, f)\n",
    "            except:\n",
    "                print('An error occured during export.')\n",
    "                print(op, 'could not be exported as a pickle object.')\n",
    "                print('Try again using different parameters.')\n",
    "    else:\n",
    "        import rioxarray as rxr\n",
    "        try:\n",
    "            if isinstance(grid_data, xr.Dataset):\n",
    "                if variable_sep:\n",
    "                    for i, var in enumerate(grid_data.data_vars):\n",
    "                        grid_data[var].rio.to_raster(outPaths[i])\n",
    "                else:\n",
    "                    grid_data.rio.to_raster(outPaths[0])\n",
    "            elif isinstance(grid_data, xr.DataArray):\n",
    "                if variable_sep:\n",
    "                    lyrs = grid_data.coords['Layer'].values\n",
    "                    for i, l in enumerate(lyrs):\n",
    "                        out_grid = grid_data.sel(Layer = l).copy()\n",
    "                        out_grid.rio.to_raster(outPaths[i])\n",
    "                else:\n",
    "                    grid_data.rio.to_raster(outPaths[0])\n",
    "            else:\n",
    "                grid_data.rio.to_raster(outPaths[0])\n",
    "        except:\n",
    "            print('An error occured during export.')\n",
    "            print('{} could not be exported as {} file.'.format(outPaths, filetype))\n",
    "            print('Try again using different parameters.')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('//isgs-sinkhole.ad.uillinois.edu/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "out_dir = pathlib.Path(out_dir)\n",
    "out_dir.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "export_grids(grid_data=layers_data, out_path=out_dir, file_id='Coarse', filetype='tif', variable_sep=True, date_stamp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyzDataIN = w4h.read_xyz(xyzpath=xyzInPATH, log=True)\n",
    "downholeData = w4h.define_dtypes(df=downholeDataIN, dtype_file='downholeDataTypes.txt', log=True) #Define datatypes of each column of the new dataframes\n",
    "headerData = w4h.define_dtypes(df=headerDataIN, dtype_file='headerDataTypes.txt', log=True)#Define datatypes of each column of the new dataframes\n",
    "xyzData = w4h.define_dtypes(df=xyzDataIN, dtype_file='xyzDataTypes.txt', log=True)\n",
    "studyAreaPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESL_StudyArea_5mi.shp\"\n",
    "studyAreaIN = w4h.read_study_area(studyareapath=studyAreaPath, log=True)\n",
    "modelGridPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\grid_625_raster.tif\"\n",
    "surfaceElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ILStateLidar_ClipExtentESL.tif\"\n",
    "bedrockElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESLBedrock.tif\"\n",
    "modelGrid = w4h.read_grid(datapath=modelGridPath, grid_type='model', study_area=studyAreaIN,  read_grid=True, clip_to_studyarea=True, log=True)#, gridcrs='EPSG:26715', studyAreacrs='EPSG:26715')\n",
    "surfaceElevGridIN = w4h.read_grid(datapath=surfaceElevPath, grid_type='surface', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "bedrockElevGridIN = w4h.read_grid(datapath=bedrockElevPath, grid_type='bedrock', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "#Code here for adding in control points\n",
    "headerData = w4h.xyz_metadata_merge(xyz=xyzData, metadata=headerData, log=True) #This probably needs to be updated\n",
    "headerData = w4h.coords2geometry(df=headerData, xcol='LONGITUDE', ycol='LATITUDE', crs='EPSG:4269', log=True)\n",
    "headerData = w4h.clip_gdf2study_area(study_area=studyAreaIN, gdf=headerData, gdf_crs='EPSG:4269', log=True)\n",
    "downholeData = w4h.remove_nonlocated(downholeData, headerData, log=True)\n",
    "headerData = w4h.remove_no_topo(df=headerData, verbose=True, log=True)\n",
    "donwholeData = w4h.drop_no_depth(downholeData, verbose=True, log=True) #Drop records with no depth information\n",
    "donwholeData = w4h.drop_bad_depth(downholeData, verbose=True, log=True)#Drop records with bad depth information (i.e., top depth > bottom depth) (Also calculates thickness of each record)\n",
    "downholeData = w4h.drop_no_formation(downholeData, verbose=True, log=True)\n",
    "#downholeData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "#headerData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "downholeData = pd.merge(left = downholeData, right = headerData, on='API_NUMBER')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "1eab39f8790fa4ef06ab4ebada9c1405c2ef16219adfedb21e90bf3fb356ecb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
