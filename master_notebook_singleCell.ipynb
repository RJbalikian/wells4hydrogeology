{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{Fill in with information about this notebook}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import numpy as np #Data manipulation\n",
    "import pandas as pd #Point data manipulation and organization\n",
    "import xarray as xr #Raster data manipulation and organization\n",
    "\n",
    "import pathlib  #For filepaths, io, etc.\n",
    "import os       #For several system-based commands\n",
    "import datetime #For manipulation of time data, including file creation/modification times\n",
    "import json     #For dictionary io, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt #For plotting and data vizualization\n",
    "import geopandas as gpd         #For organization and manipulation of vector data in space (study area and some data points)\n",
    "import rioxarray as rxr         #For orgnaization and manipulation of raster data\n",
    "from scipy import interpolate\n",
    "import shapely                  #For converting coordinates to point geometry\n",
    "#Not sure if this cell is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scripts with functions made for this specific application\n",
    "import w4h\n",
    "import pathlib\n",
    "import os\n",
    "#Variables needed throughout, best to just assign now\n",
    "todayDate, dateSuffix = w4h.get_current_date() \n",
    "repoDir = pathlib.Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging data to C:\\Users\\riley\\Desktop\\log_2023-06-03_13-22-04.txt\n",
      "Most Recent version of this file is : ISGS_DOWNHOLE_DATA_2023-06-03.txt\n",
      "Most Recent version of this file is : ISGS_HEADER_2023-06-03.txt\n",
      "Most Recent version of this file is : xyzData.csv\n",
      "Using the following files:\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\ISGS_DOWNHOLE_DATA_2023-06-03.txt\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\ISGS_HEADER_2023-06-03.txt\n",
      "\t \\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\xyzData.csv\n"
     ]
    }
   ],
   "source": [
    "directoryDir = r'\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\\\'[:-1]\n",
    "downholeDataPATH, headerDataPATH, xyzInPATH  = w4h.file_setup(db_dir=directoryDir, log_dir=r'C:\\Users\\riley\\Desktop', verbose=True, log=True)\n",
    "downholeDataIN, headerDataIN = w4h.read_raw_txt(data_filepath=downholeDataPATH, metadata_filepath=headerDataPATH, log=True) #Functions to read data into dataframes. Also excludes extraneous columns, and drops header data with no location information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\.conda\\envs\\raster38\\lib\\site-packages\\geopandas\\array.py:275: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  return GeometryArray(vectorized.points_from_xy(x, y, z), crs=crs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well records removed: 0\n",
      "Number of rows before dropping those without surface elevation information: 8150\n",
      "Number of rows after dropping those without surface elevation information: 8150\n",
      "Number of rows before dropping those without record depth information: 56331\n",
      "Number of rows after dropping those without record depth information: 55747\n",
      "Number of well records without formation information deleted: 584\n",
      "Number of rows before dropping those with obviously bad depth information: 56331\n",
      "Number of rows after dropping those with obviously bad depth information: 55725\n",
      "Well records deleted: 606\n",
      "Number of rows before dropping those without FORMATION information: 56331\n",
      "Number of rows after dropping those without FORMATION information: 56331\n",
      "Well records deleted: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m downholeData \u001b[39m=\u001b[39m w4h\u001b[39m.\u001b[39mdrop_no_formation(downholeData, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[39m#downholeData.reset_index(inplace=True,drop=True) #These may not be necessary\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m#headerData.reset_index(inplace=True,drop=True) #These may not be necessary\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m downholeData \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(left \u001b[39m=\u001b[39m downholeData, right \u001b[39m=\u001b[39m headerData, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAPI_NUMBER\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "xyzDataIN = w4h.read_xyz(xyzpath=xyzInPATH, log=True)\n",
    "downholeData = w4h.define_dtypes(df=downholeDataIN, dtype_file='downholeDataTypes.txt', log=True) #Define datatypes of each column of the new dataframes\n",
    "headerData = w4h.define_dtypes(df=headerDataIN, dtype_file='headerDataTypes.txt', log=True)#Define datatypes of each column of the new dataframes\n",
    "xyzData = w4h.define_dtypes(df=xyzDataIN, dtype_file='xyzDataTypes.txt', log=True)\n",
    "studyAreaPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESL_StudyArea_5mi.shp\"\n",
    "studyAreaIN = w4h.read_study_area(studyareapath=studyAreaPath, log=True)\n",
    "modelGridPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\grid_625_raster.tif\"\n",
    "surfaceElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ILStateLidar_ClipExtentESL.tif\"\n",
    "bedrockElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESLBedrock.tif\"\n",
    "modelGrid = w4h.read_grid(datapath=modelGridPath, grid_type='model', study_area=studyAreaIN,  read_grid=True, clip_to_studyarea=True, log=True)#, gridcrs='EPSG:26715', studyAreacrs='EPSG:26715')\n",
    "surfaceElevGridIN = w4h.read_grid(datapath=surfaceElevPath, grid_type='surface', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "bedrockElevGridIN = w4h.read_grid(datapath=bedrockElevPath, grid_type='bedrock', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "#Code here for adding in control points\n",
    "headerData = w4h.xyz_metadata_merge(xyz=xyzData, metadata=headerData, log=True) #This probably needs to be updated\n",
    "headerData = w4h.coords2geometry(df=headerData, xcol='LONGITUDE', ycol='LATITUDE', crs='EPSG:4269', log=True)\n",
    "headerData = w4h.clip_gdf2study_area(study_area=studyAreaIN, gdf=headerData, gdf_crs='EPSG:4269', log=True)\n",
    "downholeData = w4h.remove_nonlocated(downholeData, headerData, log=True)\n",
    "headerData = w4h.remove_no_topo(df=headerData, verbose=True, log=True)\n",
    "donwholeData = w4h.drop_no_depth(downholeData, verbose=True, log=True) #Drop records with no depth information\n",
    "donwholeData = w4h.drop_bad_depth(downholeData, verbose=True, log=True)#Drop records with bad depth information (i.e., top depth > bottom depth) (Also calculates thickness of each record)\n",
    "downholeData = w4h.drop_no_formation(downholeData, verbose=True, log=True)\n",
    "#downholeData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "#headerData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "downholeData = pd.merge(left = downholeData, right = headerData, on='API_NUMBER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Recent version of this file is : SearchTerms-Specific_2022-11-16_essCols.csv\n",
      "Most Recent version of this file is : SearchTerms-Start.csv\n",
      "Records Classified with full search term: 31582\n",
      "Records Classified with full search term: 56.07% of data\n",
      "Start Term process should be done by 13:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\LocalData\\Github\\wells4hydrogeology\\w4h\\classify.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['CLASS_FLAG'].where(~df[description_col].str.startswith(s,na=False),4,inplace=True)\n",
      "c:\\Users\\riley\\LocalData\\Github\\wells4hydrogeology\\w4h\\classify.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['INTERPRETATION'].where(~df[description_col].str.startswith(s,na=False),terms_df.loc[i,'INTERPRETATION'],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records classified with start search term: 3586\n",
      "Records classified with start search term: 14.49% of remaining data\n",
      "Records classified as bedrock that were deeper than 550': 359\n",
      "This represents 1.7% of the unclassified data in this dataframe.\n",
      "BEDROCK_ELEV_FT sampling should be done by 13:51\n",
      "SURFACE_ELEV_FT sampling should be done by 13:51\n",
      "BEDROCK_DEPTH_FT sampling should be done by 13:51\n",
      "LAYER_THICK_FT sampling should be done by 13:51\n",
      "REMOVING LATITUDE\n",
      "REMOVING LONGITUDE\n",
      "REMOVING ELEV_FT\n",
      "REMOVING geometry\n",
      "Completed interpolation for Layer 1\n",
      "Completed interpolation for Layer 2\n",
      "Completed interpolation for Layer 3\n",
      "Completed interpolation for Layer 4\n",
      "Completed interpolation for Layer 5\n",
      "Completed interpolation for Layer 6\n",
      "Completed interpolation for Layer 7\n",
      "Completed interpolation for Layer 8\n",
      "Completed interpolation for Layer 9\n"
     ]
    }
   ],
   "source": [
    "specTermsPATH, startTermsPATH = w4h.get_search_terms(spec_dir=str(repoDir)+'/resources/', spec_glob_pattern='*SearchTerms-Specific*', start_glob_pattern='*SearchTerms-Start*', log=True)\n",
    "specTerms = w4h.read_dictionary_terms(dict_file=specTermsPATH, log=True)\n",
    "startTerms = w4h.read_dictionary_terms(dict_file=startTermsPATH, log=True)\n",
    "oldDictPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\WellData\\Dictionaries\\DICTIONARY_Updated-06-2018.csv\"\n",
    "oldDict = w4h.read_dictionary_terms(dict_file=oldDictPath, cols={'DESCRIPTION':'FORMATION', 'LITHOLOGY':'INTERPRETATION'}, class_flag=1, log=True)\n",
    "\n",
    "specTerms = pd.concat([specTerms, oldDict])\n",
    "specTerms.drop_duplicates(subset='FORMATION', inplace=True)\n",
    "specTerms.reset_index(inplace=True, drop=True)\n",
    "downholeData = w4h.specific_define(downholeData, specTerms, verbose=True, log=True)\n",
    "classifedDF, searchDF = w4h.split_defined(downholeData)\n",
    "searchDF = w4h.start_define(df=searchDF, terms_df=startTerms, verbose=True)\n",
    "downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF)\n",
    "classifedDF, searchDF = w4h.split_defined(downholeData)\n",
    "searchDF = w4h.depth_define(searchDF, thresh=550, verbose=True, log=True)\n",
    "downholeData = w4h.remerge_data(classifieddf=classifedDF, searchdf=searchDF)\n",
    "downholeData = w4h.fill_unclassified(downholeData)\n",
    "#dictDir = \"\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\ISWS_HydroGeo\\\\WellDataAutoClassification\\\\SupportingDocs\\\\\"\n",
    "targetInterpDF = w4h.read_lithologies(log=True)\n",
    "downholeData = w4h.merge_lithologies(df=downholeData, targinterps_df=targetInterpDF)\n",
    "wellsDF = w4h.get_unique_wells(downholeData)\n",
    "downholeData = w4h.sort_dataframe(df=downholeData, sort_cols=['API_NUMBER','TOP'], remove_nans=True)\n",
    "inGrids = [bedrockElevGridIN, surfaceElevGridIN]\n",
    "bedrockGrid, surfaceGrid = w4h.align_rasters(grids_unaligned=inGrids, modelgrid=modelGrid, log=True)\n",
    "driftThickGrid, layerThickGrid = w4h.get_drift_thick(surface=surfaceGrid, bedrock=bedrockGrid, layers=9, plot=False, log=True)\n",
    "headerData = w4h.sample_raster_points(raster=bedrockGrid, points_df=headerData, new_col='BEDROCK_ELEV_FT', log=True)\n",
    "headerData = w4h.sample_raster_points(raster=surfaceGrid, points_df=headerData, new_col='SURFACE_ELEV_FT', log=True)\n",
    "headerData = w4h.sample_raster_points(raster=driftThickGrid, points_df=headerData, new_col='BEDROCK_DEPTH_FT', log=True)\n",
    "headerData = w4h.sample_raster_points(raster=layerThickGrid, points_df=headerData, new_col='LAYER_THICK_FT', log=True)\n",
    "headerData = w4h.get_layer_depths(well_metadata=headerData, no_layers=9, log=True)\n",
    "downholeData_layerInfo = w4h.merge_tables(data_df=downholeData,  data_cols=None, header_cols=None, header_df=headerData,on='API_NUMBER', how='inner', auto_pick_cols=True, log=True)\n",
    "#downholeData = downholeData_layerInfo.copy()\n",
    "resdf = w4h.layer_target_thick(downholeData_layerInfo, layers=9, outfile_prefix='CoarseFine', log=True)\n",
    "layers_data = w4h.layer_interp(points=resdf, layers=9, grid=modelGrid, method='lin', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING LATITUDE\n",
      "REMOVING LONGITUDE\n"
     ]
    }
   ],
   "source": [
    "header_cols = ['API_NUMBER', 'LATITUDE', 'LONGITUDE', 'BEDROCK_ELEV_FT', 'SURFACE_ELEV_FT', 'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT']\n",
    "data_cols = downholeData.columns\n",
    "on='API_NUMBER'\n",
    "#Drop duplicate columns\n",
    "drop_duplicate_cols=True\n",
    "if drop_duplicate_cols:\n",
    "    header_colCopy= header_cols.copy()\n",
    "    remCount = 0\n",
    "    for i, c in enumerate(header_colCopy):\n",
    "        if c in data_cols and c != on:\n",
    "            print('REMOVING', header_cols[i-remCount])\n",
    "            header_cols.pop(i - remCount)\n",
    "            remCount += 1\n",
    "\n",
    "downholeData_layerInfo = pd.merge(left=downholeData, right=headerData,on='API_NUMBER', how='inner',)\n",
    "#downholeData_layerInfo = w4h.merge_tables(data_df=downholeData,  data_cols=None, header_cols=None, header_df=headerData,on='API_NUMBER', how='inner', auto_pick_cols=True, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['API_NUMBER', 'TABLE_NAME', 'FORMATION', 'THICKNESS', 'TOP', 'BOTTOM',\n",
       "       'TOTAL_DEPTH_x', 'SECTION_x', 'TWP_x', 'TDIR_x', 'RNG_x', 'RDIR_x',\n",
       "       'MERIDIAN_x', 'QUARTERS_x', 'ELEVATION_x', 'ELEVREF_x', 'COUNTY_CODE_x',\n",
       "       'ELEVSOURCE_x', 'LATITUDE_x', 'LONGITUDE_x', 'ELEV_FT_x', 'geometry_x',\n",
       "       'INTERPRETATION', 'CLASS_FLAG', 'BEDROCK_FLAG', 'TARGET',\n",
       "       'TOTAL_DEPTH_y', 'SECTION_y', 'TWP_y', 'TDIR_y', 'RNG_y', 'RDIR_y',\n",
       "       'MERIDIAN_y', 'QUARTERS_y', 'ELEVATION_y', 'ELEVREF_y', 'COUNTY_CODE_y',\n",
       "       'ELEVSOURCE_y', 'LATITUDE', 'LONGITUDE', 'ELEV_FT_y', 'geometry',\n",
       "       'LONGITUDE_PROJ', 'LATITUDE_PROJ', 'BEDROCK_ELEV_FT', 'SURFACE_ELEV_FT',\n",
       "       'BEDROCK_DEPTH_FT', 'LAYER_THICK_FT', 'DEPTH_FT_LAYER1',\n",
       "       'DEPTH_FT_LAYER2', 'DEPTH_FT_LAYER3', 'DEPTH_FT_LAYER4',\n",
       "       'DEPTH_FT_LAYER5', 'DEPTH_FT_LAYER6', 'DEPTH_FT_LAYER7',\n",
       "       'DEPTH_FT_LAYER8', 'DEPTH_FT_LAYER9', 'ELEV_FT_LAYER1',\n",
       "       'ELEV_FT_LAYER2', 'ELEV_FT_LAYER3', 'ELEV_FT_LAYER4', 'ELEV_FT_LAYER5',\n",
       "       'ELEV_FT_LAYER6', 'ELEV_FT_LAYER7', 'ELEV_FT_LAYER8', 'ELEV_FT_LAYER9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downholeData_layerInfo=downholeData_layerInfo.rename(columns={'LATITUDE_y':\"LATITUDE\",'LONGITUDE_y':\"LONGITUDE\", 'geometry_y':'geometry'})\n",
    "downholeData_layerInfo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf = w4h.layer_target_thick(downholeData_layerInfo, layers=9, outfile_prefix='CoarseFine', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed interpolation for Layer 1\n",
      "Completed interpolation for Layer 2\n",
      "Completed interpolation for Layer 3\n",
      "Completed interpolation for Layer 4\n",
      "Completed interpolation for Layer 5\n",
      "Completed interpolation for Layer 6\n",
      "Completed interpolation for Layer 7\n",
      "Completed interpolation for Layer 8\n",
      "Completed interpolation for Layer 9\n"
     ]
    }
   ],
   "source": [
    "layers_data = w4h.layer_interp(points=resdf, layers=9, grid=modelGrid, method='nn', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir=r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "w4h.export_grids(grid_data=layers_data, out_path=out_dir, file_id='',filetype='tif', variable_sep=True, date_stamp=True, log=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export (rio)xarray dataarrays and datasets\n",
    "def export_grids(grid_data, out_path, file_id='',filetype='tif', variable_sep=True, date_stamp=True):\n",
    "    \"\"\"Function to export grids to files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grid_data : xarray DataArray or xarray Dataset\n",
    "        Dataset or dataarray to be exported\n",
    "    out_path : str or pathlib.Path object\n",
    "        Output location for data export. If variable_sep=True, this should be a directory. Otherwise, this should also include the filename. The file extension should not be included here.\n",
    "    filetype : str, optional\n",
    "        Output filetype. Can either be pickle or any file extension supported by rioxarray.rio.to_raster(). Can either include period or not., by default 'tif'\n",
    "    variable_sep : bool, optional\n",
    "        If grid_data is an xarray Dataset, this will export each variable in the dataset as a separate file, including the variable name in the filename, by default False\n",
    "    date_stamp : bool, optional\n",
    "        Whether to include a date stamp in the file name., by default True\n",
    "    \"\"\"\n",
    "\n",
    "    #Initialize lists to determine which filetype will be used for export\n",
    "    ncdfList = ['netcdf', 'ncdf', 'n']\n",
    "    tifList = ['tif', 'tiff', 'geotiff', 'geotif', 't']\n",
    "    pickleList = ['pickle', 'pkl', 'p']\n",
    "\n",
    "    #Format output string(s)\n",
    "    #Format output filepath\n",
    "    if type(out_path) is str or isinstance(out_path, pathlib.PurePath):\n",
    "        if isinstance(out_path, pathlib.PurePath):\n",
    "            pass\n",
    "        else:\n",
    "            out_path = pathlib.Path(out_path)\n",
    "        if out_path.parent.exists()==False:\n",
    "            print('Directory does not exist. Please enter a different value for the out_path parameter.')\n",
    "            return        \n",
    "\n",
    "        if out_path.is_dir():\n",
    "            if isinstance(grid_data, xr.DataArray):\n",
    "                if variable_sep:\n",
    "                    lyrs = grid_data.coords['Layer'].values\n",
    "                    filenames = []\n",
    "                    for l in lyrs:\n",
    "                        filenames.append('Layer'+str(l))\n",
    "                else:\n",
    "                    filenames = ['AllLayers']\n",
    "            if isinstance(grid_data, xr.Dataset):\n",
    "                if variable_sep:\n",
    "                    filenames = []\n",
    "                    for var in grid_data:\n",
    "                        filenames.append(var)\n",
    "                else:\n",
    "                    filenames = ['AllLayers']    \n",
    "        else:\n",
    "            filenames = [out_path.stem]\n",
    "            out_path = out_path.parent\n",
    "\n",
    "    else:\n",
    "        print('Please input string or pathlib object for out_path parameters')\n",
    "        return\n",
    "    \n",
    "    #Format datestamp, if desired in output filename\n",
    "    if date_stamp:\n",
    "        todayDate = datetime.date.today()\n",
    "        todayDateStr = '_'+str(todayDate)\n",
    "    else:\n",
    "        todayDateStr=''\n",
    "\n",
    "    #Ensure the file suffix includes .\n",
    "    if filetype[0] == '.':\n",
    "        pass\n",
    "    else:\n",
    "        filetype = '.' + filetype\n",
    "\n",
    "    if file_id != '':\n",
    "        file_id = '_'+file_id\n",
    "\n",
    "    out_path = out_path.as_posix()+'/'\n",
    "    outPaths = []\n",
    "    for f in filenames:\n",
    "        outPaths.append(out_path+f+file_id+todayDateStr+filetype)\n",
    "\n",
    "    #Do export\n",
    "    if filetype.lower() in pickleList:\n",
    "        import pickle\n",
    "        for op in outPaths:\n",
    "            try:\n",
    "                with open(op, 'wb') as f:\n",
    "                    pickle.dump(grid_data, f)\n",
    "            except:\n",
    "                print('An error occured during export.')\n",
    "                print(op, 'could not be exported as a pickle object.')\n",
    "                print('Try again using different parameters.')\n",
    "    else:\n",
    "        import rioxarray as rxr\n",
    "        try:\n",
    "            if isinstance(grid_data, xr.Dataset):\n",
    "                if variable_sep:\n",
    "                    for i, var in enumerate(grid_data.data_vars):\n",
    "                        grid_data[var].rio.to_raster(outPaths[i])\n",
    "                else:\n",
    "                    grid_data.rio.to_raster(outPaths[0])\n",
    "            elif isinstance(grid_data, xr.DataArray):\n",
    "                if variable_sep:\n",
    "                    lyrs = grid_data.coords['Layer'].values\n",
    "                    for i, l in enumerate(lyrs):\n",
    "                        out_grid = grid_data.sel(Layer = l).copy()\n",
    "                        out_grid.rio.to_raster(outPaths[i])\n",
    "                else:\n",
    "                    grid_data.rio.to_raster(outPaths[0])\n",
    "            else:\n",
    "                grid_data.rio.to_raster(outPaths[0])\n",
    "        except:\n",
    "            print('An error occured during export.')\n",
    "            print('{} could not be exported as {} file.'.format(outPaths, filetype))\n",
    "            print('Try again using different parameters.')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('//isgs-sinkhole.ad.uillinois.edu/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "out_dir = pathlib.Path(out_dir)\n",
    "out_dir.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\ProcessedData\"\n",
    "export_grids(grid_data=layers_data, out_path=out_dir, file_id='Coarse', filetype='tif', variable_sep=True, date_stamp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyzDataIN = w4h.read_xyz(xyzpath=xyzInPATH, log=True)\n",
    "downholeData = w4h.define_dtypes(df=downholeDataIN, dtype_file='downholeDataTypes.txt', log=True) #Define datatypes of each column of the new dataframes\n",
    "headerData = w4h.define_dtypes(df=headerDataIN, dtype_file='headerDataTypes.txt', log=True)#Define datatypes of each column of the new dataframes\n",
    "xyzData = w4h.define_dtypes(df=xyzDataIN, dtype_file='xyzDataTypes.txt', log=True)\n",
    "studyAreaPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESL_StudyArea_5mi.shp\"\n",
    "studyAreaIN = w4h.read_study_area(studyareapath=studyAreaPath, log=True)\n",
    "modelGridPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\grid_625_raster.tif\"\n",
    "surfaceElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ILStateLidar_ClipExtentESL.tif\"\n",
    "bedrockElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESLBedrock.tif\"\n",
    "modelGrid = w4h.read_grid(datapath=modelGridPath, grid_type='model', study_area=studyAreaIN,  read_grid=True, clip_to_studyarea=True, log=True)#, gridcrs='EPSG:26715', studyAreacrs='EPSG:26715')\n",
    "surfaceElevGridIN = w4h.read_grid(datapath=surfaceElevPath, grid_type='surface', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "bedrockElevGridIN = w4h.read_grid(datapath=bedrockElevPath, grid_type='bedrock', study_area=studyAreaIN, use_service=False, clip_to_studyarea=True, log=True)\n",
    "#Code here for adding in control points\n",
    "headerData = w4h.xyz_metadata_merge(xyz=xyzData, metadata=headerData, log=True) #This probably needs to be updated\n",
    "headerData = w4h.coords2geometry(df=headerData, xcol='LONGITUDE', ycol='LATITUDE', crs='EPSG:4269', log=True)\n",
    "headerData = w4h.clip_gdf2study_area(study_area=studyAreaIN, gdf=headerData, gdf_crs='EPSG:4269', log=True)\n",
    "downholeData = w4h.remove_nonlocated(downholeData, headerData, log=True)\n",
    "headerData = w4h.remove_no_topo(df=headerData, verbose=True, log=True)\n",
    "donwholeData = w4h.drop_no_depth(downholeData, verbose=True, log=True) #Drop records with no depth information\n",
    "donwholeData = w4h.drop_bad_depth(downholeData, verbose=True, log=True)#Drop records with bad depth information (i.e., top depth > bottom depth) (Also calculates thickness of each record)\n",
    "downholeData = w4h.drop_no_formation(downholeData, verbose=True, log=True)\n",
    "#downholeData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "#headerData.reset_index(inplace=True,drop=True) #These may not be necessary\n",
    "downholeData = pd.merge(left = downholeData, right = headerData, on='API_NUMBER')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "1eab39f8790fa4ef06ab4ebada9c1405c2ef16219adfedb21e90bf3fb356ecb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
