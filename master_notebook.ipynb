{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{Fill in with information about this notebook}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "#The rest of this is just for fun/quick dataviz at the end to map and ensure seems about right\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import geopandas\n",
    "\n",
    "from lib import readData\n",
    "from lib import mapping\n",
    "from lib import cleanData\n",
    "from lib import classify\n",
    "from lib import exportData\n",
    "\n",
    "todayDate, dateSuffix = readData.getCurrentDate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set up filepaths\n",
    "- Read in data from:\n",
    "    - downholeData table (from database)\n",
    "    - headerData table (from database)\n",
    "    - xyzData file (from previously carried out work) (will eventually make this updateable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downholeDataPATH, headerDataPATH, xyzInPATH  = readData.filesSetup()\n",
    "\n",
    "#Functions to read data into dataframes. Also excludes extraneous columns\n",
    "downholeDataIN, headerDataIN = readData.readRawTxtData(downholefile=downholeDataPATH, headerfile=headerDataPATH)\n",
    "downholeDataIN, headerDataIN = readData.essentialCols(downholeDataIN, headerDataIN)\n",
    "xyzDataIN = readData.readXYZData(xyzfile=xyzInPATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define datatypes (doing this during the read in process has presented issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define datatypes, to read into defineDataTypes() function\n",
    "##EVENTUALLY, MAKE THIS A FILE IN THE RES FOLDER TO READ IN\n",
    "downholeDataDTYPES = {'ID':np.uint32, \"API_NUMBER\":np.uint64,\"TABLE_NAME\":str,\"WHO\":str,\"INTERPRET_DATE\":str,\"FORMATION\":str,\"THICKNESS\":np.float64,\"TOP\":np.float64,\"BOTTOM\":np.float64}\n",
    "headerDataDTYPES = {'ID':np.uint32,'API_NUMBER':np.uint64,\"TDFORMATION\":str,\"PRODFORM\":str,\"TOTAL_DEPTH\":np.float64,\"SECTION\":np.float64,\"TWP\":np.float64,\"TDIR\":str,\"RNG\":np.float64,\"RDIR\":str,\"MERIDIAN\":np.float64,\"FARM_NAME\":str,\"NSFOOT\":np.float64,\"NSDIR\":str,\"EWFOOT\":np.float64,\"EWDIR\":str,\"QUARTERS\":str,\"ELEVATION\":np.float64,\"ELEVREF\":str,\"COMP_DATE\":str,\"STATUS\":str,\"FARM_NUM\":str,\"COUNTY_CODE\":np.float64,\"PERMIT_NUMBER\":str,\"COMPANY_NAME\":str,\"COMPANY_CODE\":str,\"PERMIT_DATE\":str,\"CORNER\":str,\"LATITUDE\":np.float64,\"LONGITUDE\":np.float64,\"ENTERED_BY\":str,\"UPDDATE\":str,\"ELEVSOURCE\":str, \"ELEV_FT\":np.float64}\n",
    "xyzDataDTYPES = {'ID':np.uint64, 'API_NUMBER':np.uint64, \"LATITUDE\":np.float64, \"LONGITUDE\":np.float64, \"ELEV_FT\":np.float64}\n",
    "\n",
    "#Define datatypes of each column of the new dataframes\n",
    "downholeDataIN = readData.defineDataTypes(downholeDataIN, js)\n",
    "headerDataIN = readData.defineDataTypes(headerDataIN, headerDataDTYPES)\n",
    "xyzDataIN = readData.defineDataTypes(xyzDataIN, xyzDataDTYPES)\n",
    "\n",
    "#Make a copy of the data so raw data is preserved while we work with the rest of the data\n",
    "downholeData = downholeDataIN.copy()\n",
    "headerData = headerDataIN.copy()\n",
    "xyzData = xyzDataIN.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downhole Data has 3017085 valid well records.\n",
      "Header Data has 634755 unique wells with valid location information.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'lib.readData' has no attribute 'essentialCols'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [80], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m headerDataPATH \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39misgs-sinkhole\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mgeophysics\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mBalikian\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mISWS_HydroGeo\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mRawWellData_OracleDatabase\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTxtData\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mlcl_ISGS_HEADER.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m downholeDataIN, headerDataIN \u001b[39m=\u001b[39m readData\u001b[39m.\u001b[39mreadRawTxtData(rawdir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m,downholefile\u001b[39m=\u001b[39mdownholeDataPATH, headerfile\u001b[39m=\u001b[39mheaderDataPATH)\n\u001b[1;32m---> 16\u001b[0m downholeDataIN, headerDataIN \u001b[39m=\u001b[39m readData\u001b[39m.\u001b[39;49messentialCols(downholeDataIN, headerDataIN)\n\u001b[0;32m     17\u001b[0m downholeDataIN \u001b[39m=\u001b[39m readData\u001b[39m.\u001b[39mdefineDataTypes(downholeDataIN, js)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'lib.readData' has no attribute 'essentialCols'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "downholeDataDTYPES = {'ID':np.uint32, \"API_NUMBER\":np.uint64,\"TABLE_NAME\":str,\"WHO\":str,\"INTERPRET_DATE\":str,\"FORMATION\":str,\"THICKNESS\":np.float64,\"TOP\":np.float64,\"BOTTOM\":np.float64}\n",
    "\n",
    "dir =r'\\\\isgs-sinkhole\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\\\'[:-1]\n",
    "#dir = '../res/'\n",
    "file = 'downholeDataTypes.txt'\n",
    "\n",
    "\n",
    "with open(dir+file, 'r') as f:\n",
    "    data= (f.read())\n",
    "js = json.loads(data)\n",
    "\n",
    "downholeDataPATH = r\"\\\\isgs-sinkhole\\geophysics\\Balikian\\ISWS_HydroGeo\\RawWellData_OracleDatabase\\TxtData\\lcl_ISGS_DOWNHOLE_DATA.txt\"\n",
    "headerDataPATH = r\"\\\\isgs-sinkhole\\geophysics\\Balikian\\ISWS_HydroGeo\\RawWellData_OracleDatabase\\TxtData\\lcl_ISGS_HEADER.txt\"\n",
    "downholeDataIN, headerDataIN = readData.readRawTxtData(rawdir='',downholefile=downholeDataPATH, headerfile=headerDataPATH)\n",
    "downholeDataIN, headerDataIN = readData.essentialCols(downholeDataIN, headerDataIN)\n",
    "downholeDataIN = readData.defineDataTypes(downholeDataIN, js)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Elevation Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract elevation data from consistent elevation dataset for all wells (lidar or other statewide DEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, get wells with updated xyz info\n",
    "    #Check first if xyzData needs to be updated with locations (?)\n",
    "    #Check which wells in headerData don't have associated lidar data\n",
    "\n",
    "#statewideLidar =  ow\n",
    "#mapping.rastertoPoints_extract()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge elevation data with headerData table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWells = headerData['API_NUMBER'].unique()\n",
    "xyzData['UniqueWells'] = uniqueWells\n",
    "\n",
    "headerData = mapping.addElevtoHeader(xyzData, headerData)\n",
    "##NEED TO UPDATE THIS TO WORK WITH DATA WITH NO XYZ ELEVATION DATA FROM LIDAR\n",
    "#Change xyz column name to indicate lidar\n",
    "#Use order of preference: lidar, headerData table?/30/10m DEM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's clean up records in the data without the necessary information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HeaderXYZ dataframe has headerdata with updated elevations (from lidar or other DEM)\n",
    "\n",
    "Now, remove data from downholeData table that does not have location information (Since we would not know where to put it anyway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11440 records removed without Lat/Lon\n"
     ]
    }
   ],
   "source": [
    "downholeData = mapping.removeNonlocatedData(downholeData, headerData)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip data from outside Illinois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not done yet, use geopandas clip (May need to generate geometry column first in headerData)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove headerData rows without surface elevation information (this currently clips data from outside Illinois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropping those without surface elevation information: 634755\n",
      "Number of rows after dropping those without surface elevation information: 634533\n",
      "Well records deleted: 222\n"
     ]
    }
   ],
   "source": [
    "headerData = cleanData.removenotopo(headerData, printouts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows from downholeData with no depth information and where depth information is obviously bad (i.e., top depth > bottom depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop records with no depth information\n",
    "donwholeData = cleanData.dropnodepth(downholeData, printouts=True)\n",
    "#Drop records with bad depth information (i.e., top depth > bottom depth) (Also calculates thickness of each record)\n",
    "donwholeData = cleanData.dropbaddepth(downholeData, printouts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop records with no FORMATION information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downholeData = cleanData.dropnoformation(downholeData, printouts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to export this data, to have record of cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downholeData.reset_index(inplace=True,drop=True)\n",
    "headerData.reset_index(inplace=True,drop=True)\n",
    "\n",
    "downholeData.to_csv('../out/downholeData_cleaned'+dateSuffix+'.csv',index_label='ID')\n",
    "headerData.to_csv('../out/headerData_cleaned'+dateSuffix+'.csv',index_label='ID')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following flags are used to mark the classification method:\n",
    "- 0: Not classified\n",
    "- 1: Specific Search Term Match\n",
    "- 2: Wildcard match (startTerm) - no context\n",
    "- 3: Bedrock classification for obvious bedrock\n",
    "- 4: Wildcard match (startTerm) - with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in dictionary files for downhole data\n",
    "specTermsPATH, startTermsPATH = readData.searchTermFilePaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specTerms, startTerms = readData.readSearchTerms(specfile=specTermsPATH, startfile=startTermsPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the dataframes--for the specific search terms, this is the same as classifying them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downholeData_spec = classify.specificDefine(downholeData, specTerms, printouts=True)\n",
    "downholeData = downholeData_spec.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with only the records already classified (using the specific search terms in this case, classifiedDF), and one that still needs to be searched (searchDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifedDF, searchDF = classify.splitDefined(downholeData)\n",
    "searchDF.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do the classification routine on the searchDF database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchDF = classify.startDefine(df=searchDF, starterms=startTerms, printouts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge specDF and searchDF back together all back in single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downholeData_Terms = classify.remergeData(classifieddf=classifedDF, searchdf=searchDF):\n",
    "downholeData = downholeData_Terms.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export terms that still need to be defined to csv (along with their counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify.export_toBeDefined(downholeData, '../out/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify all  data under depth threshold (default is 550') as bedrock (should not be an issue, but just in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifedDF, searchDF = classify.splitDefined(downholeData)\n",
    "searchDF = classify.depthDefine(searchDF, thresh=550, printouts=True)\n",
    "downholeData_Class = classify.remergeData(classifieddf=classifedDF, searchdf=searchDF)\n",
    "downholeData = downholeData_Class.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add '0' flag for data still not classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downholeData = classify.fillUnclassified(downholeData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    1161561\n",
       "0.0     986327\n",
       "3.0     273243\n",
       "4.0     167146\n",
       "Name: CLASS_FLAG, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downholeData['CLASS_FLAG'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add \"Flag\" for target interpratations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INTERPRETATION</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEDROCK</td>\n",
       "      <td>DoNotUse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FIRECLAY</td>\n",
       "      <td>DoNotUse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VOID</td>\n",
       "      <td>DoNotUse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CLAY AND STONE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DIRT AND BEDROCK</td>\n",
       "      <td>DoNotUse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DIRT AND STONE</td>\n",
       "      <td>DoNotUse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GRAVEL AND STONE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BOULDER</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BOULDERS AND CLAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GRAVEL WITH BOULDERS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CLAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DIRT AND CLAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CLAY AND GRAVEL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CLAY WITH GRAVEL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MUD AND GRAVEL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CLAY AND SAND</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CLAY WITH SAND</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MUD AND SAND</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CLAY WITH GRAVEL SEAMS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CLAY WITH SAND STREAKS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FILL</td>\n",
       "      <td>DoNotUse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DRIFT</td>\n",
       "      <td>DoNotUse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MUD</td>\n",
       "      <td>DoNotUse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DIRT AND GRAVEL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GRAVEL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>GRAVEL WITH SAND</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GRAVEL AND CLAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>GRAVEL WITH CLAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>GRAVEL WITH CLAY STREAKS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MARL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ORGANIC MATERIAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>DIRT AND SAND</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>SAND</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SAND AND CLAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>SAND WITH CLAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SAND AND GRAVEL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SAND WITH GRAVEL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>SAND WITH CLAY STREAKS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>SILT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>SILT AND GRAVEL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>SILT AND SAND</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>SILT WITH SAND SEAMS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>DIRT</td>\n",
       "      <td>DoNotUse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SOIL</td>\n",
       "      <td>DoNotUse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>DoNotUse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              INTERPRETATION    TARGET\n",
       "0                    BEDROCK  DoNotUse\n",
       "1                   FIRECLAY  DoNotUse\n",
       "2                       VOID  DoNotUse\n",
       "3             CLAY AND STONE         0\n",
       "4           DIRT AND BEDROCK  DoNotUse\n",
       "5             DIRT AND STONE  DoNotUse\n",
       "6           GRAVEL AND STONE         1\n",
       "7                    BOULDER         1\n",
       "8          BOULDERS AND CLAY         0\n",
       "9       GRAVEL WITH BOULDERS         1\n",
       "10                      CLAY         0\n",
       "11             DIRT AND CLAY         0\n",
       "12           CLAY AND GRAVEL         0\n",
       "13          CLAY WITH GRAVEL         0\n",
       "14            MUD AND GRAVEL         0\n",
       "15             CLAY AND SAND         0\n",
       "16            CLAY WITH SAND         0\n",
       "17              MUD AND SAND         0\n",
       "18    CLAY WITH GRAVEL SEAMS         0\n",
       "19    CLAY WITH SAND STREAKS         0\n",
       "20                      FILL  DoNotUse\n",
       "21                     DRIFT  DoNotUse\n",
       "22                       MUD  DoNotUse\n",
       "23           DIRT AND GRAVEL         1\n",
       "24                    GRAVEL         1\n",
       "25          GRAVEL WITH SAND         1\n",
       "26           GRAVEL AND CLAY         0\n",
       "27          GRAVEL WITH CLAY         0\n",
       "28  GRAVEL WITH CLAY STREAKS         0\n",
       "29                      MARL         0\n",
       "30          ORGANIC MATERIAL         0\n",
       "31             DIRT AND SAND         1\n",
       "32                      SAND         1\n",
       "33             SAND AND CLAY         0\n",
       "34            SAND WITH CLAY         0\n",
       "35           SAND AND GRAVEL         1\n",
       "36          SAND WITH GRAVEL         1\n",
       "37    SAND WITH CLAY STREAKS         0\n",
       "38                      SILT         0\n",
       "39           SILT AND GRAVEL         0\n",
       "40             SILT AND SAND         0\n",
       "41      SILT WITH SAND SEAMS         0\n",
       "42                      DIRT  DoNotUse\n",
       "43                      SOIL  DoNotUse\n",
       "44                   UNKNOWN  DoNotUse"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dictDir = \"\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\ISWS_HydroGeo\\\\WellDataAutoClassification\\\\SupportingDocs\\\\\"\n",
    "InterpFile = 'Lithology_Interp_FineCoarse.csv'\n",
    "targetInterpDF = pd.read_csv(dictDir+InterpFile)\n",
    "targetInterpDF.rename(columns={'LITHOLOGY':'INTERPRETATION', 'CODE':'TARGET'}, inplace=True)\n",
    "targetInterpDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>API_NUMBER</th>\n",
       "      <th>TABLE_NAME</th>\n",
       "      <th>FORMATION</th>\n",
       "      <th>THICKNESS</th>\n",
       "      <th>TOP</th>\n",
       "      <th>BOTTOM</th>\n",
       "      <th>INTERPRETATION</th>\n",
       "      <th>CLASS_FLAG</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120010000300</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>Colchester No. 2 coal,Penn. carb</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120010000300</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>fire clay at</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120010000300</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>interval drift?</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120010000800</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>coal No. 2 possibly at</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120010000800</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>interval</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588272</th>\n",
       "      <td>480590043000</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>red clay, firm</td>\n",
       "      <td>120.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588273</th>\n",
       "      <td>480590043000</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>sand silt &amp; gravel</td>\n",
       "      <td>165.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588274</th>\n",
       "      <td>480590043000</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>sandy blue clay, soft</td>\n",
       "      <td>85.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588275</th>\n",
       "      <td>480590043000</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>stoney blue clay</td>\n",
       "      <td>18.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588276</th>\n",
       "      <td>480590043000</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>yellow clay</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>CLAY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2588277 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           API_NUMBER   TABLE_NAME                         FORMATION  \\\n",
       "0        120010000300  WFORMATIONS  Colchester No. 2 coal,Penn. carb   \n",
       "1        120010000300  WFORMATIONS                      fire clay at   \n",
       "2        120010000300  WFORMATIONS                   interval drift?   \n",
       "3        120010000800  WFORMATIONS            coal No. 2 possibly at   \n",
       "4        120010000800  WFORMATIONS                          interval   \n",
       "...               ...          ...                               ...   \n",
       "2588272  480590043000  WFORMATIONS                    red clay, firm   \n",
       "2588273  480590043000  WFORMATIONS                sand silt & gravel   \n",
       "2588274  480590043000  WFORMATIONS             sandy blue clay, soft   \n",
       "2588275  480590043000  WFORMATIONS                  stoney blue clay   \n",
       "2588276  480590043000  WFORMATIONS                       yellow clay   \n",
       "\n",
       "         THICKNESS    TOP  BOTTOM INTERPRETATION  CLASS_FLAG TARGET  \n",
       "0              2.0   18.0    20.0            NaN         0.0     -2  \n",
       "1              0.0   20.0    20.0            NaN         0.0     -2  \n",
       "2             18.0    0.0    18.0            NaN         0.0     -2  \n",
       "3              0.0  175.0   175.0            NaN         0.0     -2  \n",
       "4            175.0    0.0   175.0            NaN         0.0     -2  \n",
       "...            ...    ...     ...            ...         ...    ...  \n",
       "2588272      120.0   45.0   165.0            NaN         0.0     -2  \n",
       "2588273      165.0    6.0   171.0            NaN         0.0     -2  \n",
       "2588274       85.0   35.0   120.0            NaN         0.0     -2  \n",
       "2588275       18.0   67.0    85.0            NaN         0.0     -2  \n",
       "2588276        0.0   18.0    18.0           CLAY         1.0      0  \n",
       "\n",
       "[2588277 rows x 9 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downholeData_targ = pd.merge(downholeData, targetInterpDF.set_index('INTERPRETATION'), right_on='INTERPRETATION',left_on='INTERPRETATION', how='left')\n",
    "downholeData = downholeData_targ.copy()\n",
    "downholeData['TARGET'].replace('DoNotUse', value=-1, inplace=True)\n",
    "downholeData['TARGET'].fillna(value=-2, inplace=True)\n",
    "downholeData['TARGET'].astype(np.int8)\n",
    "downholeData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flags used for target classification purposes:\n",
    "- -2: No classification \n",
    "- -1: Classified, not used/not definitive\n",
    "- 0: Classified, not target material\n",
    "- 1: Classified as target material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2    1337019\n",
       "-1     722307\n",
       "0      336977\n",
       "1      191974\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downholeData['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all unique wells in downhole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique wells in downholeData: 413902\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNIQUE_API</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120010000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120010000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120010000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120010001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120010001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413897</th>\n",
       "      <td>480590000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413898</th>\n",
       "      <td>480590001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413899</th>\n",
       "      <td>480590027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413900</th>\n",
       "      <td>480590028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413901</th>\n",
       "      <td>480590043000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413902 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          UNIQUE_API\n",
       "0       120010000300\n",
       "1       120010000800\n",
       "2       120010000900\n",
       "3       120010001000\n",
       "4       120010001100\n",
       "...              ...\n",
       "413897  480590000900\n",
       "413898  480590001200\n",
       "413899  480590027200\n",
       "413900  480590028100\n",
       "413901  480590043000\n",
       "\n",
       "[413902 rows x 1 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Unique well APIs\n",
    "uniqueWells = downholeData['API_NUMBER'].unique()\n",
    "wellsDF = pd.DataFrame(uniqueWells)\n",
    "print('Number of unique wells in downholeData: '+str(wellsDF.shape[0]))\n",
    "wellsDF.columns = ['UNIQUE_API']\n",
    "wellsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort dataset by API Number and Depth of top of record (will be easier to do data analysis with records in the correct order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>API_NUMBER</th>\n",
       "      <th>TABLE_NAME</th>\n",
       "      <th>FORMATION</th>\n",
       "      <th>THICKNESS</th>\n",
       "      <th>TOP</th>\n",
       "      <th>BOTTOM</th>\n",
       "      <th>INTERPRETATION</th>\n",
       "      <th>CLASS_FLAG</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>120010000300</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>interval drift?</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>120010000300</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>Colchester No. 2 coal,Penn. carb</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>120010000300</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>fire clay at</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>120010000800</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>interval</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>120010000800</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>coal No. 2 possibly at</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588272</th>\n",
       "      <td>2588273</td>\n",
       "      <td>480590043000</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>sand silt &amp; gravel</td>\n",
       "      <td>165.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588273</th>\n",
       "      <td>2588276</td>\n",
       "      <td>480590043000</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>yellow clay</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>CLAY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588274</th>\n",
       "      <td>2588274</td>\n",
       "      <td>480590043000</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>sandy blue clay, soft</td>\n",
       "      <td>85.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588275</th>\n",
       "      <td>2588272</td>\n",
       "      <td>480590043000</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>red clay, firm</td>\n",
       "      <td>120.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588276</th>\n",
       "      <td>2588275</td>\n",
       "      <td>480590043000</td>\n",
       "      <td>WFORMATIONS</td>\n",
       "      <td>stoney blue clay</td>\n",
       "      <td>18.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2588277 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index    API_NUMBER   TABLE_NAME                         FORMATION  \\\n",
       "0              2  120010000300  WFORMATIONS                   interval drift?   \n",
       "1              0  120010000300  WFORMATIONS  Colchester No. 2 coal,Penn. carb   \n",
       "2              1  120010000300  WFORMATIONS                      fire clay at   \n",
       "3              4  120010000800  WFORMATIONS                          interval   \n",
       "4              3  120010000800  WFORMATIONS            coal No. 2 possibly at   \n",
       "...          ...           ...          ...                               ...   \n",
       "2588272  2588273  480590043000  WFORMATIONS                sand silt & gravel   \n",
       "2588273  2588276  480590043000  WFORMATIONS                       yellow clay   \n",
       "2588274  2588274  480590043000  WFORMATIONS             sandy blue clay, soft   \n",
       "2588275  2588272  480590043000  WFORMATIONS                    red clay, firm   \n",
       "2588276  2588275  480590043000  WFORMATIONS                  stoney blue clay   \n",
       "\n",
       "         THICKNESS    TOP  BOTTOM INTERPRETATION  CLASS_FLAG TARGET  \n",
       "0             18.0    0.0    18.0            NaN         0.0     -2  \n",
       "1              2.0   18.0    20.0            NaN         0.0     -2  \n",
       "2              0.0   20.0    20.0            NaN         0.0     -2  \n",
       "3            175.0    0.0   175.0            NaN         0.0     -2  \n",
       "4              0.0  175.0   175.0            NaN         0.0     -2  \n",
       "...            ...    ...     ...            ...         ...    ...  \n",
       "2588272      165.0    6.0   171.0            NaN         0.0     -2  \n",
       "2588273        0.0   18.0    18.0           CLAY         1.0      0  \n",
       "2588274       85.0   35.0   120.0            NaN         0.0     -2  \n",
       "2588275      120.0   45.0   165.0            NaN         0.0     -2  \n",
       "2588276       18.0   67.0    85.0            NaN         0.0     -2  \n",
       "\n",
       "[2588277 rows x 10 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downholeData_sorted = downholeData.sort_values(['API_NUMBER','TOP'])\n",
    "downholeData_sorted.reset_index(inplace=True)\n",
    "downholeData_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bedrock Depth and Layer Thickness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproject and align raster grids for surface elevation and bedrock topo (reproject well data too if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###TO DO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the surface elevation raster and bedrock elevation raster to get depth to bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, divide by 9 to get thickness of layer at each point in model grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, sample each well point (headerData) to get layer thickness, surface elevation, and bedrock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#PRevious code that might be useful\n",
    "#xyzData = pd.read_csv(processDirectory+\"headerData__essCols_elev10mDTM.csv\",usecols = ['API_NUMBER', 'LONGITUDE','LATITUDE', '10mDTM_ft'])\n",
    "#downhole_XYZ = pd.merge(downholeData_sorted, xyzData.set_index('API_NUMBER'), on='API_NUMBER',how='left')\n",
    "#downhole_XYZ['TOP_ELEV_FT'] = downhole_XYZ['10mDTM_ft'] - downhole_XYZ['TOP']\n",
    "#downhole_XYZ['BOT_ELEV_FT'] = downhole_XYZ['10mDTM_ft'] - downhole_XYZ['BOTTOM']\n",
    "#downhole_XYZ['TOP_ELEV_M'] = downhole_XYZ['TOP_ELEV_FT'] * 0.3048\n",
    "#downhole_XYZ['BOT_ELEV_M'] = downhole_XYZ['BOT_ELEV_FT'] * 0.3048\n",
    "#downhole_XYZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate  all layer depths/elevations at all wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to calculate target thickness in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##THIS CELL MAY NEED TO BE UPDATED!!!!!\n",
    "\n",
    "\n",
    "#Define the function to export the result of thickness of target sediments in each layer\n",
    "def Layers_surfacedown(df, layer = 1):\n",
    "    \n",
    "    #Generate Column names based on (looped) integers\n",
    "    topCol = \"ESL_ModelTopoLyrs_\"+str(layer)\n",
    "    if layer != 9: #For all layers except the bottom layer....\n",
    "        botCol = \"ESL_ModelTopoLyrs_\"+str(layer+1) #use the layer below it to \n",
    "    else: #Otherwise, ...\n",
    "        botCol = \"BedrockCorr\" #Use the (corrected) bedrock depth\n",
    "\n",
    "    #Divide records into 4 separate categories for ease of calculation, to be joined back together later  \n",
    "        #Category 1: Well interval starts above layer top, ends within model layer\n",
    "        #Category 2: Well interval is entirely contained withing model layer\n",
    "        #Category 3: Well interval starts within model layer, continues through bottom of model layer\n",
    "        #Category 4: well interval begins and ends on either side of model layer (model layer is contained within well layer)\n",
    "\n",
    "    #records1 = intervals that go through the top of the layer and bottom is within layer\n",
    "    records1 = df.loc[(df['TOP_ELEV_ft'] > df[topCol]) & (df['BOT_ELEV_ft'] > df[botCol]) & (df['BOT_ELEV_ft'] <= df[topCol]) & (df['BOT_ELEV_ft'] <= df['TOP_ELEV_ft'])].copy()\n",
    "    records1['TARG_THICK'] = pd.DataFrame(np.round((records1.loc[:,topCol]-records1.loc[: , 'BOT_ELEV_ft']) * records1['Target'],3)).copy()\n",
    "    \n",
    "    #records2 = entire interval is within layer\n",
    "    records2 = df.loc[(df['TOP_ELEV_ft'] <= df[topCol]) & (df['BOT_ELEV_ft'] >= df[botCol]) & (df['BOT_ELEV_ft'] <= df['TOP_ELEV_ft'])].copy()\n",
    "    records2['TARG_THICK'] = pd.DataFrame(np.round((records2.loc[: , 'TOP_ELEV_ft'] - records2.loc[: , 'BOT_ELEV_ft']) * records2['Target'],3)).copy()\n",
    "\n",
    "    #records3 = intervals with top within layer and bottom of interval going through bottom of layer\n",
    "    records3 = df.loc[(df['TOP_ELEV_ft'] > df[botCol]) & (df['BOT_ELEV_ft'] < df[botCol]) & (df['TOP_ELEV_ft'] <= df[topCol]) & (df['BOT_ELEV_ft'] <= df['TOP_ELEV_ft'])].copy()\n",
    "    records3['TARG_THICK'] = pd.DataFrame(np.round((records3.loc[: , 'TOP_ELEV_ft'] - (records3.loc[:,botCol]))*records3['Target'],3)).copy()\n",
    "\n",
    "    #records4 = interval goes through entire layer\n",
    "    records4 = df.loc[(df['TOP_ELEV_ft'] > df[topCol]) & (df['BOT_ELEV_ft'] < df[botCol]) & (df['BOT_ELEV_ft'] <= df['TOP_ELEV_ft'])].copy()\n",
    "    records4['TARG_THICK'] = pd.DataFrame(np.round((records4.loc[: , topCol]-records4.loc[: , botCol]) * records4['Target'],3)).copy()\n",
    "\n",
    "    \n",
    "    #Put the four calculated record categories back together into single dataframe\n",
    "    res = records1.append(records2).append(records3).append(records4)\n",
    "    \n",
    "    res_df = res.groupby(['API_NUMBER','LATITUDE','LONGITUDE'],as_index=False).sum()#calculate thickness for each well interval in the layer indicated (e.g., if there are two well intervals from same well in one model layer)\n",
    "\n",
    "    res_df['TARG_THICK_PER'] = pd.DataFrame(np.round(res_df['TARG_THICK']/res_df['LyrThick'],3)) #Calculate thickness as percent of total layer thickness\n",
    "    res_df[\"LAYER\"] = layer #Just to have as part of the output file, include the present layer in the file itself as a separate column\n",
    "    res_df = res_df[['API_NUMBER', 'LATITUDE', 'LONGITUDE', 'TOP', 'BOTTOM','SURF_ELEV_ft', 'TOP_ELEV_ft', 'BOT_ELEV_ft',topCol,botCol,'LyrThick','TARG_THICK', 'TARG_THICK_PER', 'LAYER']].copy() #Format dataframe for output\n",
    "    \n",
    "    return res, res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run that function over all the layers, looping through each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codeTarget' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mgeophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\AutoClassification_OracleWellData.ipynb Cell 101'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell://isgs-sinkhole/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification/AutoClassification_OracleWellData.ipynb#ch0000129?line=0'>1</a>\u001b[0m \u001b[39m#THIS CELL WILL NEED TO BE UPDATED\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell://isgs-sinkhole/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification/AutoClassification_OracleWellData.ipynb#ch0000129?line=2'>3</a>\u001b[0m outDIR \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\\\\\u001b[39;00m\u001b[39misgs-sinkhole\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mgeophysics\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mBalikian\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mISWS_HydroGeo\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mMetroEast_HydroGeo\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mCodeOutput\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mcodeTarget\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell://isgs-sinkhole/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification/AutoClassification_OracleWellData.ipynb#ch0000129?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(\u001b[39m1\u001b[39m,\u001b[39m10\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell://isgs-sinkhole/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification/AutoClassification_OracleWellData.ipynb#ch0000129?line=5'>6</a>\u001b[0m     res, res_df \u001b[39m=\u001b[39m Layers_surfacedown(df, layer \u001b[39m=\u001b[39m i)\u001b[39m#Run the function defined above for each layer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'codeTarget' is not defined"
     ]
    }
   ],
   "source": [
    "#THIS CELL WILL NEED TO BE UPDATED\n",
    "\n",
    "outDIR = \"\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\ISWS_HydroGeo\\\\MetroEast_HydroGeo\\\\CodeOutput\\\\\"+codeTarget+\"\\\\\"\n",
    "\n",
    "for i in np.arange(1,10):\n",
    "    res, res_df = Layers_surfacedown(df, layer = i)#Run the function defined above for each layer\n",
    "    outputname = codeTargShort+'Lyr'+str(i)+'.csv' #Create a filename based on the layer and target\n",
    "    res_df.to_csv(outDIR+outputname)  #Export the file to csv\n",
    "    #Could also potentially save these to variables for use in following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate thickness values in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through each layer and interpolate (use same parameters (?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure rasters align (are co-registered) with grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export data \n",
    "downhole_bedrockDepth_XYZ.to_csv('\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\BedrockWellData\\\\Wells\\\\ProcessedWellData\\\\Downhole_BedrockPicks.csv',index_label=\"ID\")\n",
    "wPermits_XYZ.to_csv('\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\BedrockWellData\\\\Wells\\\\ProcessedWellData\\\\wPermits_BedrockPicks.csv',index_label=\"ID\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raster38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "eacfbd7833b09ac8bfcf3597a4f98d1ccaa412ac01c00d3f955e77aac9f1d08d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
