{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{Fill in with information about this notebook}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import numpy as np #Data manipulation\n",
    "import pandas as pd #Point data manipulation and organization\n",
    "import xarray as xr #Raster data manipulation and organization\n",
    "\n",
    "import pathlib  #For filepaths, io, etc.\n",
    "import os       #For several system-based commands\n",
    "import datetime #For manipulation of time data, including file creation/modification times\n",
    "import json     #For dictionary io, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt #For plotting and data vizualization\n",
    "import geopandas                #For organization and manipulation of vector data in space (study area and some data points)\n",
    "import rioxarray as rxr         #For orgnaization and manipulation of raster data\n",
    "import shapely                  #For converting coordinates to point geometry\n",
    "\n",
    "#Scripts with functions made for this specific application\n",
    "from lib import readData    #For reading data \n",
    "from lib import mapping     #For geospatial data manipulation\n",
    "from lib import cleanData   #For cleaning well data\n",
    "from lib import classify    #For classifying well data\n",
    "from lib import exportData  #For exporting data\n",
    "\n",
    "#Variables needed throughout, best to just assign now\n",
    "todayDate, dateSuffix = readData.getCurrentDate() \n",
    "repoDir = pathlib.Path(os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set up filepaths\n",
    "- Read in data from:\n",
    "    - downholeData table (from database)\n",
    "    - headerData table (from database)\n",
    "    - xyzData file (from previously carried out work) (will eventually make this updateable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Recent version of this file is : ISGS_DOWNHOLE_DATA_2023-01-06.txt\n",
      "Most Recent version of this file is : ISGS_HEADER_2023-01-06.txt\n",
      "Most Recent version of this file is : xyzData.csv\n",
      "Using the following files:\n",
      "\n",
      "\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\ISGS_DOWNHOLE_DATA_2023-01-06.txt\n",
      "\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\ISGS_HEADER_2023-01-06.txt\n",
      "\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\xyzData.csv\n",
      "Downhole Data has 3054409 valid well records.\n",
      "Header Data has 636855 unique wells with valid location information.\n"
     ]
    }
   ],
   "source": [
    "directoryDir = r'\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\BedrockWellData\\Wells\\RawWellData_OracleDatabase\\TxtData\\\\'[:-1]\n",
    "\n",
    "downholeDataPATH, headerDataPATH, xyzInPATH  = readData.filesSetup(db_dir=directoryDir)\n",
    "\n",
    "#Functions to read data into dataframes. Also excludes extraneous columns, and drops header data with no location information\n",
    "headerDataIN, downholeDataIN = readData.readRawTxtData(downholefile=downholeDataPATH, headerfile=headerDataPATH) \n",
    "xyzDataIN = readData.readXYZData(xyzfile=xyzInPATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define datatypes (doing this during the read in process has presented issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\Documents\\GitHub\\wells4hydrogeology\\lib\\readData.py:125: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:,i] = dfIN.iloc[:,i].astype(dtypes[dfIN.iloc[:,i].name])\n"
     ]
    }
   ],
   "source": [
    "#Define datatypes, to read into defineDataTypes() function\n",
    "##EVENTUALLY, MAKE THIS A FILE IN THE RES FOLDER TO READ IN\n",
    "#downholeDataDTYPES = {'ID':np.uint32, \"API_NUMBER\":np.uint64,\"TABLE_NAME\":str,\"WHO\":str,\"INTERPRET_DATE\":str,\"FORMATION\":str,\"THICKNESS\":np.float64,\"TOP\":np.float64,\"BOTTOM\":np.float64}\n",
    "#headerDataDTYPES = {'ID':np.uint32,'API_NUMBER':np.uint64,\"TDFORMATION\":str,\"PRODFORM\":str,\"TOTAL_DEPTH\":np.float64,\"SECTION\":np.float64,\"TWP\":np.float64,\"TDIR\":str,\"RNG\":np.float64,\"RDIR\":str,\"MERIDIAN\":np.float64,\"FARM_NAME\":str,\"NSFOOT\":np.float64,\"NSDIR\":str,\"EWFOOT\":np.float64,\"EWDIR\":str,\"QUARTERS\":str,\"ELEVATION\":np.float64,\"ELEVREF\":str,\"COMP_DATE\":str,\"STATUS\":str,\"FARM_NUM\":str,\"COUNTY_CODE\":np.float64,\"PERMIT_NUMBER\":str,\"COMPANY_NAME\":str,\"COMPANY_CODE\":str,\"PERMIT_DATE\":str,\"CORNER\":str,\"LATITUDE\":np.float64,\"LONGITUDE\":np.float64,\"ENTERED_BY\":str,\"UPDDATE\":str,\"ELEVSOURCE\":str, \"ELEV_FT\":np.float64}\n",
    "#xyzDataDTYPES = {'ID':np.uint64, 'API_NUMBER':np.uint64, \"LATITUDE\":np.float64, \"LONGITUDE\":np.float64, \"ELEV_FT\":np.float64}\n",
    "\n",
    "#Define datatypes of each column of the new dataframes\n",
    "downholeDataIN = readData.defineDataTypes(downholeDataIN, dtypeFile='downholeDataTypes.txt')\n",
    "headerDataIN = readData.defineDataTypes(headerDataIN, dtypeFile='headerDataTypes.txt')\n",
    "xyzDataIN = readData.defineDataTypes(xyzDataIN, dtypeFile='xyzDataTypes.txt')\n",
    "\n",
    "#Make a copy of the data so raw data is preserved while we work with the rest of the data\n",
    "downholeData = downholeDataIN.copy()\n",
    "headerData = headerDataIN.copy()\n",
    "xyzData = xyzDataIN.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in Control points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEED CODE HERE FOR ADDING IN CONTROL Wells MANUALLY\n",
    "#Add control headerInfo\n",
    "#Add control description info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Elevation Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract elevation data from consistent elevation dataset for all wells (lidar or other statewide DEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#First, get wells with updated xyz info\n",
    "    #Check first if xyzData needs to be updated with locations (?)\n",
    "    #Check which wells in headerData don't have associated lidar data\n",
    "\n",
    "#statewideLidar =  ow\n",
    "#mapping.rastertoPoints_extract()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge elevation data with headerData table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'lib.mapping' has no attribute 'coords2Geometry'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m#xyzData['UniqueWells'] = uniqueWells\u001b[39;00m\n\u001b[0;32m      4\u001b[0m headerData \u001b[39m=\u001b[39m mapping\u001b[39m.\u001b[39maddElevtoHeader(xyzData, headerData)\n\u001b[1;32m----> 5\u001b[0m geoHeaderData \u001b[39m=\u001b[39m mapping\u001b[39m.\u001b[39;49mcoords2Geometry(headerData, xCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLONGITUDE\u001b[39m\u001b[39m'\u001b[39m, yCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLATITUDE\u001b[39m\u001b[39m'\u001b[39m, crs\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEPSG:4269\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'lib.mapping' has no attribute 'coords2Geometry'"
     ]
    }
   ],
   "source": [
    "uniqueWells = headerData['API_NUMBER'].unique()\n",
    "#xyzData['UniqueWells'] = uniqueWells\n",
    "\n",
    "headerData = mapping.addElevtoHeader(xyzData, headerData)\n",
    "##NEED TO UPDATE THIS TO WORK WITH DATA WITH NO XYZ ELEVATION DATA FROM LIDAR\n",
    "#Change xyz column name to indicate lidar\n",
    "#Use order of preference: lidar, headerData table?/30/10m DEM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's clean up records in the data without the necessary information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HeaderXYZ dataframe has headerdata with updated elevations (from lidar or other DEM)\n",
    "\n",
    "Now, remove data from downholeData table that does not have location information (Since we would not know where to put it anyway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13615 records removed without location information.\n"
     ]
    }
   ],
   "source": [
    "downholeData = cleanData.removeNonlocatedData(downholeData, headerData)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip data from outside Study Area"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in Study Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studyAreaPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESL_StudyArea_5mi.shp\"\n",
    "studyAreaIN, saExtent = mapping.readStudyArea(studyAreaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoHeaderData = mapping.coords2Geometry(df=headerData, xCol='LONGITUDE', yCol='LATITUDE', crs='EPSG:4269')\n",
    "geoHeaderData_StudyAreaOnly = geoHeaderData.clip(studyAreaIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>API_NUMBER</th>\n",
       "      <th>TOTAL_DEPTH</th>\n",
       "      <th>SECTION</th>\n",
       "      <th>TWP</th>\n",
       "      <th>TDIR</th>\n",
       "      <th>RNG</th>\n",
       "      <th>RDIR</th>\n",
       "      <th>MERIDIAN</th>\n",
       "      <th>QUARTERS</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>ELEVREF</th>\n",
       "      <th>COUNTY_CODE</th>\n",
       "      <th>ELEVSOURCE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEV_FT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4045883208</td>\n",
       "      <td>570.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>5.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4.0</td>\n",
       "      <td>SW SE</td>\n",
       "      <td>620.0</td>\n",
       "      <td>GL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>40.080723</td>\n",
       "      <td>-90.921097</td>\n",
       "      <td>616.728271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4045883308</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>5.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4.0</td>\n",
       "      <td>SE SE</td>\n",
       "      <td>590.0</td>\n",
       "      <td>GL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>40.079857</td>\n",
       "      <td>-90.913589</td>\n",
       "      <td>580.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4045883408</td>\n",
       "      <td>725.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>5.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>731.0</td>\n",
       "      <td>TM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>40.051773</td>\n",
       "      <td>-90.995911</td>\n",
       "      <td>679.606506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4045883508</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>5.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4.0</td>\n",
       "      <td>SE NE NW</td>\n",
       "      <td>732.0</td>\n",
       "      <td>GL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>40.059509</td>\n",
       "      <td>-90.963264</td>\n",
       "      <td>737.663574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4045883608</td>\n",
       "      <td>732.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>5.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>745.0</td>\n",
       "      <td>TM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>40.054066</td>\n",
       "      <td>-90.932716</td>\n",
       "      <td>741.997925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636850</th>\n",
       "      <td>3848631244</td>\n",
       "      <td>139.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>21.0</td>\n",
       "      <td>E</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SW NE NW</td>\n",
       "      <td>817.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>59.0</td>\n",
       "      <td>GPS</td>\n",
       "      <td>42.522205</td>\n",
       "      <td>-88.064186</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636851</th>\n",
       "      <td>3848631344</td>\n",
       "      <td>50.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>19.0</td>\n",
       "      <td>E</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SW SW SE</td>\n",
       "      <td>975.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>59.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>42.495945</td>\n",
       "      <td>-88.232162</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636852</th>\n",
       "      <td>3848657344</td>\n",
       "      <td>209.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>20.0</td>\n",
       "      <td>E</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SE SE</td>\n",
       "      <td>780.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>59.0</td>\n",
       "      <td>GPS</td>\n",
       "      <td>42.526997</td>\n",
       "      <td>-88.128387</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636853</th>\n",
       "      <td>3848658244</td>\n",
       "      <td>200.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>20.0</td>\n",
       "      <td>E</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SE NE</td>\n",
       "      <td>840.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>59.0</td>\n",
       "      <td>TM</td>\n",
       "      <td>42.504860</td>\n",
       "      <td>-88.075172</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636854</th>\n",
       "      <td>3848673144</td>\n",
       "      <td>176.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>20.0</td>\n",
       "      <td>E</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SW SE</td>\n",
       "      <td>825.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>59.0</td>\n",
       "      <td>TM</td>\n",
       "      <td>42.496857</td>\n",
       "      <td>-88.095139</td>\n",
       "      <td>825.878113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>636855 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        API_NUMBER  TOTAL_DEPTH  SECTION  TWP TDIR   RNG RDIR  MERIDIAN  \\\n",
       "0       4045883208        570.0     12.0  1.0    N   5.0    W       4.0   \n",
       "1       4045883308         20.0     12.0  1.0    N   5.0    W       4.0   \n",
       "2       4045883408        725.0     20.0  1.0    N   5.0    W       4.0   \n",
       "3       4045883508       1129.0     22.0  1.0    N   5.0    W       4.0   \n",
       "4       4045883608        732.0     24.0  1.0    N   5.0    W       4.0   \n",
       "...            ...          ...      ...  ...  ...   ...  ...       ...   \n",
       "636850  3848631244        139.0     30.0  1.0    N  21.0    E       0.0   \n",
       "636851  3848631344         50.0     34.0  1.0    N  19.0    E       0.0   \n",
       "636852  3848657344        209.0     21.0  1.0    N  20.0    E       0.0   \n",
       "636853  3848658244        200.0     36.0  1.0    N  20.0    E       0.0   \n",
       "636854  3848673144        176.0     35.0  1.0    N  20.0    E       0.0   \n",
       "\n",
       "        QUARTERS  ELEVATION ELEVREF  COUNTY_CODE ELEVSOURCE   LATITUDE  \\\n",
       "0          SW SE      620.0      GL          1.0        nan  40.080723   \n",
       "1          SE SE      590.0      GL          1.0        nan  40.079857   \n",
       "2             SE      731.0      TM          1.0        nan  40.051773   \n",
       "3       SE NE NW      732.0      GL          1.0        nan  40.059509   \n",
       "4             SW      745.0      TM          1.0        nan  40.054066   \n",
       "...          ...        ...     ...          ...        ...        ...   \n",
       "636850  SW NE NW      817.0     nan         59.0        GPS  42.522205   \n",
       "636851  SW SW SE      975.0     nan         59.0        nan  42.495945   \n",
       "636852     SE SE      780.0     nan         59.0        GPS  42.526997   \n",
       "636853     SE NE      840.0     nan         59.0         TM  42.504860   \n",
       "636854     SW SE      825.0     nan         59.0         TM  42.496857   \n",
       "\n",
       "        LONGITUDE     ELEV_FT  \n",
       "0      -90.921097  616.728271  \n",
       "1      -90.913589  580.442200  \n",
       "2      -90.995911  679.606506  \n",
       "3      -90.963264  737.663574  \n",
       "4      -90.932716  741.997925  \n",
       "...           ...         ...  \n",
       "636850 -88.064186         NaN  \n",
       "636851 -88.232162    0.000000  \n",
       "636852 -88.128387         NaN  \n",
       "636853 -88.075172    0.000000  \n",
       "636854 -88.095139  825.878113  \n",
       "\n",
       "[636855 rows x 16 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headerData"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove headerData rows without surface elevation information (this currently clips data from outside Illinois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropping those without surface elevation information: 636855\n",
      "Number of rows after dropping those without surface elevation information: 632702\n",
      "Well records deleted: 4153\n"
     ]
    }
   ],
   "source": [
    "headerData = cleanData.removenotopo(headerData, printouts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows from downholeData with no depth information and where depth information is obviously bad (i.e., top depth > bottom depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropping those without record depth information: 3040794\n",
      "Number of rows after dropping those without record depth information: 2625120\n",
      "Number of well records without formation information deleted: 415674\n",
      "Number of rows before dropping those with obviously bad depth information: 3040794\n",
      "Number of rows after dropping those with obviously bad depth information: 2622533\n",
      "Well records deleted: 418261\n"
     ]
    }
   ],
   "source": [
    "#Drop records with no depth information\n",
    "donwholeData = cleanData.dropnodepth(downholeData, printouts=True)\n",
    "#Drop records with bad depth information (i.e., top depth > bottom depth) (Also calculates thickness of each record)\n",
    "donwholeData = cleanData.dropbaddepth(downholeData, printouts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop records with no FORMATION information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropping those without FORMATION information: 3040794\n",
      "Number of rows after dropping those without FORMATION information: 3040794\n",
      "Well records deleted: 0\n"
     ]
    }
   ],
   "source": [
    "downholeData = cleanData.dropnoformation(downholeData, printouts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to export this data, to have record of cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "downholeData.reset_index(inplace=True,drop=True)\n",
    "headerData.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#downholeData.to_csv(str(repoDir)+'/out/downholeData_cleaned'+dateSuffix+'.csv',index_label='ID')\n",
    "#headerData.to_csv(str(repoDir)+'/out/headerData_cleaned'+dateSuffix+'.csv',index_label='ID')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following flags are used to mark the classification method:\n",
    "- 0: Not classified\n",
    "- 1: Specific Search Term Match\n",
    "- 2: Wildcard match (startTerm) - no context\n",
    "- 3: Bedrock classification for obvious bedrock\n",
    "- 4: Wildcard match (startTerm) - with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Recent version of this file is : SearchTerms-Specific_2022-11-16_essCols.csv\n",
      "Most Recent version of this file is : SearchTerms-Start.csv\n"
     ]
    }
   ],
   "source": [
    "#Read in dictionary files for downhole data\n",
    "specTermsPATH, startTermsPATH = readData.searchTermFilePaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\Documents\\GitHub\\wells4hydrogeology\\lib\\readData.py:155: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  specTerms.iloc[:,i] = specTerms.iloc[:,i].astype(specTermsDtypes[specTerms.iloc[:,i].name])\n",
      "c:\\Users\\riley\\Documents\\GitHub\\wells4hydrogeology\\lib\\readData.py:156: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  startTerms.iloc[:,i] = startTerms.iloc[:,i].astype(startTermsDtypes[startTerms.iloc[:,i].name])\n"
     ]
    }
   ],
   "source": [
    "specTerms, startTerms = readData.readSearchTerms(specfile=specTermsPATH, startfile=startTermsPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the dataframes--for the specific search terms, this is the same as classifying them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records Classified with full search term: 2138359\n",
      "Records Classified with full search term: 70.32% of data\n"
     ]
    }
   ],
   "source": [
    "downholeData_spec = classify.specificDefine(downholeData, specTerms, printouts=True)\n",
    "downholeData = downholeData_spec.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with only the records already classified (using the specific search terms in this case, classifiedDF), and one that still needs to be searched (searchDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902435"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifedDF, searchDF = classify.splitDefined(downholeData)\n",
    "searchDF.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do the classification routine on the searchDF database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Term process should be done by 10:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\Documents\\GitHub\\wells4hydrogeology\\lib\\classify.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['CLASS_FLAG'].where(~df['FORMATION'].str.startswith(s,na=False),4,inplace=True)\n",
      "c:\\Users\\riley\\Documents\\GitHub\\wells4hydrogeology\\lib\\classify.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['INTERPRETATION'].where(~df['FORMATION'].str.startswith(s,na=False),starterms.loc[i,'INTERPRETATION'],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records classified with start search term: 107010\n",
      "Records classified with start search term: 11.86% of remaining data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\Documents\\GitHub\\wells4hydrogeology\\lib\\classify.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['BEDROCK_FLAG'] = df[\"INTERPRETATION\"] == 'BEDROCK'\n"
     ]
    }
   ],
   "source": [
    "searchDF = classify.startDefine(df=searchDF, starterms=startTerms, printouts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge specDF and searchDF back together all back in single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "downholeData_Terms = classify.remergeData(classifieddf=classifedDF, searchdf=searchDF)\n",
    "downholeData = downholeData_Terms.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export terms that still need to be defined to csv (along with their counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify.export_toBeDefined(downholeData, str(repoDir)+'/out/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify all  data under depth threshold (default is 550') as bedrock (should not be an issue, but just in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riley\\Documents\\GitHub\\wells4hydrogeology\\lib\\classify.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['CLASS_FLAG'].mask(df['TOP']>thresh, 3 ,inplace=True) #Add a Classification Flag of 3 (bedrock b/c it's deepter than 550') to all records where the top of the interval is >550'\n",
      "c:\\Users\\riley\\Documents\\GitHub\\wells4hydrogeology\\lib\\classify.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['BEDROCK_FLAG'].mask(df['TOP']>thresh, True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records classified deeper than 550' :79219\n",
      "Records classified deeper than 550', as a percentage of remaining unclassified records: 11.06%\n",
      "Total bedrock records classified: 158438\n",
      "This is now a total of 19.92% of the data classified.\n"
     ]
    }
   ],
   "source": [
    "classifedDF, searchDF = classify.splitDefined(downholeData)\n",
    "searchDF = classify.depthDefine(searchDF, thresh=550, printouts=True)\n",
    "downholeData_Class = classify.remergeData(classifieddf=classifedDF, searchdf=searchDF)\n",
    "downholeData = downholeData_Class.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add '0' flag for data still not classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "downholeData = classify.fillUnclassified(downholeData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    2138359\n",
       "0.0     716206\n",
       "4.0     107010\n",
       "3.0      79219\n",
       "Name: CLASS_FLAG, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downholeData['CLASS_FLAG'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add \"Flag\" for target interpratations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictDir = \"\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\ISWS_HydroGeo\\\\WellDataAutoClassification\\\\SupportingDocs\\\\\"\n",
    "targetInterpDF = readData.readLithologies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "downholeData = classify.mergeLithologies(downholedata=downholeData, targinterps=targetInterpDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flags used for target classification purposes:\n",
    "- -2: No classification \n",
    "- -1: Classified, not used/not definitive\n",
    "- 0: Classified, not target material\n",
    "- 1: Classified as target material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    1456486\n",
       "-2     796686\n",
       "0      549662\n",
       "1      237960\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downholeData['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all unique wells in downhole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique wells in downholeData: 458260\n"
     ]
    }
   ],
   "source": [
    "#Get Unique well APIs\n",
    "wellsDF = classify.getUniqueWells(downholeData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort dataset by API Number and Depth of top of record (will be easier to do data analysis with records in the correct order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>API_NUMBER</th>\n",
       "      <th>TABLE_NAME</th>\n",
       "      <th>FORMATION</th>\n",
       "      <th>THICKNESS</th>\n",
       "      <th>TOP</th>\n",
       "      <th>BOTTOM</th>\n",
       "      <th>INTERPRETATION</th>\n",
       "      <th>CLASS_FLAG</th>\n",
       "      <th>BEDROCK_FLAG</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>MUD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>274385</td>\n",
       "      <td>10915812</td>\n",
       "      <td>FORMATION_TOPS</td>\n",
       "      <td>Herrin Coal #6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>BEDROCK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DIRT AND GRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274386</td>\n",
       "      <td>10915912</td>\n",
       "      <td>FORMATION_TOPS</td>\n",
       "      <td>Herrin Coal #6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>BEDROCK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DIRT AND GRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>274387</td>\n",
       "      <td>10916012</td>\n",
       "      <td>FORMATION_TOPS</td>\n",
       "      <td>Herrin Coal #6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>BEDROCK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DIRT AND GRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>274388</td>\n",
       "      <td>10916112</td>\n",
       "      <td>FORMATION_TOPS</td>\n",
       "      <td>Danville Coal #7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>BEDROCK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DIRT AND GRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274389</td>\n",
       "      <td>10916112</td>\n",
       "      <td>FORMATION_TOPS</td>\n",
       "      <td>Herrin Coal #6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>BEDROCK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DIRT AND GRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040789</th>\n",
       "      <td>274370</td>\n",
       "      <td>4289724608</td>\n",
       "      <td>HWYBRIDGE_LOG</td>\n",
       "      <td>medium, very damp, sandy clay loam with 2\" thi...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>39.5</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040790</th>\n",
       "      <td>274381</td>\n",
       "      <td>4289724608</td>\n",
       "      <td>HWYBRIDGE_LOG</td>\n",
       "      <td>very loose, wet, fine grain, sand</td>\n",
       "      <td>4.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040791</th>\n",
       "      <td>274374</td>\n",
       "      <td>4289724608</td>\n",
       "      <td>HWYBRIDGE_LOG</td>\n",
       "      <td>soft, wet, sandy clay loam with 1/2\" thick san...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>46.0</td>\n",
       "      <td>48.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040792</th>\n",
       "      <td>274376</td>\n",
       "      <td>4289724608</td>\n",
       "      <td>HWYBRIDGE_LOG</td>\n",
       "      <td>stiff, damp, cohesive mixture of sand, clay, g...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48.5</td>\n",
       "      <td>50.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040793</th>\n",
       "      <td>274380</td>\n",
       "      <td>4289724608</td>\n",
       "      <td>HWYBRIDGE_LOG</td>\n",
       "      <td>very dense, very moist, weathered, sandstone</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.5</td>\n",
       "      <td>52.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3040794 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  API_NUMBER      TABLE_NAME  \\\n",
       "0        274385    10915812  FORMATION_TOPS   \n",
       "1        274386    10915912  FORMATION_TOPS   \n",
       "2        274387    10916012  FORMATION_TOPS   \n",
       "3        274388    10916112  FORMATION_TOPS   \n",
       "4        274389    10916112  FORMATION_TOPS   \n",
       "...         ...         ...             ...   \n",
       "3040789  274370  4289724608   HWYBRIDGE_LOG   \n",
       "3040790  274381  4289724608   HWYBRIDGE_LOG   \n",
       "3040791  274374  4289724608   HWYBRIDGE_LOG   \n",
       "3040792  274376  4289724608   HWYBRIDGE_LOG   \n",
       "3040793  274380  4289724608   HWYBRIDGE_LOG   \n",
       "\n",
       "                                                 FORMATION  THICKNESS    TOP  \\\n",
       "0                                           Herrin Coal #6        5.0  400.0   \n",
       "1                                           Herrin Coal #6        6.0  403.0   \n",
       "2                                           Herrin Coal #6        6.0  406.0   \n",
       "3                                         Danville Coal #7        3.0  390.0   \n",
       "4                                           Herrin Coal #6        6.0  404.0   \n",
       "...                                                    ...        ...    ...   \n",
       "3040789  medium, very damp, sandy clay loam with 2\" thi...        2.5   39.5   \n",
       "3040790                  very loose, wet, fine grain, sand        4.0   42.0   \n",
       "3040791  soft, wet, sandy clay loam with 1/2\" thick san...        2.5   46.0   \n",
       "3040792  stiff, damp, cohesive mixture of sand, clay, g...        2.0   48.5   \n",
       "3040793       very dense, very moist, weathered, sandstone        2.0   50.5   \n",
       "\n",
       "         BOTTOM INTERPRETATION  CLASS_FLAG  BEDROCK_FLAG TARGET  Unnamed: 2  \\\n",
       "0         405.0        BEDROCK         1.0          True     -1         NaN   \n",
       "1         409.0        BEDROCK         1.0          True     -1         NaN   \n",
       "2         412.0        BEDROCK         1.0          True     -1         NaN   \n",
       "3         393.0        BEDROCK         1.0          True     -1         NaN   \n",
       "4         410.0        BEDROCK         1.0          True     -1         NaN   \n",
       "...         ...            ...         ...           ...    ...         ...   \n",
       "3040789    42.0            NaN         0.0         False     -2         NaN   \n",
       "3040790    46.0            NaN         0.0         False     -2         NaN   \n",
       "3040791    48.5            NaN         0.0         False     -2         NaN   \n",
       "3040792    50.5            NaN         0.0         False     -2         NaN   \n",
       "3040793    52.5            NaN         0.0         False     -2         NaN   \n",
       "\n",
       "                     MUD  \n",
       "0        DIRT AND GRAVEL  \n",
       "1        DIRT AND GRAVEL  \n",
       "2        DIRT AND GRAVEL  \n",
       "3        DIRT AND GRAVEL  \n",
       "4        DIRT AND GRAVEL  \n",
       "...                  ...  \n",
       "3040789              NaN  \n",
       "3040790              NaN  \n",
       "3040791              NaN  \n",
       "3040792              NaN  \n",
       "3040793              NaN  \n",
       "\n",
       "[3040794 rows x 13 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downholeData_sorted = downholeData.sort_values(['API_NUMBER','TOP'])\n",
    "downholeData_sorted.reset_index(inplace=True)\n",
    "downholeData_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bedrock Depth and Layer Thickness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in/Define Model Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGridPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\grid_625_raster.tif\"\n",
    "modelGrid = mapping.readModelGrid(saExtent=saExtent, gridpath=modelGridPath, nodataval=0, readGrid=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in surface elevation grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'noDataSurf' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\riley\\Documents\\GitHub\\wells4hydrogeology\\lib\\mapping.py:109\u001b[0m, in \u001b[0;36mreadSurfaceGrid\u001b[1;34m(surfaceelevpath, nodataval)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m     noDataBed \u001b[39m=\u001b[39m surfaceElevGridIN\u001b[39m.\u001b[39;49mattrs[\u001b[39m'\u001b[39;49m\u001b[39m_FillValue\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m#Extract from dataset itself\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: '_FillValue'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m surfaceElevPath \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39misgs-sinkhole.ad.uillinois.edu\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mgeophysics\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mBalikian\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mISWS_HydroGeo\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mWellDataAutoClassification\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSampleData\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mILStateLidar_ClipExtentESL.tif\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m surfaceElevGridIN \u001b[39m=\u001b[39m mapping\u001b[39m.\u001b[39;49mreadSurfaceGrid(surfaceelevpath\u001b[39m=\u001b[39;49msurfaceElevPath)\n",
      "File \u001b[1;32mc:\\Users\\riley\\Documents\\GitHub\\wells4hydrogeology\\lib\\mapping.py:111\u001b[0m, in \u001b[0;36mreadSurfaceGrid\u001b[1;34m(surfaceelevpath, nodataval)\u001b[0m\n\u001b[0;32m    109\u001b[0m     noDataBed \u001b[39m=\u001b[39m surfaceElevGridIN\u001b[39m.\u001b[39mattrs[\u001b[39m'\u001b[39m\u001b[39m_FillValue\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m#Extract from dataset itself\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m     \u001b[39mif\u001b[39;00m noDataSurf \u001b[39m<\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1000\u001b[39m:\n\u001b[0;32m    112\u001b[0m         noDataSurf \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1000\u001b[39m\n\u001b[0;32m    113\u001b[0m     \u001b[39melif\u001b[39;00m noDataSurf \u001b[39m>\u001b[39m \u001b[39m50000\u001b[39m:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'noDataSurf' referenced before assignment"
     ]
    }
   ],
   "source": [
    "surfaceElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ILStateLidar_ClipExtentESL.tif\"\n",
    "surfaceElevGridIN = mapping.readSurfaceGrid(surfaceelevpath=surfaceElevPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in bedrock elevation grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrockElevPath = r\"\\\\isgs-sinkhole.ad.uillinois.edu\\geophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\SampleData\\ESLBedrock.tif\"\n",
    "bedrockElevGridIN=mapping.readBedrockGrid(bedrockelevpath=bedrockElevPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot just to see them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols = 2, nrows=1)\n",
    "bedrockElevGridIN.plot(ax=ax[0])\n",
    "surfaceElevGridIN.plot(ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproject and align raster grids for surface elevation and bedrock topo (reproject well data too if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrockGrid, surfaceGrid = mapping.alignRasters(bedrockgrid=bedrockElevGridIN, surfacegrid=surfaceElevGridIN, modelgrid=modelGrid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(ncols = 2, nrows=1)\n",
    "bedrockGrid.plot(ax=ax[0])\n",
    "surfaceGrid.plot(ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the surface elevation raster and bedrock elevation raster to get depth to bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driftThickGrid, layerThickGrid = mapping.getDriftThick(surface=surfaceGrid, bedrock=bedrockGrid, noLayers=9, plotData=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work here next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, sample each well point (headerData) to get layer thickness, surface elevation, and bedrock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#PRevious code that might be useful\n",
    "#xyzData = pd.read_csv(processDirectory+\"headerData__essCols_elev10mDTM.csv\",usecols = ['API_NUMBER', 'LONGITUDE','LATITUDE', '10mDTM_ft'])\n",
    "#downhole_XYZ = pd.merge(downholeData_sorted, xyzData.set_index('API_NUMBER'), on='API_NUMBER',how='left')\n",
    "#downhole_XYZ['TOP_ELEV_FT'] = downhole_XYZ['10mDTM_ft'] - downhole_XYZ['TOP']\n",
    "#downhole_XYZ['BOT_ELEV_FT'] = downhole_XYZ['10mDTM_ft'] - downhole_XYZ['BOTTOM']\n",
    "#downhole_XYZ['TOP_ELEV_M'] = downhole_XYZ['TOP_ELEV_FT'] * 0.3048\n",
    "#downhole_XYZ['BOT_ELEV_M'] = downhole_XYZ['BOT_ELEV_FT'] * 0.3048\n",
    "#downhole_XYZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate  all layer depths/elevations at all wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to calculate target thickness in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##THIS CELL MAY NEED TO BE UPDATED!!!!!\n",
    "\n",
    "\n",
    "#Define the function to export the result of thickness of target sediments in each layer\n",
    "def Layers_surfacedown(df, layer = 1):\n",
    "    \n",
    "    #Generate Column names based on (looped) integers\n",
    "    topCol = \"ESL_ModelTopoLyrs_\"+str(layer)\n",
    "    if layer != 9: #For all layers except the bottom layer....\n",
    "        botCol = \"ESL_ModelTopoLyrs_\"+str(layer+1) #use the layer below it to \n",
    "    else: #Otherwise, ...\n",
    "        botCol = \"BedrockCorr\" #Use the (corrected) bedrock depth\n",
    "\n",
    "    #Divide records into 4 separate categories for ease of calculation, to be joined back together later  \n",
    "        #Category 1: Well interval starts above layer top, ends within model layer\n",
    "        #Category 2: Well interval is entirely contained withing model layer\n",
    "        #Category 3: Well interval starts within model layer, continues through bottom of model layer\n",
    "        #Category 4: well interval begins and ends on either side of model layer (model layer is contained within well layer)\n",
    "\n",
    "    #records1 = intervals that go through the top of the layer and bottom is within layer\n",
    "    records1 = df.loc[(df['TOP_ELEV_ft'] > df[topCol]) & (df['BOT_ELEV_ft'] > df[botCol]) & (df['BOT_ELEV_ft'] <= df[topCol]) & (df['BOT_ELEV_ft'] <= df['TOP_ELEV_ft'])].copy()\n",
    "    records1['TARG_THICK'] = pd.DataFrame(np.round((records1.loc[:,topCol]-records1.loc[: , 'BOT_ELEV_ft']) * records1['Target'],3)).copy()\n",
    "    \n",
    "    #records2 = entire interval is within layer\n",
    "    records2 = df.loc[(df['TOP_ELEV_ft'] <= df[topCol]) & (df['BOT_ELEV_ft'] >= df[botCol]) & (df['BOT_ELEV_ft'] <= df['TOP_ELEV_ft'])].copy()\n",
    "    records2['TARG_THICK'] = pd.DataFrame(np.round((records2.loc[: , 'TOP_ELEV_ft'] - records2.loc[: , 'BOT_ELEV_ft']) * records2['Target'],3)).copy()\n",
    "\n",
    "    #records3 = intervals with top within layer and bottom of interval going through bottom of layer\n",
    "    records3 = df.loc[(df['TOP_ELEV_ft'] > df[botCol]) & (df['BOT_ELEV_ft'] < df[botCol]) & (df['TOP_ELEV_ft'] <= df[topCol]) & (df['BOT_ELEV_ft'] <= df['TOP_ELEV_ft'])].copy()\n",
    "    records3['TARG_THICK'] = pd.DataFrame(np.round((records3.loc[: , 'TOP_ELEV_ft'] - (records3.loc[:,botCol]))*records3['Target'],3)).copy()\n",
    "\n",
    "    #records4 = interval goes through entire layer\n",
    "    records4 = df.loc[(df['TOP_ELEV_ft'] > df[topCol]) & (df['BOT_ELEV_ft'] < df[botCol]) & (df['BOT_ELEV_ft'] <= df['TOP_ELEV_ft'])].copy()\n",
    "    records4['TARG_THICK'] = pd.DataFrame(np.round((records4.loc[: , topCol]-records4.loc[: , botCol]) * records4['Target'],3)).copy()\n",
    "\n",
    "    \n",
    "    #Put the four calculated record categories back together into single dataframe\n",
    "    res = records1.append(records2).append(records3).append(records4)\n",
    "    \n",
    "    res_df = res.groupby(['API_NUMBER','LATITUDE','LONGITUDE'],as_index=False).sum()#calculate thickness for each well interval in the layer indicated (e.g., if there are two well intervals from same well in one model layer)\n",
    "\n",
    "    res_df['TARG_THICK_PER'] = pd.DataFrame(np.round(res_df['TARG_THICK']/res_df['LyrThick'],3)) #Calculate thickness as percent of total layer thickness\n",
    "    res_df[\"LAYER\"] = layer #Just to have as part of the output file, include the present layer in the file itself as a separate column\n",
    "    res_df = res_df[['API_NUMBER', 'LATITUDE', 'LONGITUDE', 'TOP', 'BOTTOM','SURF_ELEV_ft', 'TOP_ELEV_ft', 'BOT_ELEV_ft',topCol,botCol,'LyrThick','TARG_THICK', 'TARG_THICK_PER', 'LAYER']].copy() #Format dataframe for output\n",
    "    \n",
    "    return res, res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run that function over all the layers, looping through each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codeTarget' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mgeophysics\\Balikian\\ISWS_HydroGeo\\WellDataAutoClassification\\AutoClassification_OracleWellData.ipynb Cell 101'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell://isgs-sinkhole/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification/AutoClassification_OracleWellData.ipynb#ch0000129?line=0'>1</a>\u001b[0m \u001b[39m#THIS CELL WILL NEED TO BE UPDATED\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell://isgs-sinkhole/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification/AutoClassification_OracleWellData.ipynb#ch0000129?line=2'>3</a>\u001b[0m outDIR \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\\\\\u001b[39;00m\u001b[39misgs-sinkhole\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mgeophysics\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mBalikian\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mISWS_HydroGeo\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mMetroEast_HydroGeo\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mCodeOutput\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mcodeTarget\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell://isgs-sinkhole/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification/AutoClassification_OracleWellData.ipynb#ch0000129?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(\u001b[39m1\u001b[39m,\u001b[39m10\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell://isgs-sinkhole/geophysics/Balikian/ISWS_HydroGeo/WellDataAutoClassification/AutoClassification_OracleWellData.ipynb#ch0000129?line=5'>6</a>\u001b[0m     res, res_df \u001b[39m=\u001b[39m Layers_surfacedown(df, layer \u001b[39m=\u001b[39m i)\u001b[39m#Run the function defined above for each layer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'codeTarget' is not defined"
     ]
    }
   ],
   "source": [
    "#THIS CELL WILL NEED TO BE UPDATED\n",
    "\n",
    "outDIR = \"\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\ISWS_HydroGeo\\\\MetroEast_HydroGeo\\\\CodeOutput\\\\\"+codeTarget+\"\\\\\"\n",
    "\n",
    "for i in np.arange(1,10):\n",
    "    res, res_df = Layers_surfacedown(df, layer = i)#Run the function defined above for each layer\n",
    "    outputname = codeTargShort+'Lyr'+str(i)+'.csv' #Create a filename based on the layer and target\n",
    "    res_df.to_csv(outDIR+outputname)  #Export the file to csv\n",
    "    #Could also potentially save these to variables for use in following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate thickness values in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through each layer and interpolate (use same parameters (?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure rasters align (are co-registered) with grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export data \n",
    "downhole_bedrockDepth_XYZ.to_csv('\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\BedrockWellData\\\\Wells\\\\ProcessedWellData\\\\Downhole_BedrockPicks.csv',index_label=\"ID\")\n",
    "wPermits_XYZ.to_csv('\\\\\\\\isgs-sinkhole\\\\geophysics\\\\Balikian\\\\BedrockWellData\\\\Wells\\\\ProcessedWellData\\\\wPermits_BedrockPicks.csv',index_label=\"ID\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "1eab39f8790fa4ef06ab4ebada9c1405c2ef16219adfedb21e90bf3fb356ecb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
